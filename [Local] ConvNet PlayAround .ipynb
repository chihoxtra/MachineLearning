{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"This is my first attempt to put together a ConvNet without using TensorFlow\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as Image\n",
    "from random import randint\n",
    "import json\n",
    "\n",
    "#%matplotlib inline\n",
    "#plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "#plt.rcParams['image.interpolation'] = 'nearest'\n",
    "#plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yToVector(y, C=6):\n",
    "    # convert the Y value into a vector of dimension C x 1\n",
    "    m = y.shape[0]\n",
    "    y = np.eye(6)[y].T.reshape(C,m) \n",
    "    #each row of eye matrix corresponds to the row vector of Y and [Y} specify which row to take\n",
    "    #and then transpose it to a column vector and reshape it\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData ():\n",
    "    # Training data\n",
    "    #read for h5 datafile and convert into dictionary\n",
    "    train_dataset = h5py.File('convdata/train_signs.h5', 'r')\n",
    "    #for keys in test_dataset: print(keys) #print all the keys from the dict var\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_x_orig = train_set_x_orig/255 #normalize X\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    train_set_y_orig = yToVector(train_set_y_orig) #vectorize y\n",
    "    list_classes = np.array(train_dataset[\"list_classes\"][:]) # your train set labels  \n",
    "    \n",
    "    # Test Data\n",
    "    test_dataset = h5py.File('convdata/test_signs.h5', 'r')\n",
    "    #for keys in test_dataset: print(keys) #print all the keys from the dict var\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your train set features\n",
    "    test_set_x_orig = test_set_x_orig/255 #normalize X\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your train set labels\n",
    "    test_set_y_orig = yToVector(test_set_y_orig) #vectorize y \n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, list_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showFilters(params, key='CN_W1'):\n",
    "    \n",
    "    p = params[key]\n",
    "    \n",
    "    fig, axarr = plt.subplots(1, p.shape[3],  figsize=(16, 12))\n",
    "\n",
    "    for i in range(p.shape[3]):\n",
    "        pgray = np.zeros([p.shape[0], p.shape[1], p.shape[2]])\n",
    "        pgray = rgb2gray(p[:,:,:,i])\n",
    "        axarr[i].set_title(key)\n",
    "        axarr[i].imshow(pgray, cmap = plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "\n",
    "    gray = np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showImage(imageArr, title = ''):\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.imshow(imageArr,)\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padToKeepDim(inDim, fDim, stride=1):\n",
    "    outdim = inDim\n",
    "    p = int((((outdim - 1) * stride) + fDim - inDim)/2)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calOutDim(inDim, fDim, padding, stride):\n",
    "\n",
    "    outDim = int((inDim + 2*padding - fDim)/stride) +  1\n",
    "    \n",
    "    return outDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addPadding(arr, p):\n",
    "    m, h, w, c = arr.shape\n",
    "    \n",
    "    padded = np.zeros([m, (h + 2*p) , (w + 2*p), c])\n",
    "    padded[ :, p:(p+h), p:(p+w), :] = arr[:,:,:,:]\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
    "    s = W * a_slice_prev\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = float(Z + b)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, nC_prev, noOfFilters) = W.shape\n",
    "    \n",
    "    # b would be in shape (1,1,1,noOfFilters)\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" \n",
    "    s = hparameters['stride']\n",
    "    p = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume \n",
    "    nH = calOutDim(nH_prev, f, p, s)\n",
    "    nW = calOutDim(nW_prev, f, p, s)\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. \n",
    "    Z = np.zeros([m, nH, nW, noOfFilters])\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = addPadding(A_prev, p)\n",
    "    \n",
    "    for i in range(m):                  # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]    # Select ith training example's padded activation\n",
    "        #print(\"conv forward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        for h in range(nH):                 # loop over vertical axis of the output volume\n",
    "            for w in range(nW):             # loop over horizontal axis of the output volume\n",
    "                for c in range(noOfFilters):        # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad \n",
    "                    a_slice_prev = a_prev_pad[h*s : h*s + f, w*s : w*s + f ,:]\n",
    "                    \n",
    "                    # For each slice, convolve the slice with the each filter W and b\n",
    "                    # ie fill the output matrix across filter using each slice\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
    "                                        \n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, nH, nW, noOfFilters))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_forward(Z, hparameters, mode = \"max\"):\n",
    "\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, nH_prev, nW_prev, nC_prev) = Z.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    s = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    nH = calOutDim(nH_prev, f, 0, s)\n",
    "    nW = calOutDim(nW_prev, f, 0, s)\n",
    "    nC = nC_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, nH, nW, nC))              \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        #print(\"pool forward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        for h in range(nH):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(nW):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (nC):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c.\n",
    "                    z_prev_slice = Z[i,h*s:h*s + f, w*s:w*s + f,c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. \n",
    "                    # Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(z_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(z_prev_slice)\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (Z, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, nH, nW, nC))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Take dZ which is m, nH, nW, nC_prev and calculate:\n",
    "    dA (for the slice) m, f, f, nC_prev by adding W along nH, nW and for each corresponding slot, muliply by  \n",
    "    corresponding right dZ\n",
    "    dW same calculation except adding up the right slice of A_prev instead of W, then for each one \n",
    "    muliply by the corresponding dZ\n",
    "    dB just add up all the dZ to form a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, nC_prev, noOfFilters) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    s = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros(list(A_prev.shape))                     \n",
    "    dW = np.zeros(list(W.shape))   \n",
    "    db = np.zeros(list(b.shape) )  \n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = addPadding(A_prev, pad)\n",
    "    dA_prev_pad = addPadding(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]\n",
    "        da_prev_pad = dA_prev_pad[i,:,:,:]\n",
    "        \n",
    "        #print(\"conv backward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[h*s:h*s+f,w*s: w*s+f,:]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    #scan out the da\n",
    "                    da_prev_pad[h*s : h*s+f, w*s : w*s+f, :] += W[:,:,:,c]*dZ[i,h,w,c]\n",
    "                    #repeatedly darken the dW\n",
    "                    dW[:,:,:,c] += a_slice*dZ[i,h,w,c]  #note that DZ[i,h,w,c] is a scalar\n",
    "                    #add all up\n",
    "                    db[:,:,:,c] += dZ[i,h,w,c] #broadcast single value to 1,1,1\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad,pad:-pad,:] #unpad -pad is padth index from the row/col end\n",
    "\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    #take the previous input layer to identify which \"slot\" the backward prop should be allocated to\n",
    "    \n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute_value(da, shape):\n",
    "    \"\"\"\n",
    "    distribute the value of da to the corresponding slice of Z and pass backward\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (nH, nW) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = 1/(nH * nW)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    z = np.ones([nH, nW])*da*average\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    #backward prop for the pooling layer: max or average\n",
    "    \n",
    "    # Retrieve information from cache for identifying slots to pass the backward prop\n",
    "    (Z, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" to \"locate\" previous slice\n",
    "    s = hparameters['stride']\n",
    "    f = hparameters['f']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, nH_Z, nW_Z, nC_Z = Z.shape\n",
    "    m, nH, nW, nC = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dZ = np.zeros([m, nH_Z, nW_Z, nC_Z])\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev\n",
    "        z = Z[i,:,:,:]\n",
    "        \n",
    "        #print(\"pool backward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        \n",
    "        for h in range(nH):                   # loop on the vertical axis of the current layer\n",
    "            for w in range(nW):               # loop on the horizontal axis of the current layer\n",
    "                for c in range(nC):           # loop over the channels of the current layer\n",
    "                    \n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        z_slice = z[h*s : h*s+f, w*s : w*s+f, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(z_slice)\n",
    "                        #mask =  mask/np.sum(mask)  #just in case there are more than one maxima\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dZ[i, h*s: h*s+f, w*s: w*s+f, c] += dA[i,h,w,c]*mask\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i,h,w,c] #get the current slow of dA\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dZ. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dZ[i, h*s: h*s+f, w*s: w*s+f, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSavedParams():\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    with open('convNet.json', 'r') as infile:\n",
    "        param_data = json.load(infile)\n",
    "    \n",
    "    for key in param_data:\n",
    "        if isinstance(param_data[key], list):\n",
    "            params[key] = np.asarray(param_data[key])\n",
    "        else:\n",
    "            params[key] = param_data[key]\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(inDim, FCLayers_dim, hparams):\n",
    "    params = {}\n",
    "    smooth_grad = hparams['smooth_grad']\n",
    "    save2File = hparams['save2File']\n",
    "    \n",
    "    if save2File == True:\n",
    "        params = loadSavedParams()\n",
    "    else:\n",
    "        if not ('CN_W1'in params):\n",
    "            keepDims = True\n",
    "            f1 = 5\n",
    "            s1 = 1\n",
    "            nc1 = 3\n",
    "            noOfFilter1 = 6\n",
    "            p1 = None\n",
    "            if keepDims == True: #override the value of p1 if KeepDims == True\n",
    "                p1 = padToKeepDim(inDim, f1, s1)\n",
    "                z1Dim = inDim\n",
    "            else:\n",
    "                z1Dim = calOutDim(inDim, f1, p1, s1)\n",
    "            params['CN_W1'] = np.random.randn(f1,f1,nc1,noOfFilter1)\n",
    "            params['CN_b1'] = np.zeros([1,1,1,noOfFilter1])\n",
    "            params['hparam_conv1'] = {'stride': s1, 'pad': p1}\n",
    "            params['hparam_pool1'] = {'stride': 2, 'f': 2}\n",
    "            a1Dim = calOutDim(z1Dim, params['hparam_pool1']['f'], 0, params['hparam_pool1']['stride'])\n",
    "    \n",
    "        if not ('CN_W2'in params):\n",
    "            keepDims = True\n",
    "            f2 = 3\n",
    "            s2 = 1\n",
    "            nc2 = noOfFilter1\n",
    "            noOfFilter2 = 16\n",
    "            p2 = None\n",
    "            if keepDims == True:\n",
    "                p2 = padToKeepDim(a1Dim, f2, s2)\n",
    "                z2Dim = a1Dim\n",
    "            else:\n",
    "                z2Dim = calOutDim(a1Dim, f2, p2, s2)\n",
    "            params['CN_W2'] = np.random.randn(f2,f2,nc2,noOfFilter2)\n",
    "            params['CN_b2'] = np.zeros([1,1,1,noOfFilter2])\n",
    "            params['hparam_conv2'] = {'stride': s2, 'pad': p2}\n",
    "            params['hparam_pool2'] = {'stride': 2, 'f': 2}\n",
    "            a2Dim = calOutDim(z2Dim, params['hparam_pool2']['f'], 0, params['hparam_pool2']['stride'])\n",
    "        \n",
    "            ### FC layers dimension ###\n",
    "            FCLayers_dim[0] = a2Dim*a2Dim*noOfFilter2\n",
    "        \n",
    "        for l in range(1, len(FCLayers_dim)):\n",
    "            if smooth_grad == True:\n",
    "                smooth_gradient_adj = np.sqrt(2/FCLayers_dim[l-1])    # to avoid vanishing or exploding grads\n",
    "            else:\n",
    "                smooth_gradient_adj = 1\n",
    "            params['FC_W' + str(l)] = np.random.randn(FCLayers_dim[l],FCLayers_dim[l-1]) * smooth_gradient_adj\n",
    "            params['FC_b' + str(l)] = np.zeros([FCLayers_dim[l],1])\n",
    "    \n",
    "    return params, FCLayers_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    cache = (A, W, b)           #linear_cache\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    a = np.maximum(0,Z)\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_forward(Z):\n",
    "    \n",
    "    Zshift = Z - np.max(Z)\n",
    "    t = np.exp(Zshift)\n",
    "    a = np.divide(t, (np.sum(t, axis=0, keepdims=True)))\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flattenArray(arr):\n",
    "    m = arr.shape[0]\n",
    "    n = arr.shape[1]*arr.shape[2]*arr.shape[3]\n",
    "    a = np.zeros([m, n])\n",
    "    for i in range(m):\n",
    "        a[i,:] = arr[i,:,:,:].reshape(n,)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, FCLayers_dim, params):\n",
    "    \"\"\"    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    FC_caches = []\n",
    "    CN_caches = ()\n",
    "    L = len(FCLayers_dim)                  # number of layers in the neural network\n",
    "    \n",
    "    #Conv Forward\n",
    "    Z1, cache_conv1 = conv_forward(X, params['CN_W1'], params['CN_b1'], params['hparam_conv1'])\n",
    "    A1, cache_pool1 = pool_forward(Z1,params['hparam_pool1'], \"max\")\n",
    "    Z2, cache_conv2 = conv_forward(A1, params['CN_W2'], params['CN_b2'], params['hparam_conv2'])\n",
    "    A2, cache_pool2 = pool_forward(Z2, params['hparam_pool2'], \"max\")\n",
    "    \n",
    "    CN_caches = (cache_conv1, cache_pool1, cache_conv2, cache_pool2, A2.shape)\n",
    "    \n",
    "    A_unrolled = unrollMatrix(A2)\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    A = A_unrolled\n",
    "    for l in range(1, L - 1):\n",
    "        A_prev = A\n",
    "                \n",
    "        A, FC_cache = linear_activation_forward(A_prev, params['FC_W' + str(l)], params['FC_b' + str(l)], \"relu\")\n",
    "        \n",
    "        FC_caches.append(FC_cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    \n",
    "    AL, FC_cache = linear_activation_forward(A, params['FC_W' + str(L-1)], params['FC_b' + str(L-1)], \"sigmoid\")   \n",
    "    \n",
    "    FC_caches.append(FC_cache)          # (linear_cache, z_activation_cache) \n",
    "        \n",
    "    assert(AL.shape == (params['FC_W' + str(L-1)].shape[0],X.shape[0]))\n",
    "    \n",
    "    return AL, FC_caches, CN_caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unrollMatrix(A):\n",
    "    m = A.shape[0]\n",
    "    unrolledA = A.transpose(1,2,3,0).reshape(-1,m)\n",
    "    return unrolledA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    ####def linear_activation_forward(A_prev, W, b, activation):    \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b)   \n",
    "        A, z_activation_cache = softmax_forward(Z)\n",
    "\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b)\n",
    "        A, z_activation_cache = relu(Z)\n",
    "            \n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    FC_cache = (linear_cache, z_activation_cache)         #linear_cache is A, W, b, activation_cache is Z\n",
    "\n",
    "    return A, FC_cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, FCLayers_dim, params, _lambda):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    Y = np.array(Y, dtype=float)     # to avoid division by zero\n",
    "    SumSqW = 0                       # for regularization\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "\n",
    "    #cost = (1/m)*np.sum(-(Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    \n",
    "    if np.sum(AL <= 0) > 0:                    #check if there is any instances, true = 1\n",
    "        AL[AL <= 0] = 1e-7\n",
    "        print(\"AL below zeros detected\")\n",
    "        \n",
    "    if np.sum(AL >= 1.0) > 0:\n",
    "        sub = 1.0 - 1e-7\n",
    "        AL[AL >= 1.0] = sub      #make it just slightly smaller than 1\n",
    "        print(\"(1 - AL) below zeros detected\")\n",
    "  \n",
    "    logprobs = np.multiply(-np.log(AL),Y) + np.multiply(-np.log(1 - AL), 1 - Y)\n",
    "\n",
    "    \n",
    "    ### Regularization ###\n",
    "    L = len(FCLayers_dim)\n",
    "    \n",
    "    for l in range(L-1): \n",
    "        SumSqW = SumSqW + np.sum(np.square(params[\"FC_W\" + str(l + 1)]))\n",
    "        L2_reg = (1./(2 * m)) * _lambda * SumSqW\n",
    "    \n",
    "    cost = 1./m * np.sum(logprobs) + L2_reg\n",
    "        \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FC_model_backward(AL, Y, FCLayers_dim, FC_caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(FCLayers_dim) - 1 # the number of layers\n",
    "    \n",
    "    m = AL.shape[1] # A or Z retains the dimension of number of training examples m\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    noOfClass = Y.shape[0]\n",
    "    \n",
    "    dAL = -1 * np.divide( Y, AL ) \n",
    "    \n",
    "    current_cache = FC_caches[L-1]   # contains of linear cache (A, W, b,) and activation cache (Z)\n",
    "    \n",
    "    ### first backpropagation :-> sigmoid\n",
    "    grads[\"FC_dA\" + str(L-1)], grads[\"FC_dW\" + str(L)], grads[\"FC_db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"FC_dA\" + str(l+1)], FC_caches[l] , 'relu')\n",
    "        \n",
    "        grads[\"FC_dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"FC_dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"FC_db\" + str(l+1)] = db_temp\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    #dJ_dZ = dJ_dA * dA_dZ\n",
    "    #dA_dZ = 0 when Z <=0\n",
    "    #dA_dZ = 1 when Z > 0\n",
    "    #dJ_dZ = 0 when z <=0; = dJ_dA when Z > 0\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0 \n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_backward_extended(dA, cache):\n",
    "    \n",
    "    Z = cache    \n",
    "    a, _ = softmax_forward(Z)\n",
    "    needVerify = False\n",
    "    useForLoop = False\n",
    "    \n",
    "    assert(Z.shape == dA.shape)\n",
    "    \n",
    "    noOfClass, m = Z.shape\n",
    "    dJdZ = np.zeros([noOfClass, m])\n",
    "    \n",
    "    for k in range(m):\n",
    "        dAdZMatrix = np.zeros((noOfClass, noOfClass))\n",
    "        \n",
    "        if useForLoop == False:\n",
    "            dAdZMatrix = -np.outer(a[:, k], a[:, k]) + np.diag(a[:, k].flatten())\n",
    "        else:\n",
    "            dAdZ_forLoop = np.zeros((noOfClass, noOfClass))\n",
    "        \n",
    "            for i in range(noOfClass):\n",
    "                for j in range(noOfClass):\n",
    "                    dAdZ_forLoop[i, j] = a[i, k] * ((i == j) - a[j, k])\n",
    "            \n",
    "            dAdZMatrix = dAdZ_forLoop\n",
    "        \n",
    "        if needVerify == True and useForLoop == True:\n",
    "            if (np.sum(dAdZ_forLoop) - np.sum(dAdZMatrix)) > 1e-15:\n",
    "                print(\"difference between dAdZ_forLoop and Matrix is too big\")\n",
    "        \n",
    "            assert(dAdZMatrix.shape ==  dAdZ_forLoop.shape)\n",
    "        \n",
    "        assert(dAdZMatrix.shape == (noOfClass,noOfClass))\n",
    "\n",
    "        new_vector = np.sum ( (dA[:,k].reshape(noOfClass,1) * dAdZMatrix).T, axis=1, keepdims=True)\n",
    "    \n",
    "        if k == 0:\n",
    "            dJdZMatrix = new_vector\n",
    "\n",
    "        else:\n",
    "            dJdZMatrix = np.concatenate((dJdZMatrix, new_vector), axis=1)\n",
    "    \n",
    "    #hardcoded answer\n",
    "    dJdZa = a + dA*a\n",
    "    \n",
    "    #if np.sum(dJdZMatrix) - np.sum(dJdZa) > 1e-10:\n",
    "        #print(\"difference between dJdZMatrix and hardcode calculation is too big\")\n",
    "    \n",
    "    return dJdZMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "       \n",
    "    dW = 1./m * np.dot(dZ, A_prev.T)   \n",
    "    dA_prev = np.dot(W.T, dZ)    \n",
    "        \n",
    "    db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    assert (db.shape == b.shape)\n",
    "        \n",
    "        \n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_params(m, FCLayers_dim, params, grads, alpha, _lambda):\n",
    "    \n",
    "    for l in range(2 ):\n",
    "        \n",
    "        params[\"CN_W\" + str(l+1)] = params[\"CN_W\" + str(l+1)] - alpha * (grads[\"CN_dW\" + str(l+1)]  \n",
    "                                                 + (params[\"CN_W\" + str(l+1)] * (_lambda/m))) \n",
    "        params[\"CN_b\" + str(l+1)] = params[\"CN_b\" + str(l+1)] - alpha * grads[\"CN_db\" + str(l+1)]\n",
    "    \n",
    "    L = len(FCLayers_dim)\n",
    "    \n",
    "    for l in range(L-1): \n",
    "\n",
    "        params[\"FC_W\" + str(l+1)] = params[\"FC_W\" + str(l+1)] - alpha * (grads[\"FC_dW\" + str(l+1)] \n",
    "                                                                         + (params[\"FC_W\" + str(l+1)] * (_lambda/m)) )  \n",
    "        params[\"FC_b\" + str(l+1)] = params[\"FC_b\" + str(l+1)] - alpha * grads[\"FC_db\" + str(l+1)]\n",
    "                \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        dZ = softmax_backward_extended(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "                \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict2vector(_dict, skip_term, include_term = \"_\"):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    len_no_term = sum(1 for i in _dict if skip_term not in i and include_term in i)  # find the length of vector without hparam*\n",
    "    \n",
    "    no_term_key_labels = np.array(range(len_no_term*2), dtype='U8').reshape(len_no_term,2)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    dict_cache = {}\n",
    "    \n",
    "    for key, value in sorted(_dict.items()):\n",
    "        \n",
    "        if skip_term not in key and include_term in key:\n",
    "        \n",
    "            #Storing key names and dimenson\n",
    "            no_term_key_labels[count, 0] = key\n",
    "            \n",
    "            shape_str = \"\"\n",
    "            for i in range(len(value.shape)):\n",
    "                shape_str = shape_str + str(value.shape[i])\n",
    "                if i < (len(value.shape) - 1):\n",
    "                    shape_str = shape_str + \"-\"\n",
    "            no_term_key_labels[count,1] = shape_str\n",
    "\n",
    "        \n",
    "            #storing a N x 1 dimensional value vector\n",
    "            new_vector = np.reshape(_dict[key], (-1,1))\n",
    "        \n",
    "            if count == 0:\n",
    "                dict_values = new_vector\n",
    "\n",
    "            else:\n",
    "                dict_values = np.concatenate((dict_values, new_vector), axis=0)\n",
    "                \n",
    "            count = count + 1\n",
    "        else:\n",
    "            dict_cache[key] = value\n",
    "            \n",
    "        \n",
    "    return no_term_key_labels, dict_values, dict_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector2dict(keys_labels, param_values, params_cache):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    last_index = 0\n",
    "    params = {}\n",
    "    \n",
    "    for i in range(keys_labels.shape[0]):\n",
    "        \n",
    "        key = keys_labels[i][0]\n",
    "        dimlist = keys_labels[i][1].split('-')\n",
    "        \n",
    "        index_length = 1\n",
    "        this_shape = ()\n",
    "        \n",
    "        for i in range(len(dimlist)):\n",
    "            index_length = index_length * int(dimlist[i])\n",
    "            this_shape = this_shape + (int(dimlist[i]),)\n",
    "        \n",
    "        temp_array = param_values[last_index:last_index+index_length,0]\n",
    "        \n",
    "        temp_array = temp_array.reshape(*this_shape)\n",
    "        params[key] = temp_array\n",
    "        \n",
    "        last_index = last_index + index_length\n",
    "        \n",
    "    for k in params_cache:\n",
    "        params[k] = params_cache[k]\n",
    "\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_check_n(params, grads, X, Y, FCLayers_dim):\n",
    "\n",
    "    \n",
    "    # Set-up variables\n",
    "    epsilon = 1e-7\n",
    "    printdiff = True\n",
    "    \n",
    "    no_hparam_key_labels, param_values, params_cache = dict2vector(params, \"hparam\", \"CN_W2\")\n",
    "    no_dA_grad_labels, no_dA_grad_values, grads_cache = dict2vector(grads, \"dA\", \"CN_dW2\")\n",
    "    #no_hparam_key_labels, param_values, params_cache = dict2vector(params, \"hparam\")\n",
    "    #no_dA_grad_labels, no_dA_grad_values, grads_cache = dict2vector(grads, \"dA\")\n",
    "\n",
    "    num_parameters = param_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    assert (no_dA_grad_values.shape == param_values.shape)\n",
    "    \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        #print(\"Testing \" + str(i) + \"th parameter...\")\n",
    "        \n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "\n",
    "        thetaplus = np.copy(param_values)                           # Step 1\n",
    "        \n",
    "        if True:\n",
    "        \n",
    "            thetaplus[i][0] = thetaplus[i][0] + epsilon                 # Step 2\n",
    "\n",
    "            updated_params_plus = vector2dict(no_hparam_key_labels, thetaplus, params_cache)\n",
    "        \n",
    "            AL_plus, _ , _ = L_model_forward(X, FCLayers_dim, updated_params_plus)\n",
    "            J_plus[i] = compute_cost(AL_plus, Y, FCLayers_dim, params, 0.0)     # Step 3\n",
    "\n",
    "\n",
    "            # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "\n",
    "            thetaminus = np.copy(param_values)                          # Step 1\n",
    "            thetaminus[i][0] = thetaminus[i][0] - epsilon               # Step 2        \n",
    "\n",
    "            updated_params_minus = vector2dict(no_hparam_key_labels, thetaminus, params_cache)\n",
    "            \n",
    "            AL_minus, _ , _ = L_model_forward(X, FCLayers_dim, updated_params_minus)            \n",
    "            J_minus[i] = compute_cost(AL_minus, Y, FCLayers_dim, params, 0.0)   # Step 3\n",
    "\n",
    "        \n",
    "            # Compute gradapprox[i]\n",
    "            gradapprox[i] = (J_plus[i] - J_minus[i])/(2 * epsilon)\n",
    "    \n",
    "            # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "\n",
    "            numerator = np.linalg.norm(no_dA_grad_values[i] - gradapprox[i])                          # Step 1'\n",
    "            denominator = np.linalg.norm(no_dA_grad_values[i]) + np.linalg.norm(gradapprox[i])        # Step 2'\n",
    "            difference = np.divide(numerator, denominator)                                            # Step 3'\n",
    "\n",
    "            if printdiff == True:\n",
    "                if difference > 1e-7:\n",
    "                    print (\"\\033[93m\" + \"Gradient Check on \" + str(i) \n",
    "                           + \"th param: backward Prop error! difference = \" + str(difference) + \"\\033[0m\")\n",
    "                    print(\"grad value: \" + str(no_dA_grad_values[i]) + \"; grad approx: \" + str(gradapprox[i]))\n",
    "                    #subprocess.call([\"afplay\", \"beep-08b.wav\"])\n",
    "                else:\n",
    "                    print (\"\\033[92m\" + \"Gradient Check on \" + str(i) \n",
    "                           + \"th param: Backward Prop OKAY! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveParams(params):\n",
    "    \n",
    "    toSaveParam = {}\n",
    "    \n",
    "    for key in params:\n",
    "        if isinstance(params[key], np.ndarray):\n",
    "            toSaveParam[key] = params[key].tolist()\n",
    "        else:\n",
    "            toSaveParam[key] = params[key]\n",
    "    \n",
    "    with open('convNet.json', 'w') as outfile:\n",
    "        json.dump(toSaveParam, outfile)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showOutcome(X, Y, outcome, noOfexamples = 5, showWrongOnly = True):\n",
    "\n",
    "    wrongSlot = (np.argmax(outcome, axis=0) != np.argmax(Y, axis=0) )*1 # find the slot where prediction is wrong\n",
    "    posWrongSlot = list(np.where(wrongSlot==1)[0])\n",
    "         \n",
    "    fig, axarr = plt.subplots(1, noOfexamples, figsize=(16, 12))\n",
    "    \n",
    "    for p in range(noOfexamples):\n",
    "        pos = randint(0,len(posWrongSlot)-1)\n",
    "        i = posWrongSlot[pos]\n",
    "        t = \"Y: \" + str(np.argmax(Y[:,i])) + \" ; predict: \" + str(np.argmax(outcome[:,i]))\n",
    "\n",
    "        axarr[p].set_title(t,  fontsize=14)\n",
    "        axarr[p].imshow(X[i,:,:,:])\n",
    " \n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calAccuracy(AL, Y):\n",
    "    \n",
    "    accuracy = (sum(np.argmax(Y, axis=0) == np.argmax(AL, axis=0))/Y.shape[1]) * 100\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(X, Y, params):\n",
    "        \n",
    "    #Conv and FC Forward\n",
    "    AL, _ , _ = L_model_forward(X, FCLayers_dim, params)\n",
    "    accuracy = calAccuracy(AL, Y)\n",
    "    \n",
    "    print(\"Training Accuracy on this data set is: \" + str(accuracy) + \"%.\")\n",
    "    \n",
    "    return AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_model_run(X, Y, params, hparams):\n",
    "\n",
    "    _lambda = hparams['_lambda']\n",
    "    alpha = hparams['alpha']\n",
    "    noOfIterations = hparams['noOfIterations']\n",
    "    checkGradient = hparams['checkGradient']\n",
    "\n",
    "    for i in range(noOfIterations):\n",
    "        #start of one epoch\n",
    "        \n",
    "        #Conv and FC Forward\n",
    "        AL, FC_caches, CN_caches = L_model_forward(X, FCLayers_dim, params)\n",
    "\n",
    "        #Cost compute    \n",
    "        cost = compute_cost(AL, Y, FCLayers_dim, params, _lambda)\n",
    "        print(\"Cost: \" + str(cost) + \" after \" + str(i+1) + \" iterations\")\n",
    "        \n",
    "        cache_conv1, cache_pool1, cache_conv2, cache_pool2, A2shape = CN_caches\n",
    "        \n",
    "        # FC backward\n",
    "        grads = FC_model_backward(AL, Y, FCLayers_dim, FC_caches)\n",
    "        \n",
    "        # Undo the unrolling\n",
    "        grads['FC_dA0'] = rollMatrix(grads['FC_dA0'], A2shape)\n",
    "        \n",
    "        #Conv backward\n",
    "        CN_dZ2 = pool_backward(grads['FC_dA0'], cache_pool2, mode = \"max\")\n",
    "        #print(np.sum(CN_dZ2 == 0))\n",
    "        grads['CN_dA1'], grads['CN_dW2'], grads['CN_db2'] = conv_backward(CN_dZ2, cache_conv2)\n",
    "        \n",
    "        CN_dZ1 = pool_backward(grads['CN_dA1'], cache_pool1, mode = \"max\")\n",
    "        grads['CN_dA0'], grads['CN_dW1'], grads['CN_db1'] = conv_backward(CN_dZ1, cache_conv1)\n",
    "        \n",
    "        assert(grads['CN_dA0'].shape == X.shape)\n",
    "        \n",
    "        if checkGradient == True and (i+1)%1 == 0:\n",
    "            diff = gradient_check_n(params, grads, X, Y, FCLayers_dim)\n",
    "        \n",
    "        #update params\n",
    "        params = update_params(X.shape[0], FCLayers_dim, params, grads, alpha, _lambda)\n",
    "    \n",
    "        # end of one epoch\n",
    "    return params, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollMatrix(theVector, A2shape):\n",
    "\n",
    "    rolledMatrix = theVector.transpose(1,0).reshape(A2shape[0],A2shape[1],A2shape[2],A2shape[3])\n",
    "    \n",
    "    assert(rolledMatrix.shape == A2shape)\n",
    "    \n",
    "    return rolledMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.222235487949 after 1 iterations\n",
      "\u001b[93mGradient Check on 0th param: backward Prop error! difference = 0.702464486309\u001b[0m\n",
      "grad value: [ 0.18421144]; grad approx: [ 0.03219418]\n",
      "\u001b[93mGradient Check on 1th param: backward Prop error! difference = 0.683889899905\u001b[0m\n",
      "grad value: [-0.07126445]; grad approx: [-0.0133782]\n",
      "\u001b[93mGradient Check on 2th param: backward Prop error! difference = 0.698566810156\u001b[0m\n",
      "grad value: [-0.48326544]; grad approx: [-0.08576186]\n",
      "\u001b[93mGradient Check on 3th param: backward Prop error! difference = 0.708369048035\u001b[0m\n",
      "grad value: [ 0.12963974]; grad approx: [ 0.02213044]\n",
      "\u001b[93mGradient Check on 4th param: backward Prop error! difference = 0.729037191503\u001b[0m\n",
      "grad value: [-0.04511126]; grad approx: [-0.00706953]\n",
      "\u001b[93mGradient Check on 5th param: backward Prop error! difference = 0.718123579567\u001b[0m\n",
      "grad value: [ 0.3332183]; grad approx: [ 0.054668]\n",
      "\u001b[93mGradient Check on 6th param: backward Prop error! difference = 0.696591092344\u001b[0m\n",
      "grad value: [ 0.35442797]; grad approx: [ 0.06338393]\n",
      "\u001b[93mGradient Check on 7th param: backward Prop error! difference = 0.704695700013\u001b[0m\n",
      "grad value: [-0.36270812]; grad approx: [-0.0628319]\n",
      "\u001b[93mGradient Check on 8th param: backward Prop error! difference = 0.695957386009\u001b[0m\n",
      "grad value: [ 0.39773052]; grad approx: [ 0.0713031]\n",
      "\u001b[93mGradient Check on 9th param: backward Prop error! difference = 0.70383777212\u001b[0m\n",
      "grad value: [ 0.37956086]; grad approx: [ 0.06597552]\n",
      "\u001b[93mGradient Check on 10th param: backward Prop error! difference = 0.60919649782\u001b[0m\n",
      "grad value: [-0.05141188]; grad approx: [-0.0124857]\n",
      "\u001b[93mGradient Check on 11th param: backward Prop error! difference = 1.0\u001b[0m\n",
      "grad value: [ 0.00759142]; grad approx: [-0.00133527]\n",
      "\u001b[93mGradient Check on 12th param: backward Prop error! difference = 0.704525036216\u001b[0m\n",
      "grad value: [ 0.24030843]; grad approx: [ 0.04165684]\n",
      "\u001b[93mGradient Check on 13th param: backward Prop error! difference = 0.679174485435\u001b[0m\n",
      "grad value: [ 0.38450294]; grad approx: [ 0.07346369]\n",
      "\u001b[93mGradient Check on 14th param: backward Prop error! difference = 0.684656977375\u001b[0m\n",
      "grad value: [-0.10338471]; grad approx: [-0.0193521]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-2a2a50b69dfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFCLayers_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFCLayers_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_model_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save2File'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-904da545593f>\u001b[0m in \u001b[0;36mclass_model_run\u001b[0;34m(X, Y, params, hparams)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheckGradient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_check_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFCLayers_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#update params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-12613a389b6b>\u001b[0m in \u001b[0;36mgradient_check_n\u001b[0;34m(params, grads, X, Y, FCLayers_dim)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mupdated_params_minus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector2dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_hparam_key_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthetaminus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mAL_minus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFCLayers_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdated_params_minus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mJ_minus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL_minus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFCLayers_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Step 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-9012955a72f6>\u001b[0m in \u001b[0;36mL_model_forward\u001b[0;34m(X, FCLayers_dim, params)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#Conv Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mZ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CN_W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CN_b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hparam_conv1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_pool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hparam_pool1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mZ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_conv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CN_W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CN_b2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hparam_conv2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-8ea1a61b2db6>\u001b[0m in \u001b[0;36mconv_forward\u001b[0;34m(A_prev, W, b, hparameters)\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0;31m# For each slice, convolve the slice with the each filter W and b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;31m# ie fill the output matrix across filter using each slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                     \u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_single_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_slice_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig, allClasses = loadData()\n",
    "minibatch_end = 10\n",
    "X = train_x_orig[0:minibatch_end,:]\n",
    "Y = train_y_orig[:,0:minibatch_end]\n",
    "#X = train_x_orig\n",
    "#Y = train_y_orig\n",
    "FCLayers_dim = [ None, 120, 84, len(allClasses)]\n",
    "\n",
    "hparams = {}\n",
    "hparams['_lambda'] = 0\n",
    "hparams['alpha'] = 0.0001\n",
    "hparams['noOfIterations'] = 2\n",
    "hparams['checkGradient'] = True\n",
    "hparams['save2File'] = True\n",
    "hparams['smooth_grad'] = True\n",
    "hparams['showImage'] = True\n",
    "\n",
    "params, FCLayers_dim = init_params(X.shape[1], FCLayers_dim, hparams)\n",
    "\n",
    "params, grads = class_model_run(X, Y, params, hparams)\n",
    "\n",
    "if hparams['save2File'] == True:\n",
    "    saveParams(params)\n",
    "\n",
    "outcome = make_prediction(X, Y, params)\n",
    "\n",
    "if hparams['showImage'] == True:\n",
    "    showOutcome(X, Y, outcome, showWrongOnly = True)\n",
    "    showFilters(params, 'CN_W1')\n",
    "    showFilters(params, 'CN_W2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-9810e71de080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                \"stride\": 1.0}\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_forwardC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-e87f29e51830>\u001b[0m in \u001b[0;36mconv_forwardC\u001b[0;34m(X, W, b, stride, padding)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mh_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mX_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim2col_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mW_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-6d6d4a80fb20>\u001b[0m in \u001b[0;36mim2col_indices\u001b[0;34m(x, field_height, field_width, padding, stride)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding,\n\u001b[0;32m---> 29\u001b[0;31m                                stride)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_padded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-6d6d4a80fb20>\u001b[0m in \u001b[0;36mget_im2col_indices\u001b[0;34m(x_shape, field_height, field_width, padding, stride)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mj0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_height\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mj1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelpun_old/anaconda/lib/python3.6/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mtile\u001b[0;34m(A, reps)\u001b[0m\n\u001b[1;32m    879\u001b[0m                 \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m//=\u001b[0m \u001b[0mdim_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
