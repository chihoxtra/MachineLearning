{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.30f'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This is my first attempt to put together a ConvNet without using TensorFlow or Keras\"\"\"\n",
    "\"\"\"\n",
    "This program uses ConvNet to learn to recognize images of hand gestures showing numbers. \n",
    "There are 1080 x 64 x 64 x 3 training set. Here I used 3 conv net layers and 3 dense/FC layers\n",
    "Output is softmax. I tried to vectorize as much as I could so that the progam runs faster.\n",
    "contact: samuelpun@gmail.com\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as Image\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "\n",
    "#%matplotlib inline\n",
    "#plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "#plt.rcParams['image.interpolation'] = 'nearest'\n",
    "#plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%precision 30\n",
    "\n",
    "#np.random.seed(3) #FC_dW2 -> 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yToVector(y, C=6):\n",
    "    # convert the Y value into a vector of dimension C x 1\n",
    "    m = y.shape[0]\n",
    "    y = np.eye(6)[y].T.reshape(C,m) \n",
    "    #each row of eye matrix corresponds to the row vector of Y and [Y} specify which row to take\n",
    "    #and then transpose it to a column vector and reshape it\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData ():\n",
    "    # Training data\n",
    "    #read for h5 datafile and convert into dictionary\n",
    "    train_dataset = h5py.File('convdata/train_signs.h5', 'r')\n",
    "    #for keys in test_dataset: print(keys) #print all the keys from the dict var\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_x_orig = train_set_x_orig/255 #normalize X\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    train_set_y_orig = yToVector(train_set_y_orig) #vectorize y\n",
    "    list_classes = np.array(train_dataset[\"list_classes\"][:]) # your train set labels  \n",
    "    \n",
    "    # Test Data\n",
    "    test_dataset = h5py.File('convdata/test_signs.h5', 'r')\n",
    "    #for keys in test_dataset: print(keys) #print all the keys from the dict var\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your train set features\n",
    "    test_set_x_orig = test_set_x_orig/255 #normalize X\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your train set labels\n",
    "    test_set_y_orig = yToVector(test_set_y_orig) #vectorize y \n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, list_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: random_mini_batches\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :, :, :]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(m/mini_batch_size) \n",
    "    # number of mini batches of size mini_batch_size in your partitionning\n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : (k+1) * mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[(num_complete_minibatches * mini_batch_size) : m,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[:,(num_complete_minibatches * mini_batch_size) : m]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showActivationPortion(activateArray, params):\n",
    "    noOfAct = 9\n",
    "    ImagePortionList = []\n",
    "    \n",
    "    for i in range(noOfAct):\n",
    "        m, nH, nW, nC = activateArray[0]['CN_A2'].shape\n",
    "        maxarg = np.argmax(activateArray[0]['CN_A2'], axis=0)\n",
    "        neuro_pos = (random.randint(0,nH-1),random.randint(0,nW-1))\n",
    "        max_sample = maxarg[neuro_pos[0],neuro_pos[1],0]\n",
    "\n",
    "        #calculate the corresponding pos in pool 1\n",
    "        f, f, nC_prev, nC = params['CN_W2'].shape\n",
    "        p = params['hparam_conv2']['pad']\n",
    "        s = params['hparam_conv2']['stride']\n",
    "        conv2_vstart = neuro1[0]*s\n",
    "        conv2_vend = neuro1[0]*s+f\n",
    "        conv2_hstart = neuro1[1]*s\n",
    "        conv2_hend = neuro1[1]*s+f\n",
    "        #print(conv2_hend-conv2_hstart)\n",
    "\n",
    "        #calculate the corresponding pos in conv 1\n",
    "        f = params['hparam_pool1']['f']\n",
    "        s = params['hparam_pool1']['stride']\n",
    "\n",
    "        pool1_vstart = conv2_vstart*s\n",
    "        pool1_vend = conv2_vend*s\n",
    "        pool1_hstart = conv2_hstart*s\n",
    "        pool1_hend = conv2_hend*s\n",
    "\n",
    "        #print(pool1_vend-pool1_vstart)\n",
    "\n",
    "        #calculate the corresponding pos in input image\n",
    "        f, f, nC_prev, nC = params['CN_W1'].shape\n",
    "        p = params['hparam_conv1']['pad']\n",
    "        s = params['hparam_conv1']['stride']\n",
    "        conv1_vstart = pool1_vstart*s\n",
    "        conv1_vend = pool1_vend*s+f\n",
    "        conv1_hstart = pool1_hstart*s\n",
    "        conv1_hend = pool1_hend*s+f\n",
    "        #print(conv1_hend-conv1_hstart)\n",
    "\n",
    "        input_padded = addPadding(test_x_orig, p)\n",
    "        imageportion = input_padded[max_sample,conv1_vstart:conv1_vend,conv1_hstart:conv1_hend,:]\n",
    "        ImagePortionList.append(imageportion)\n",
    "    \n",
    "    fig, axarr = plt.subplots(3, 3,  figsize=(6, 6))\n",
    "\n",
    "    for i in range(noOfAct):\n",
    "        rowth = int(i/3)\n",
    "        colth = i - rowth*3\n",
    "        #print(rowth)\n",
    "        #print(colth)\n",
    "        axarr[rowth,colth].imshow(ImagePortionList[i])\n",
    "    plt.show()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showFilters(params, key='CN_W1'):\n",
    "    \n",
    "    p = params[key]\n",
    "    \n",
    "    fig, axarr = plt.subplots(1, p.shape[3],  figsize=(16, 12))\n",
    "\n",
    "    for i in range(p.shape[3]):\n",
    "        pgray = np.zeros([p.shape[0], p.shape[1], p.shape[2]])\n",
    "        pgray = rgb2gray(p[:,:,:,i])\n",
    "        axarr[i].set_title(key)\n",
    "        axarr[i].imshow(pgray, cmap = plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "\n",
    "    gray = np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showImage(imageArr, title = ''):\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.imshow(imageArr,)\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(cost_array, stitle):\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    i = cost_array.shape[0]\n",
    "    \n",
    "    plt.plot(np.arange(0,i), cost_array,'-')\n",
    "    plt.title(stitle)\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 5), dpi=100)    \n",
    "    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padToKeepDim(inDim, fDim, stride=1):\n",
    "    outdim = inDim\n",
    "    p = int((((outdim - 1) * stride) + fDim - inDim)/2)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calOutDim(inDim, fDim, padding, stride):\n",
    "\n",
    "    outDim = int((inDim + 2*padding - fDim)/stride) +  1\n",
    "    \n",
    "    return outDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addPadding(arr, p):\n",
    "    m, h, w, c = arr.shape\n",
    "    \n",
    "    padded = np.zeros([m, (h + 2*p) , (w + 2*p), c])\n",
    "    padded[ :, p:(p+h), p:(p+w), :] = arr[:,:,:,:]\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "\n",
    "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
    "    s = W * a_slice_prev\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = float(Z + b)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input2Col(A_prev, f, nC_prev, s, p, nH, nW):\n",
    "\n",
    "    m = A_prev.shape[0]\n",
    "    #print(A_prev.shape)\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. \n",
    "    col_A_prev = np.zeros([m, f, f, nC_prev, nH*nW])\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    if p == 0:\n",
    "        A_prev_pad = A_prev\n",
    "    else:\n",
    "        A_prev_pad = addPadding(A_prev, p)\n",
    "    \n",
    "    for i in range(m):                  # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]    # Select ith training example's padded activation\n",
    "        #print(\"conv forward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        for h in range(nH):                 # loop over vertical axis of the output volume\n",
    "            for w in range(nW):             # loop over horizontal axis of the output volume\n",
    "                \n",
    "                a_prev_slice = a_prev_pad[h*s : h*s + f, w*s : w*s + f , :]\n",
    "                #if i ==0 and h ==0 and w ==0:\n",
    "                    #print(a_prev_slice[:,:,0])\n",
    "                index = h * nW + w\n",
    "                col_A_prev[i, :, :, :, index] = a_prev_slice\n",
    "        \n",
    "    return col_A_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward2(A_prev, W, b, hparameters):\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    \n",
    "    #retrieve the filter dimension\n",
    "    f, f, nC_prev, noOffilters = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" \n",
    "    s = hparameters['stride']\n",
    "    p = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume \n",
    "    nH = calOutDim(nH_prev, f, p, s)\n",
    "    nW = calOutDim(nW_prev, f, p, s)\n",
    "    \n",
    "    #column-ize the input of X\n",
    "    col_A_prev = input2Col(A_prev, f, nC_prev, s, p, nH, nW)\n",
    "    \n",
    "    a = col_A_prev.transpose(0,4,1,2,3)\n",
    "    \n",
    "    Z = np.zeros([m, nH, nW, noOffilters])\n",
    "                         \n",
    "    for c in range(W.shape[3]):\n",
    "        thisW = W[:,:,:,c]\n",
    "        thisb = b[:,:,:,c]\n",
    "        \n",
    "        thisproduct = a*thisW\n",
    "        thisZ = np.sum(thisproduct, axis=(2,3,4)) + thisb\n",
    "        \n",
    "        thisSumT =  thisZ.transpose(1,2,0)\n",
    "        thisConv = thisSumT.reshape(m,nH,nW)\n",
    "                         \n",
    "        Z[:,:,:,c] = thisConv\n",
    "    \n",
    "    cache = (A_prev, W, b, hparameters, col_A_prev)\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, nH, nW, noOffilters))\n",
    "        \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, nC_prev, noOfFilters) = W.shape\n",
    "    \n",
    "    # b would be in shape (1,1,1,noOfFilters)\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" \n",
    "    s = hparameters['stride']\n",
    "    p = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume \n",
    "    nH = calOutDim(nH_prev, f, p, s)\n",
    "    nW = calOutDim(nW_prev, f, p, s)\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. \n",
    "    Z = np.zeros([m, nH, nW, noOfFilters])\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = addPadding(A_prev, p)\n",
    "    \n",
    "    for i in range(m):                  # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]    # Select ith training example's padded activation\n",
    "        #print(\"conv forward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        for h in range(nH):                 # loop over vertical axis of the output volume\n",
    "            for w in range(nW):             # loop over horizontal axis of the output volume\n",
    "                for c in range(noOfFilters):        # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad \n",
    "                    a_slice_prev = a_prev_pad[h*s : h*s + f, w*s : w*s + f ,:]\n",
    "                    \n",
    "                    # For each slice, convolve the slice with the each filter W and b\n",
    "                    # ie fill the output matrix across filter using each slice\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
    "                                        \n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, nH, nW, noOfFilters))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_forward(Z, hparameters, mode = \"max\"):\n",
    "\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, nH_prev, nW_prev, nC_prev) = Z.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    s = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    nH = calOutDim(nH_prev, f, 0, s)\n",
    "    nW = calOutDim(nW_prev, f, 0, s)\n",
    "    nC = nC_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    PooledZ = np.zeros((m, nH, nW, nC))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        #print(\"pool forward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        for h in range(nH):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(nW):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (nC):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c.\n",
    "                    z_prev_slice = Z[i,h*s:h*s + f, w*s:w*s + f,c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. \n",
    "                    # Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        PooledZ[i, h, w, c] = np.max(z_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        PooledZ[i, h, w, c] = np.mean(z_prev_slice)\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (Z, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(PooledZ.shape == (m, nH, nW, nC))\n",
    "    \n",
    "    return PooledZ, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_forward2(Z, hparameters, mode = \"max\"):\n",
    "\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, nH_prev, nW_prev, nC_prev) = Z.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    s = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    nH = calOutDim(nH_prev, f, 0, s)\n",
    "    nW = calOutDim(nW_prev, f, 0, s)\n",
    "    nC = nC_prev\n",
    "    \n",
    "    col_Z = input2Col(Z, f, nC_prev, s, 0, nH, nW)\n",
    "    \n",
    "    # Compute the pooling operation on the column\n",
    "\n",
    "    if mode == \"max\":\n",
    "        PooledColZ = np.max(col_Z, axis=(1,2))\n",
    "        #maskColZ = np.argmax(col_Z, axis=(1,2))\n",
    "    elif mode == \"average\":\n",
    "        PooledColZ = np.mean(col_Z, axis=(1,2))\n",
    "        \n",
    "    PooledZ = PooledColZ.transpose(0,2,1).reshape(m, nH, nW, nC)\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (Z, hparameters, col_Z)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(PooledZ.shape == (m, nH, nW, nC))\n",
    "    \n",
    "    return PooledZ, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Take dZ which is m, nH, nW, nC_prev and calculate:\n",
    "    dA (for the slice) m, f, f, nC_prev by adding W along nH, nW and for each corresponding slot, muliply by  \n",
    "    corresponding right dZ\n",
    "    dW same calculation except adding up the right slice of A_prev instead of W, then for each one \n",
    "    muliply by the corresponding dZ\n",
    "    dB just add up all the dZ to form a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, nC_prev, noOfFilters) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    s = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros(list(A_prev.shape))                     \n",
    "    dW = np.zeros(list(W.shape))   \n",
    "    db = np.zeros(list(b.shape) )  \n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = addPadding(A_prev, pad)\n",
    "    dA_prev_pad = addPadding(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]\n",
    "        da_prev_pad = dA_prev_pad[i,:,:,:]\n",
    "        \n",
    "        #print(\"conv backward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[h*s:h*s+f,w*s: w*s+f,:]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    #scan out the da\n",
    "                    da_prev_pad[h*s : h*s+f, w*s : w*s+f, :] += W[:,:,:,c]*dZ[i,h,w,c]\n",
    "                    #repeatedly darken the dW\n",
    "                    dW[:,:,:,c] += a_slice*dZ[i,h,w,c]  #note that DZ[i,h,w,c] is a scalar\n",
    "                    #add all up\n",
    "                    db[:,:,:,c] += dZ[i,h,w,c] #broadcast single value to 1,1,1\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad,pad:-pad,:] #unpad -pad is padth index from the row/col end\n",
    "\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    db = 1./m * db\n",
    "    dW = 1./m * dW\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward2(dZ, cache):\n",
    "    \"\"\"\n",
    "    Take dZ which is m, nH, nW, nC and calculate:\n",
    "    dA (for the slice) m, f, f, nC_prev by adding W along nH, nW and for each corresponding slot, muliply by  \n",
    "    corresponding right dZ\n",
    "    dW same calculation except adding up the right slice of A_prev instead of W, then for each one \n",
    "    muliply by the corresponding dZ\n",
    "    dB just add up all the dZ to form a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters, col_A_prev_pad) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, nC_prev, noOfFilters) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    s = hparameters['stride']\n",
    "    p = hparameters['pad']\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, nH, nW, nC) = dZ.shape\n",
    "    #nHW = nH*nW\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros(list(A_prev.shape))                     \n",
    "    dW = np.zeros(list(W.shape))   \n",
    "    db = np.zeros(list(b.shape))  \n",
    "\n",
    "    # add pad to dA_prev for matching the dimension \n",
    "    dA_prev_pad = addPadding(dA_prev, p)\n",
    "    \n",
    "    # Part I: Compute: dA_prev\n",
    "    # Transform A_prev into a colm matrix, including padding\n",
    "    #col_A_prev_pad = input2Col(A_prev, f, nC_prev, s, p, nH, nW)\n",
    "    # shape = m, f, f, nC_prev, nH * nW\n",
    "    \n",
    "    # Columize dZ, W\n",
    "    dZ_squeezed = dZ.reshape(m, nH*nW, nC)\n",
    "    W_squeezed = W.reshape(f*f*nC_prev ,noOfFilters).transpose(1,0)\n",
    "    \n",
    "    # dot products between dZ and @\n",
    "    dotprod = np.zeros([nH*nW, m, f*f*nC_prev])\n",
    "    \n",
    "    #print(dotproducts_arr.shape)\n",
    "    for colslot in range(nH*nW):\n",
    "        dotprod[colslot,:,:] = np.dot(dZ_squeezed[:,colslot,:], W_squeezed[:,:])\n",
    "    \n",
    "    #print(dotprod.shape)\n",
    "    dotprod_unroll = dotprod.transpose(1,0,2).reshape(m, nH*nW, f, f, nC_prev)\n",
    "\n",
    "    for colslot in range(nH*nW):\n",
    "        vstart = int(colslot/nH)\n",
    "        hstart = colslot - vstart*nH\n",
    "        #assign the corresponding \"slot\" of col matrix to the DZ canvas\n",
    "        dA_prev_pad[:,vstart*s:vstart*s+f,hstart*s:hstart*s+f,:] += dotprod_unroll[:,colslot,:,:,:]\n",
    "    \n",
    "    dA_prev = dA_prev_pad[:,p:-p,p:-p,:]\n",
    "    \n",
    "    # Part II: Compute: dW\n",
    "    # reshape col_A_prev_pad\n",
    "    col_A_prev_pad_squeezed = col_A_prev_pad.transpose(1,2,3,0,4).reshape(f, f, nC_prev, m * nH*nW)\n",
    "    \n",
    "    # reshape dZ\n",
    "    col_dZ_squeezed = dZ.reshape(m * nH*nW, nC)\n",
    "    \n",
    "    dW = 1./m * np.dot(col_A_prev_pad_squeezed,col_dZ_squeezed)\n",
    "    \n",
    "    db = 1./m * np.sum(dZ, axis=(0,1,2))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    #take the previous input layer to identify which \"slot\" the backward prop should be allocated to\n",
    "    \n",
    "    mask = (x == np.max(x))\n",
    "    if np.sum(mask) > 1:\n",
    "        print(\"More than one local maxima\")\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute_value(da, shape):\n",
    "    \"\"\"\n",
    "    distribute the value of da to the corresponding slice of Z and pass backward\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (nH, nW) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = 1/(nH * nW)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    z = np.ones([nH, nW])*da*average\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_backward(pooled_dZ, cache, mode = \"max\"):\n",
    "    #backward prop for the pooling layer: max or average\n",
    "    \n",
    "    # Retrieve information from cache for identifying slots to pass the backward prop\n",
    "    (Z, hparameters, _) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" to \"locate\" previous slice\n",
    "    s = hparameters['stride']\n",
    "    f = hparameters['f']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, nH_Z, nW_Z, nC_Z = Z.shape\n",
    "    m, nH, nW, nC = pooled_dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dZ = np.zeros([m, nH_Z, nW_Z, nC_Z])\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev\n",
    "        z = Z[i,:,:,:]\n",
    "        \n",
    "        #print(\"pool backward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        \n",
    "        for h in range(nH):                   # loop on the vertical axis of the current layer\n",
    "            for w in range(nW):               # loop on the horizontal axis of the current layer\n",
    "                for c in range(nC):           # loop over the channels of the current layer\n",
    "                    \n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        z_slice = z[h*s : h*s+f, w*s : w*s+f, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(z_slice)\n",
    "                        #mask =  mask/np.sum(mask)  #just in case there are more than one maxima\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dZ[i, h*s: h*s+f, w*s: w*s+f, c] += pooled_dZ[i,h,w,c]*mask\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        pooled_dz = pooled_dZ[i,h,w,c] #get the current slow of dA\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dZ. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dZ[i, h*s: h*s+f, w*s: w*s+f, c] += distribute_value(pooled_dz, shape)\n",
    "                        \n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    #print(dZ.shape)\n",
    "    #print(np.sum(dZ == 0))\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward2(pooled_dZ, cache, mode = \"max\"):\n",
    "    #backward prop for the pooling layer: max or average\n",
    "    \n",
    "    # Retrieve information from cache for identifying slots to pass the backward prop\n",
    "    (Z, hparameters, col_Z) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" to \"locate\" previous slice\n",
    "    s = hparameters['stride']\n",
    "    f = hparameters['f']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, nH_Z, nW_Z, nC_Z = Z.shape\n",
    "    m, nH, nW, nC = pooled_dZ.shape\n",
    "    m, f, f, nC, nHW = col_Z.shape\n",
    "    \n",
    "    #convert dZ to column\n",
    "    #col_pooled_dZ = pooled_dZ.reshape(m,nH,nW,nC,1,1).transpose(0,4,5,1,2,3).reshape(m,1,1,nC,nHW)\n",
    "    col_pooled_dZ = pooled_dZ.transpose(0,3,1,2).reshape(m,nC,nHW).reshape(m,1,1,nC,nHW)\n",
    "    \n",
    "    if mode == \"max\":   \n",
    "        # Create mask for colZ\n",
    "        maskZ = (np.amax(col_Z, axis=(1,2), keepdims=True) == col_Z)\n",
    "        \n",
    "        # Calculate col dZ\n",
    "        col_dZ = col_pooled_dZ * maskZ\n",
    "    else:\n",
    "        col_dZ = col_pooled_dZ * (1/(f*f))\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dZ = np.zeros([m, nH_Z, nW_Z, nC_Z])\n",
    "    \n",
    "    #find the corresponding h and w from a columized matrix\n",
    " \n",
    "    for colslot in range(nHW):\n",
    "        vstart = int(colslot/nH)\n",
    "        hstart = colslot - vstart*nH\n",
    "        #assign the corresponding \"slot\" of col matrix to the DZ canvas\n",
    "        dZ[:,vstart*s:vstart*s+f,hstart*s:hstart*s+f,:] += col_dZ[:,:,:,:,colslot]\n",
    "        \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    #print(dZ.shape)\n",
    "    #print(np.sum(dZ == 0))\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSavedParams():\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    with open('convNet3.json', 'r') as infile:\n",
    "        param_data = json.load(infile)\n",
    "    \n",
    "    for key in param_data:\n",
    "        if isinstance(param_data[key], list):\n",
    "            params[key] = np.asarray(param_data[key])\n",
    "        else:\n",
    "            params[key] = param_data[key]\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initConvAndPoolLayer(params, name, noOfFilter, f, s, inDim, prev_nc, PoolHParams, keepDims = True, p = None):\n",
    "    \n",
    "    if keepDims == True: #override the value of p1 if KeepDims == True\n",
    "        p = padToKeepDim(inDim, f, s)\n",
    "        z_out_Dim = inDim\n",
    "    else:\n",
    "        z_out_Dim = calOutDim(inDim, f, p, s)\n",
    "    \n",
    "    smooth_factor = np.sqrt(2/(f*f)) \n",
    "    params['CN_W' + str(name)] = np.random.randn(f,f,prev_nc,noOfFilter) * smooth_factor\n",
    "    params['CN_b' + str(name)] = np.zeros([1,1,1,noOfFilter]) + 0.001\n",
    "    params['hparam_conv' + str(name)] = {'stride': s, 'pad': p}\n",
    "    params['hparam_pool' + str(name)] = PoolHParams\n",
    "    aDim = calOutDim(z_out_Dim, params['hparam_pool'+ str(name)]['f'], 0, params['hparam_pool'+ str(name)]['stride'])\n",
    "    \n",
    "    return aDim, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(inDim, FCLayers_dim, hparams):\n",
    "    params = {}\n",
    "    smooth_grad = hparams['smooth_grad']\n",
    "    loadFile = hparams['loadFile']\n",
    "    noOfCNLayers = 3  # as defined as no of repeating CN pattern\n",
    "    \n",
    "    if loadFile == True:\n",
    "        params = loadSavedParams()\n",
    "        print(\"saved parameters loaded!\")\n",
    "    else:\n",
    "        noOfFilter1 = 6\n",
    "        noOfFilter2 = 16\n",
    "        noOfFilter3 = 32        \n",
    "        #initConvAndPoolLayer(params, name, noOfFilter, f, s, inDim, prev_nc, PoolHParams, keepDims = True, p = None)\n",
    "        a1Dim, params = initConvAndPoolLayer(params, '1', noOfFilter1, 3, 1, inDim, 3, {'stride': 2, 'f': 2})\n",
    "        a2Dim, params = initConvAndPoolLayer(params, '2', noOfFilter2, 3, 1, a1Dim, noOfFilter1, {'stride': 2, 'f': 2})\n",
    "        a3Dim, params = initConvAndPoolLayer(params, '3', noOfFilter3, 3, 1, a2Dim, noOfFilter2, {'stride': 2, 'f': 2})\n",
    "        \n",
    "        ### FC layers dimension ###\n",
    "        FCLayers_dim[0] = a3Dim*a3Dim*noOfFilter3\n",
    "        \n",
    "        for l in range(1, len(FCLayers_dim)):\n",
    "            if smooth_grad == True:\n",
    "                smooth_gradient_adj = np.sqrt(2/FCLayers_dim[l-1])    # to avoid vanishing or exploding grads\n",
    "            else:\n",
    "                smooth_gradient_adj = 1\n",
    "            params['FC_W' + str(l)] = np.random.randn(FCLayers_dim[l],FCLayers_dim[l-1]) * smooth_gradient_adj\n",
    "            params['FC_b' + str(l)] = np.zeros([FCLayers_dim[l],1]) + 0.001\n",
    "    \n",
    "    return params, FCLayers_dim, noOfCNLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    cache = (A, W, b)           #linear_cache\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    a = np.maximum(0,Z)\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_forward(Z):\n",
    "    \n",
    "    #Zshift = Z - np.max(Z)\n",
    "    t = np.exp(Z)\n",
    "    a = np.divide(t, (np.sum(t, axis=0, keepdims=True)))\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flattenArray(arr):\n",
    "    m = arr.shape[0]\n",
    "    n = arr.shape[1]*arr.shape[2]*arr.shape[3]\n",
    "    a = np.zeros([m, n])\n",
    "    for i in range(m):\n",
    "        a[i,:] = arr[i,:,:,:].reshape(n,)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, FCLayers_dim, params):\n",
    "    \"\"\"    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    FC_caches = []\n",
    "    CN_caches = {}\n",
    "    activation = {}\n",
    "    L = len(FCLayers_dim)                  # number of layers in the FC network\n",
    "    \n",
    "    A_CN = X\n",
    "    \n",
    "    for l in range(noOfCNLayers):\n",
    "        A_prev = A_CN\n",
    "        Z, CN_caches['cache_conv' + str(l+1)] = conv_forward2(A_prev, params['CN_W' + str(l+1)], \n",
    "                                            params['CN_b'+ str(l+1)], params['hparam_conv'  + str(l+1)])\n",
    "        A_CN, CN_caches['cache_pool' + str(l+1)] = pool_forward2(Z,params['hparam_pool'+str(l+1)], \"max\")\n",
    "        #A_CN, CN_caches['cache_relu' + str(l+1)] = relu(PooledZ)\n",
    "        activation['CN_A' + str(l+1)] = A_CN\n",
    "        CN_caches['LastCNAshape'] = A_CN.shape\n",
    "    \n",
    "    \n",
    "    A_unrolled = unrollMatrix(A_CN)\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    A = A_unrolled\n",
    "    for l in range(1, L - 1):\n",
    "        A_prev = A\n",
    "                \n",
    "        A, FC_cache = linear_activation_forward(A_prev, params['FC_W' + str(l)], params['FC_b' + str(l)], \"relu\")\n",
    "        \n",
    "        activation['FC_A' + str(l)] = A\n",
    "        \n",
    "        FC_caches.append(FC_cache)\n",
    "    \n",
    "    # Implement LINEAR -> softmax. Add \"cache\" to the \"caches\" list.\n",
    "    \n",
    "    AL, FC_cache = linear_activation_forward(A, params['FC_W' + str(L-1)], params['FC_b' + str(L-1)], \"softmax\")   \n",
    "    \n",
    "    activation['FC_A' + str(L-1)] = AL\n",
    "    \n",
    "    FC_caches.append(FC_cache)          # (linear_cache, z_activation_cache) \n",
    "        \n",
    "    assert(AL.shape == (params['FC_W' + str(L-1)].shape[0],X.shape[0]))\n",
    "    \n",
    "    return AL, FC_caches, CN_caches, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    ####def linear_activation_forward(A_prev, W, b, activation):    \n",
    "    \n",
    "    if activation == \"softmax\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b)   \n",
    "        A, z_activation_cache = softmax_forward(Z)\n",
    "\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b)\n",
    "        A, z_activation_cache = relu(Z)\n",
    "            \n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    FC_cache = (linear_cache, z_activation_cache)         #linear_cache is A, W, b, activation_cache is Z\n",
    "\n",
    "    return A, FC_cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, FCLayers_dim, params, _lambda):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    Y = np.array(Y, dtype=float)     # to avoid division by zero\n",
    "    SumSqW = 0                       # for regularization\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "\n",
    "    #cost = (1/m)*np.sum(-(Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    \n",
    "    \"\"\"\n",
    "    if np.sum(AL <= 0) > 0:                    #check if there is any instances, true = 1\n",
    "        AL[AL <= 0] = 1e-10\n",
    "        print(\"AL below zeros detected\")\n",
    "        \n",
    "    if np.sum(AL >= 1.0) > 0:\n",
    "        sub = 1.0 - 1e-10\n",
    "        AL[AL >= 1.0] = sub      #make it just slightly smaller than 1\n",
    "        print(\"(1 - AL) below zeros detected\")\n",
    "    \"\"\"\n",
    "\n",
    "    #logprobs = np.multiply(-np.log(AL),Y) + np.multiply(-np.log(1.0 - AL), 1.0 - Y)\n",
    "    #logprobs = (-np.log(AL) * Y) + (-np.log(1.0 - AL) * (1.0 - Y))\n",
    "    logprobs = -Y*np.log(AL)\n",
    "    \n",
    "    #print(logprobs.shape)\n",
    "    \n",
    "    ### Regularization ###\n",
    "    L = len(FCLayers_dim)\n",
    "    if _lambda > 0:\n",
    "        for l in range(L-1): \n",
    "            SumSqW = SumSqW + np.sum(np.square(params[\"FC_W\" + str(l + 1)]))\n",
    "            L2_reg = (1./(2 * m)) * _lambda * SumSqW\n",
    "    else:\n",
    "        L2_reg = 0\n",
    "    \n",
    "    cost = 1./m * np.sum(logprobs) + L2_reg\n",
    "        \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollMatrix(theVector, A2shape):\n",
    "\n",
    "    rolledMatrix = theVector.transpose(1,0).reshape(list(A2shape))\n",
    "    \n",
    "    assert(rolledMatrix.shape == A2shape)\n",
    "    \n",
    "    return rolledMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unrollMatrix(A):\n",
    "    m = A.shape[0]\n",
    "    \n",
    "    unrolledA = A.transpose(1,2,3,0).reshape(-1,m)\n",
    "    \n",
    "    return unrolledA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FC_model_backward(AL, Y, FCLayers_dim, FC_caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(FCLayers_dim) - 1 # the number of layers\n",
    "    \n",
    "    m = AL.shape[1] # A or Z retains the dimension of number of training examples m\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    noOfClass = Y.shape[0]\n",
    "    \n",
    "    dAL = -1 * np.divide( Y, AL )   #note that dAL doesnt depend on the actual cost and m\n",
    "    \n",
    "    current_cache = FC_caches[L-1]   # contains of linear cache (A, W, b,) and activation cache (Z)\n",
    "    \n",
    "    ### first backpropagation :-> softmax\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, 'softmax')\n",
    "    \n",
    "    grads[\"FC_dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"FC_dW\" + str(L)] = dW_temp\n",
    "    grads[\"FC_db\" + str(L)] = db_temp\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"FC_dA\" + str(l+1)], FC_caches[l] , 'relu')\n",
    "        \n",
    "        grads[\"FC_dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"FC_dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"FC_db\" + str(l+1)] = db_temp\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    #dJ_dZ = dJ_dA * dA_dZ\n",
    "    #dA_dZ = 0 when Z <=0\n",
    "    #dA_dZ = 1 when Z > 0\n",
    "    #dJ_dZ = 0 when z <=0; = dJ_dA when Z > 0\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0 \n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_backward_extended(dA, cache):\n",
    "    \n",
    "    Z = cache    \n",
    "    a, _ = softmax_forward(Z)\n",
    "    needVerify = True\n",
    "    useForLoop = False\n",
    "    \n",
    "    assert(Z.shape == dA.shape)\n",
    "    \n",
    "    noOfClass, m = Z.shape\n",
    "    dJdZ = np.zeros([noOfClass, m])\n",
    "    \n",
    "    for k in range(m):  #reconstruct dJdZMatrix per training example\n",
    "        dAdZMatrix = np.zeros((noOfClass, noOfClass))\n",
    "        \n",
    "        dAdZMatrix = -np.outer(a[:, k], a[:, k]) + np.diag(a[:, k].flatten())\n",
    "        \n",
    "        if useForLoop == True:\n",
    "            dAdZ_forLoop = np.zeros((noOfClass, noOfClass))\n",
    "        \n",
    "            for i in range(noOfClass):\n",
    "                for j in range(noOfClass):\n",
    "                    dAdZ_forLoop[i, j] = a[i, k] * ((i == j) - a[j, k])\n",
    "            \n",
    "            #dAdZMatrix = dAdZ_forLoop\n",
    "        \n",
    "        if needVerify == True and useForLoop == True:\n",
    "            if (np.sum(dAdZ_forLoop) - np.sum(dAdZMatrix)) > 1e-15:\n",
    "                print(\"difference between dAdZ_forLoop and Matrix is too big\")\n",
    "            else:\n",
    "                print(\"both method of backward softmax return similar results\")\n",
    "        \n",
    "            assert(dAdZMatrix.shape ==  dAdZ_forLoop.shape)\n",
    "        \n",
    "        assert(dAdZMatrix.shape == (noOfClass,noOfClass))\n",
    "\n",
    "        new_vector = np.sum ( (dA[:,k].reshape(noOfClass,1) * dAdZMatrix).T, axis=1, keepdims=True)\n",
    "    \n",
    "        if k == 0:\n",
    "            dJdZMatrix = new_vector\n",
    "\n",
    "        else:\n",
    "            dJdZMatrix = np.concatenate((dJdZMatrix, new_vector), axis=1)\n",
    "    \n",
    "    #hardcoded answer\n",
    "    hardCoded_dJdZa = a + dA*a\n",
    "    if np.sum(dJdZMatrix - hardCoded_dJdZa) > 1e-10:\n",
    "        print(\"There is an error in dJdZ first step of softmax backward\")\n",
    "    \n",
    "    return dJdZMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]   # n x m\n",
    "\n",
    "    dW = 1./m * np.dot(dZ, A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    dA_prev = np.dot(W.T, dZ) \n",
    "    \n",
    "    assert (db.shape == b.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectDeadNodes(ActivateArray, threhold = 0.0, selected_m = 0):\n",
    "    \n",
    "    for key in ActivateArray[0]:\n",
    "        if \"CN\" in key:\n",
    "            for filt in range(ActivateArray[0][key].shape[3]):\n",
    "                new_array = np.zeros([ActivateArray[0][key].shape[1],ActivateArray[0][key].shape[2]])\n",
    "                for i in range(len(ActivateArray)):\n",
    "                    #print(ActivateArray[i][key][selected_m,:,:,selected_f].shape)\n",
    "                        new_array = new_array + ActivateArray[i][key][selected_m,:,:,filt]\n",
    "                deadrate = np.sum(new_array == 0)/(new_array.shape[0]*new_array.shape[1])\n",
    "                if deadrate > 0.5: \n",
    "                    colorstr = \"\\033[93m\"\n",
    "                else:\n",
    "                    colorstr = \"\\033[92m\"\n",
    "                \n",
    "                if deadrate >= threhold:\n",
    "                    print(colorstr + str(key) + \": dead nodes rate for filter \" \n",
    "                          + str(filt+1) + \" is: \" + str(deadrate) + \"\\033[0m\")\n",
    "\n",
    "        else:\n",
    "            new_array = np.zeros([ActivateArray[0][key].shape[0],ActivateArray[0][key].shape[1]])\n",
    "            ####\n",
    "            for i in range(len(ActivateArray)):\n",
    "                #print(ActivateArray[i][key].shape)\n",
    "                new_array = new_array + ActivateArray[i][key][:,:]\n",
    "            deadrate = np.sum(new_array == 0)/(new_array.shape[0]*new_array.shape[1])\n",
    "            if deadrate > 0.5: \n",
    "                colorstr = \"\\033[93m\"\n",
    "            else:\n",
    "                colorstr = \"\\033[92m\"\n",
    "            \n",
    "            if deadrate >= threhold:\n",
    "                print(colorstr + str(key) + \": dead nodes rate is: \" + str(deadrate) + \"\\033[0m\")\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        \n",
    "        dZ = softmax_backward_extended(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "                \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_params(m, FCLayers_dim, params, grads, alpha, _lambda):\n",
    "    \n",
    "    for l in range(2 ):\n",
    "        \n",
    "        params[\"CN_W\" + str(l+1)] = params[\"CN_W\" + str(l+1)] - alpha * (grads[\"CN_dW\" + str(l+1)]  \n",
    "                                                 + (params[\"CN_W\" + str(l+1)] * (_lambda/m))) \n",
    "        params[\"CN_b\" + str(l+1)] = params[\"CN_b\" + str(l+1)] - alpha * grads[\"CN_db\" + str(l+1)]\n",
    "    \n",
    "    L = len(FCLayers_dim)\n",
    "    \n",
    "    for l in range(L-1): \n",
    "\n",
    "        params[\"FC_W\" + str(l+1)] = params[\"FC_W\" + str(l+1)] - alpha * (grads[\"FC_dW\" + str(l+1)] \n",
    "                                                                         + (params[\"FC_W\" + str(l+1)] * (_lambda/m)) )  \n",
    "        params[\"FC_b\" + str(l+1)] = params[\"FC_b\" + str(l+1)] - alpha * grads[\"FC_db\" + str(l+1)]\n",
    "                \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict2vector(_dict, skip_term, include_term = \"\"):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    len_no_term = sum(1 for i in _dict if skip_term not in i and include_term in i)  \n",
    "    # find the length of vector without hparam*\n",
    "    \n",
    "    #this term stores list of gradients and its size for checking. To be used later to restall \"params\"\n",
    "    keyLabelsInfo = np.array(range(len_no_term*2), dtype='U32').reshape(len_no_term,2)\n",
    "    \n",
    "    count = 0\n",
    "        \n",
    "    dict_cache = {}\n",
    "    \n",
    "    for key, value in sorted(_dict.items()):\n",
    "        \n",
    "        if skip_term not in key and include_term in key:\n",
    "        \n",
    "            #Storing key names and dimenson\n",
    "            keyLabelsInfo[count, 0] = key\n",
    "            \n",
    "            shape_str = \"\"\n",
    "\n",
    "            for i in range(len(value.shape)):\n",
    "                shape_str = shape_str + str(value.shape[i])\n",
    "                \n",
    "                if i < (len(value.shape) - 1):\n",
    "                    shape_str = shape_str + \"-\"\n",
    "            keyLabelsInfo[count,1] = shape_str\n",
    "        \n",
    "            #storing a N x 1 dimensional value vector\n",
    "            new_vector = np.reshape(_dict[key], (-1,1))\n",
    "            thiskeylist = [key]*new_vector.shape[0]\n",
    "        \n",
    "            if count == 0:\n",
    "                dict_values = new_vector\n",
    "                keylist = thiskeylist\n",
    "\n",
    "            else:\n",
    "                dict_values = np.concatenate((dict_values, new_vector), axis=0)\n",
    "                keylist.extend(thiskeylist)\n",
    "                \n",
    "            count = count + 1\n",
    "        else:\n",
    "            dict_cache[key] = value\n",
    "            \n",
    "    assert(len(keylist) == len(dict_values))\n",
    "                \n",
    "    return keyLabelsInfo, keylist, dict_values, dict_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector2dict(keys_labels, param_values, params_cache):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    last_index = 0\n",
    "    params = {}\n",
    "    \n",
    "    for i in range(keys_labels.shape[0]):\n",
    "        \n",
    "        key = keys_labels[i][0]\n",
    "        dimlist = keys_labels[i][1].split('-')\n",
    "        \n",
    "        index_length = 1\n",
    "        this_shape = ()\n",
    "        \n",
    "        for i in range(len(dimlist)):\n",
    "            index_length = index_length * int(dimlist[i])\n",
    "            this_shape = this_shape + (int(dimlist[i]),)\n",
    "        \n",
    "        temp_array = param_values[last_index:last_index+index_length,0]\n",
    "        \n",
    "        temp_array = temp_array.reshape(*this_shape)\n",
    "        params[key] = temp_array\n",
    "        \n",
    "        last_index = last_index + index_length\n",
    "        \n",
    "    for k in params_cache:\n",
    "        params[k] = params_cache[k]\n",
    "\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(params, grads, X, Y, FCLayers_dim, hparams):\n",
    "\n",
    "    \n",
    "    # Set-up variables\n",
    "    epsilon = 1e-7\n",
    "    printdiff = True\n",
    "    randomCheck = True\n",
    "    \n",
    "    hparam_keylabels, pkeylist, param_values, params_cache = dict2vector(params, \"hparam\", \"\")\n",
    "    no_dA_grad_labels, gkeylist, no_dA_grad_values, grads_cache = dict2vector(grads, \"dA\", \"\")\n",
    "\n",
    "    num_parameters = param_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    assert (no_dA_grad_values.shape == param_values.shape)\n",
    "    \n",
    "    # random to display results of 50 gradient checks\n",
    "    if randomCheck == True:\n",
    "        randpos = list(range(len(no_dA_grad_values)))\n",
    "        random.shuffle(randpos)\n",
    "        \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        \n",
    "        if (randomCheck == True) and (i in randpos[0:50]):\n",
    "\n",
    "            thetaplus = np.copy(param_values)                           # Step 1\n",
    "        \n",
    "            if no_dA_grad_values[i] != 0:\n",
    "        \n",
    "                thetaplus[i][0] = thetaplus[i][0] + epsilon                 # Step 2\n",
    "\n",
    "                updated_params_plus = vector2dict(hparam_keylabels, thetaplus, params_cache)\n",
    "        \n",
    "                AL_plus, _ , _ , _ = L_model_forward(X, FCLayers_dim, updated_params_plus)\n",
    "                J_plus[i] = compute_cost(AL_plus, Y, FCLayers_dim, params, 0.0)     # Step 3\n",
    "\n",
    "\n",
    "                # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "\n",
    "                thetaminus = np.copy(param_values)                          # Step 1\n",
    "                thetaminus[i][0] = thetaminus[i][0] - epsilon               # Step 2        \n",
    "\n",
    "                updated_params_minus = vector2dict(hparam_keylabels, thetaminus, params_cache)\n",
    "            \n",
    "                AL_minus, _ , _ , _ = L_model_forward(X, FCLayers_dim, updated_params_minus)            \n",
    "                J_minus[i] = compute_cost(AL_minus, Y, FCLayers_dim, params, 0.0)   # Step 3\n",
    "\n",
    "        \n",
    "                # Compute gradapprox[i]\n",
    "                gradapprox[i] = (J_plus[i] - J_minus[i])/(2 * epsilon)\n",
    "    \n",
    "                # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "\n",
    "                numerator = np.linalg.norm(no_dA_grad_values[i] - gradapprox[i])                \n",
    "                denominator = np.linalg.norm(no_dA_grad_values[i]) + np.linalg.norm(gradapprox[i]) \n",
    "                difference = np.divide(numerator, denominator)                                      \n",
    "\n",
    "                if printdiff == True:\n",
    "                    if difference > 1e-7:\n",
    "                        print (\"\\033[93m\" + \"Gradient Check on param: \" + str(gkeylist[i]) \n",
    "                               + \". backward Prop error! difference = \" + str(difference) + \"\\033[0m\")\n",
    "                        #subprocess.call([\"afplay\", \"beep-08b.wav\"])\n",
    "                    else:\n",
    "                        print (\"\\033[92m\" + \"Gradient Check on param: \" + str(gkeylist[i]) \n",
    "                               + \". Backward Prop OKAY! difference = \" + str(difference) + \"\\033[0m\")\n",
    "                    print(\"grad value: \" + str(no_dA_grad_values[i]) + \"; grad approx: \" \n",
    "                          + str(gradapprox[i]) + \"; ratio: \" + str(no_dA_grad_values[i]/gradapprox[i]))\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveParams(params, printmsg = False):\n",
    "    \n",
    "    toSaveParam = {}\n",
    "    \n",
    "    for key in params:\n",
    "        if isinstance(params[key], np.ndarray):\n",
    "            toSaveParam[key] = params[key].tolist()\n",
    "        else:\n",
    "            toSaveParam[key] = params[key]\n",
    "    \n",
    "    with open('convNet3.json', 'w') as outfile:\n",
    "        json.dump(toSaveParam, outfile)\n",
    "        if printmsg == True:\n",
    "            print(\"parameters saved!\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showOutcome(X, Y, outcome, noOfexamples = 5, showWrongOnly = True):\n",
    "\n",
    "    wrongSlot = (np.argmax(outcome, axis=0) != np.argmax(Y, axis=0) )*1 # find the slot where prediction is wrong\n",
    "    posWrongSlot = list(np.where(wrongSlot==1)[0])\n",
    "    \n",
    "    if showWrongOnly == True and len(posWrongSlot) > 0:\n",
    "         \n",
    "        fig, axarr = plt.subplots(1, noOfexamples, figsize=(16, 12))\n",
    "    \n",
    "        for p in range(noOfexamples):\n",
    "            pos = random.randint(0,len(posWrongSlot)-1)\n",
    "            i = posWrongSlot[pos]\n",
    "            t = \"Y: \" + str(np.argmax(Y[:,i])) + \" ; predict: \" + str(np.argmax(outcome[:,i]))\n",
    "\n",
    "            axarr[p].set_title(t,  fontsize=14)\n",
    "            axarr[p].imshow(X[i,:,:,:])\n",
    " \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"There is no wrong prediction.\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calAccuracy(AL, Y):\n",
    "    \n",
    "    accuracy = (sum(np.argmax(Y, axis=0) == np.argmax(AL, axis=0))/Y.shape[1]) * 100\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(X, Y, params):\n",
    "    \n",
    "    activateArray = []\n",
    "    \n",
    "    #Conv and FC Forward\n",
    "    AL, _ , _ , activation = L_model_forward(X, FCLayers_dim, params)\n",
    "    activateArray.append(activation)\n",
    "    \n",
    "    accuracy = calAccuracy(AL, Y)\n",
    "    \n",
    "    print(\"Training Accuracy on this data set is: \" + str(accuracy) + \"%.\")\n",
    "    \n",
    "    return AL, activateArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CN_model_backward(grads, CN_caches):\n",
    "    \n",
    "    dA = grads['FC_dA0']\n",
    "    \n",
    "    for l in reversed(range(noOfCNLayers)):\n",
    "        dA_prev = dA\n",
    "        #CN_Pooled_dZ = relu_backward(dA_prev, CN_caches['cache_relu' + str(l+1)])       \n",
    "        \n",
    "        CN_dZ = pool_backward2(dA_prev, CN_caches['cache_pool' + str(l+1)], mode = \"max\")\n",
    "        dA, dW, db = conv_backward2(CN_dZ, CN_caches['cache_conv' + str(l+1)])\n",
    "        \n",
    "        grads['CN_dA' + str(l)] = dA\n",
    "        grads['CN_dW' + str(l+1)] = dW\n",
    "        grads['CN_db' + str(l+1)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, FCLayers_dim, FC_caches, CN_caches):\n",
    "    \n",
    "    # FC backward\n",
    "    grads = FC_model_backward(AL, Y, FCLayers_dim, FC_caches)\n",
    "    \n",
    "    #cache_conv1, cache_pool1, cache_conv2, cache_pool2, cache_relu1, cache_relu2, A2shape = CN_caches\n",
    "    \n",
    "    # Undo the unrolling\n",
    "    grads['FC_dA0'] = rollMatrix(grads['FC_dA0'], CN_caches['LastCNAshape'])\n",
    "    \n",
    "    #Conv backward\n",
    "    \n",
    "    grads = CN_model_backward(grads, CN_caches)\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerbar(achieved, total):\n",
    "\n",
    "    powerbar = [\" \"]\n",
    "    powerbar = powerbar*total\n",
    "    powerbar[0:achieved] = \"=\"*achieved\n",
    "    powerbar = ''.join(powerbar)\n",
    "    \n",
    "    return powerbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_model_run(dataBatches, params, hparams):\n",
    "\n",
    "    _lambda = hparams['_lambda']\n",
    "    alpha = hparams['alpha']\n",
    "    noOfIterations = hparams['noOfIterations']\n",
    "    checkGradient = hparams['checkGradient']\n",
    "    printCost = hparams['printCost']\n",
    "    minibatches = hparams['minibatches']\n",
    "    costArray = np.zeros([noOfIterations,1])\n",
    "    activateArray = []  #a list of all the activation history: iterations x mini-batch size. \n",
    "    \n",
    "    if minibatches == True:\n",
    "        X = dataBatches[0][0]\n",
    "        Y = dataBatches[0][1]\n",
    "        dataBatches = random_mini_batches(X, Y, mini_batch_size = 64, seed = 0)\n",
    "        noOfbatches = len(dataBatches)\n",
    "    else:\n",
    "        noOfbatches = 1\n",
    "\n",
    "    for i in range(noOfIterations):\n",
    "        \n",
    "        ###start of one epoch\n",
    "        for d in range(noOfbatches):\n",
    "            X = dataBatches[d][0]\n",
    "            Y = dataBatches[d][1]\n",
    "        \n",
    "            #Conv and FC Forward\n",
    "            AL, FC_caches, CN_caches, activation = L_model_forward(X, FCLayers_dim, params)\n",
    "        \n",
    "            #keep track of all activations after RELU/SIGMOID/TANH\n",
    "            activateArray.append(activation)\n",
    "\n",
    "            #Cost compute    \n",
    "            cost = compute_cost(AL, Y, FCLayers_dim, params, _lambda)\n",
    "                    \n",
    "            #FC and Conv backward\n",
    "            grads = L_model_backward(AL, Y, FCLayers_dim, FC_caches, CN_caches)\n",
    "        \n",
    "            assert(grads['CN_dA0'].shape == X.shape)\n",
    "        \n",
    "            costArray[i,0] = cost\n",
    "\n",
    "            if checkGradient == True and (i+1)%1 == 0:\n",
    "                diff = gradient_check_n(params, grads, X, Y, FCLayers_dim, hparams)        \n",
    "        \n",
    "            #update params\n",
    "            params = update_params(X.shape[0], FCLayers_dim, params, grads, alpha, _lambda)\n",
    "        \n",
    "            # Print Update           \n",
    "            if minibatches == True:\n",
    "                bar = powerbar(d, noOfbatches-1)\n",
    "                status = \"Mini-batch: \" + \"|\" + bar + \"|\"\n",
    "            else:\n",
    "                status = ''\n",
    "\n",
    "            acc = calAccuracy(AL, Y)\n",
    "            print(status + \" Cost: \" + str(round(cost,7)) + \" at \" + str(i+1)\n",
    "                                    + \"th iters; accuracy: \" + str(round(acc,2)) + \"%   \", end='\\r',flush=True)\n",
    "        \n",
    "        ### end of one epoch \n",
    "        \n",
    "        if hparams['save2File'] == True:\n",
    "            saveParams(params, False)\n",
    "        \n",
    "        print('')        \n",
    "        \n",
    "    #print cost graph\n",
    "    if printCost == True:\n",
    "        plot_graph(costArray, \"cost change per iteration\")\n",
    "    \n",
    "\n",
    "    return params, grads, activateArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved parameters loaded!\n",
      "Mini-batch: |================| Cost: 0.5074587 at 1th iters; accuracy: 87.5%    \n",
      "Mini-batch: |================| Cost: 0.5533679 at 2th iters; accuracy: 80.36%   \n",
      "Mini-batch: |================| Cost: 0.4618212 at 3th iters; accuracy: 87.5%    \n",
      "Mini-batch: |================| Cost: 0.481056 at 4th iters; accuracy: 87.5%     \n",
      "Mini-batch: |================| Cost: 0.4280171 at 5th iters; accuracy: 87.5%    \n",
      "Mini-batch: |================| Cost: 0.4394667 at 6th iters; accuracy: 89.29%   \n",
      "Mini-batch: |================| Cost: 0.4087468 at 7th iters; accuracy: 87.5%    \n",
      "Mini-batch: |================| Cost: 0.4354536 at 8th iters; accuracy: 87.5%    \n",
      "Mini-batch: |================| Cost: 0.3879775 at 9th iters; accuracy: 87.5%    \n",
      "Mini-batch: |================| Cost: 0.4238423 at 10th iters; accuracy: 87.5%    \n",
      "Mini-batch: |================| Cost: 0.3699433 at 11th iters; accuracy: 87.5%    \n",
      "Mini-batch: |================| Cost: 0.3662082 at 12th iters; accuracy: 91.07%   \n",
      "Mini-batch: |================| Cost: 0.3515769 at 13th iters; accuracy: 89.29%   \n",
      "Mini-batch: |================| Cost: 0.3446828 at 14th iters; accuracy: 89.29%   \n",
      "Mini-batch: |================| Cost: 0.3440104 at 15th iters; accuracy: 91.07%   \n",
      "Mini-batch: |================| Cost: 0.3366484 at 16th iters; accuracy: 87.5%    \n",
      "Mini-batch: |================| Cost: 0.3190777 at 17th iters; accuracy: 91.07%   \n",
      "Mini-batch: |================| Cost: 0.3096447 at 18th iters; accuracy: 91.07%   \n",
      "Mini-batch: |================| Cost: 0.3124735 at 19th iters; accuracy: 91.07%   \n",
      "Mini-batch: |================| Cost: 0.2931871 at 20th iters; accuracy: 91.07%   \n"
     ]
    }
   ],
   "source": [
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig, allClasses = loadData()\n",
    "minibatch_start = 0\n",
    "minibatch_end = 1080\n",
    "X = train_x_orig[minibatch_start:minibatch_end,:]\n",
    "Y = train_y_orig[:,minibatch_start:minibatch_end]\n",
    "#X = train_x_orig\n",
    "#Y = train_y_orig\n",
    "FCLayers_dim = [ None, 120, 84, len(allClasses)]\n",
    "\n",
    "hparams = {}\n",
    "hparams['_lambda'] = 0.0\n",
    "hparams['alpha'] = 0.001\n",
    "hparams['noOfIterations'] = 20\n",
    "hparams['save2File'] = False\n",
    "hparams['loadFile'] = True\n",
    "hparams['smooth_grad'] = True\n",
    "hparams['checkGradient'] = False\n",
    "hparams['showImage'] = False\n",
    "hparams['printCost'] = False\n",
    "hparams['minibatches'] = True\n",
    "np.random.seed(10) \n",
    "\n",
    "params, FCLayers_dim, noOfCNLayers = init_params(X.shape[1], FCLayers_dim, hparams)\n",
    "\n",
    "params, grads, activateArray = class_model_run([[X, Y]] , params, hparams)\n",
    "\n",
    "outcome, activateArray = make_prediction(test_x_orig, test_y_orig, params)\n",
    "\n",
    "if hparams['showImage'] == True:\n",
    "    showOutcome(X, Y, outcome, showWrongOnly = True)\n",
    "    showFilters(params, 'CN_W1')\n",
    "    showFilters(params, 'CN_W2')\n",
    "#detectDeadNodes(activateArray, 0.8)\n",
    "#showActivationPortion(activateArray, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAFpCAYAAABj6bgoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3WuIpOd5JuD7rmOfu2c0J2k0OsSZNZGW9YFmYhxBFEKM\nZmIiFkQySrCxCDuW1wYbEpYkP8ySX/kVEqPgYXD8Q2DHsCTjFWQkR4awthYcNJpodRjJZqyDpdbI\nI8kzfa6urqpnf3SN0mr3qO9P831d1W/dFwyqrnr0fm/VU/X011X1vC8jAmZmloZSrydgZmb5cVE3\nM0uIi7qZWUJc1M3MEuKibmaWEBd1M7OEuKibmSXERd3MLCEu6mZmCXFRNzNLSKVXBx6uVWJypJ7r\nmFkWPMi0PIIYWq5W5SF37Tsgx5Yrapooj/ncc8+/HRF75f9BNDpUjanRITFam287Q6pKFT0HY1O7\npbhKOcPLhHoOMoTKzp9/oZC8AsDk+Fjs26M9Zp1OR4pbmpuVj18q6Q9YSXxws+RgZHKXHFupD0tx\nIRaXN954E5cvX5FmKz1bSd4D4G8BlAF8IyL+asPt7N5+DMASgM9FxLn3G3NypI4/+s07lMOD4h8U\n7Y7+6m+1WhlitSfoDQf2y2P+1//+P+TYyb3auKT+h9ev/uqdrxaR16nRITx4z7Q2iZI23/mGntf6\nnoNy7G/e+wdS3NQNe+QxKxl+satFihl+Wf+Xj3y8kLwCwL49u/E3f/mn0jway0tS3L//yz9LcQBQ\nr9fk2OGa9ou4KsYBwMeO3SfH7v/Qr0lxrXZbivv9P/xj+dhbvqpIlgH8HYCjAO4AcD/JjdX4KIDD\n3X8nAHxdnoH1kvOaJud1gCmnSkcAXIiIlyKiCeA7AO7dEHMvgIdjzY8ATJG8Mee5Wr5G4bymyHkd\ncEpRPwjgtXU/v969LmuM9ZcanNcUOa8Dblu//ULyBMmzJM8uNfX3tK2/rc/rYmO119OxHK3P7ez8\nQq+nYwKlqM8AOLTu55u712WNQUSciojpiJgeyfABhRWiiQLyOjqkf1Bohcgtr8B7czs5PpbrRK0Y\nSlF/EsBhkreTrAE4DuCRDTGPAPgs13wCwGxEXMx5rpavRTivKXJeB9yWp8sR0SL5JQDfw9pXpL4Z\nEc+TfLB7+0kAZ7D29agLWPuK1APFTdly5LymyXkdYNJ7IBFxBmtPhPXXnVx3OQB8Md+pWdGc1zQ5\nr4PNywSYmSWkZ59WBoC22EoMsaV/dVUcD0BzpSnHqq285bL+O7KUIVZf0iDLQgnFiNC75NT7FRW9\nk/D2Oz8ix5ar2rgrKw15zOaq/rzKsqxDP2CJqIldnW/+7GUpLsvrIMvjxVJZiqsPj8pj1sYm5NiV\npvY86DRXpLhQayV8pm5mlhQXdTOzhLiom5klxEXdzCwhLupmZglxUTczS4iLuplZQlzUzcwS4qJu\nZpaQ3q1/GwDEJil1j9BGQ+/mW13V1/2uVrTutGpVfzj1zaQhd9SG+oAWKtAJbR7tjtYhOHnjzfLR\ndx84tHVQl9r52snwvMrSJKrvfd4fnacRgZbYMdtYmJfiqhk2Cq8U0IVdGRmRx6yN6EsPqx2g8xdf\nleI6GTqVfaZuZpYQF3Uzs4S4qJuZJcRF3cwsIS7qZmYJcVE3M0vIlkWd5CGS/0ryPMnnSX55k5i7\nSc6SfLr776vFTNdyVHVek+S8Djjly9ItAH8SEedIjgN4iuTjEXF+Q9wPI+LT+U/RCuS8psl5HWBb\nnqlHxMWIONe9PA/gBQAHi56YFW7VeU2S8zrgMr2nTvI2AB8D8G+b3PxJks+QfJTknTnMzbaJ85om\n53Uwyb3qJMcA/COAr0TE3IabzwG4JSIWSB4D8F0AhzcZ4wSAEwAwPlRFdLRW3mZTa+lvipu9AkBH\nPDYAVMX25OEhveW4nKE9Wm27j5bW9r5e3nmdGKnL21+3S9pjsOvgh8QRgaDeUi9vPp5hzE6GDYLV\ncUviJsrvHfr689od593c7r1hFyK0OXdWW1Lc2GiGjZ9L+mu2KW4WXq4Py2OWMizt0V5ZluIuv/wT\nKa6VYfNzqVqRrGLtCfKtiPinjbdHxFxELHQvn8HahzV7Nok7FRHTETE9XOvdsjO2poi8jtSd117L\nK6/d29/N7eSEvvaJ9Y7y7RcC+HsAL0TEX18j5kA3DiSPdMd9J8+JWiGc1zQ5rwNMOa36DQCfAfAs\nyae71/0FgFsAICJOArgPwBdItgAsAzge6jJp1itjcF5T5LwOuC2LekQ8gS3W/oyIhwA8lNekbFss\nxBZvkDqvO5LzOuDcUWpmlhAXdTOzhLiom5klxEXdzCwhLupmZglxUTczS0jP2v8igHZLa6lebWkt\nx5223qJdpv77bGxoSIobH9FbnpGhnXzh0utS3Jvnn9KPXxCSejt1fUIKK9f1x3VhcUmOrTTUZSX0\nZQJKFf15VRGXiqjV+uPciyDKYm5rZe0xm5rUu1RjVW+Vn21qbfrDw/rSHlksvzUjxa3MX5Hioq0v\nAdIfzxYzM8uFi7qZWUJc1M3MEuKibmaWEBd1M7OEuKibmSXERd3MLCEu6mZmCXFRNzNLSM86Slki\navWaFit23nWGtM5TANg1rneyHdi3W4obihV5zJf+7z/LsQtvvSbFXXnrkjxmUcgyqnXtsV2paJv+\nLsudn0Ab83JsrV7PNQ4Ahsp6bLmsbShdybDhcZE67RYaV96SYieGtdf2/sld8vFbDb1beGhE6wIf\ny7DvaohdqgCwfPFVKa65uKgdO0MHurrx9CsknyX5NMmzm9xOkl8jeYHkMyQ/Ls/AesZ5TZPzOtiy\nnAL8VkS8fY3bjgI43P336wC+3v2v9T/nNU3O64DK6z31ewE8HGt+BGCK5I05jW2947ymyXlNmFrU\nA8D3ST5F8sQmtx8EsP6N39e711l/c17T5LwOMPXtl7siYobkPgCPk3wxIn6Q9WDdJ9gJABgXP0ix\nQuWe18lR7cNPK1QueQXem9u9uyfznKMVRDpTj4iZ7n8vATgN4MiGkBkAh9b9fHP3uo3jnIqI6YiY\nHq71xyf6g6yIvI4O+Zd1r+WV1+4Y7+Z2YizDfgHWM1sWdZKjJMevXgbwKQDPbQh7BMBnu5+qfwLA\nbERczH22lqeS85ok53XAKafL+wGcJnk1/tsR8RjJBwEgIk4COAPgGIALAJYAPFDMdC1HFQBPOK/J\ncV4H3JZFPSJeAvCRTa4/ue5yAPhivlOzgjUjYnrjlc7rjue8DjgvE2BmlpCefVpZrVSwd6/Wfk9x\n49/mktZyCwB7pvRP8g/s01qZI/TNYX/x0vNy7GpbW/5geVlvYy4KuZZbRbWk5XV1Rd9wuFLT2/S7\nb1FsPWZFa+cHgHKG2GpVfJzEuKLF6gpWLr4sxe6eGJfiJif0D1+XoS8D0uloywQMdfSlPeKS1voP\nAOWmVosW5hekuHbHG0+bmQ0kF3Uzs4S4qJuZJcRF3cwsIS7qZmYJcVE3M0uIi7qZWUJc1M3MEuKi\nbmaWEBd1M7OE9Kz/uERipKatvR0drT24UtV/R9Wo784dDa1NvZShnXtxQW9974h3q9HSW4mLUiuV\nceuY1iKOqtbK3W5ca6vNX7YyXJVjqzVtqYhaTR+zXteXKahVtdia+DopWonASEV7MlY62utrcW5O\nPv7qkh7Lttb+35nXn1utDNWyJC5Bsba2mhKY4dh6qJmZ9TsXdTOzhLiom5klxEXdzCwhLupmZglR\nNp7+MMmn1/2bI/mVDTF3k5xdF/PV4qZsOak7r0lyXgecskfpjwF8FABIlgHMADi9SegPI+LT+U7P\nCrRydS9L5zUpzuuAy/r2y28D+GlE6Ps62U7gvKbJeR1AWYv6cQD/cI3bPknyGZKPkrzzOudl28t5\nTZPzOoDkHimSNQC/B+DPN7n5HIBbImKB5DEA3wVweJMxTgA4AQC7x4YxVtU65dpNrVNytap3/kHt\n5AKwuqp1tJb0JlXMzS7Jse2SNtdmO3tHad553Ts+huGS1lE6Wh+R4sp1fTPndxYuybGd/TdJcUN1\nrfMVAIaH9Ni6GFuvZ+8ozSOv3XH+I7eTYyhBe401lptS3JLYeQoAaK/qsQV0oS/O6a/ZVkub69io\n1lVcLuvzzHKmfhTAuYj4+cYbImIuIha6l88AqJLcs0ncqYiYjojp8WG9ndoKlWteJ0aGi5+xKa47\nr93b383t5Kj+C8t6J0tRvx/X+FOO5AFybbEDkke6475z/dOzbeC8psl5HVDS2y8kRwH8DoDPr7vu\nQQCIiJMA7gPwBZItAMsAjoe8Uo31ivOaJud1sElFPSIWAdyw4bqT6y4/BOChfKdmRXNe0+S8DjZ3\nlJqZJcRF3cwsIS7qZmYJcVE3M0uIi7qZWUJc1M3MEsJefT2V5FsANltoaA8AfTfYnaEf79OtEbE3\n70Gd154rJK/ANXPbj4/B9erH+yTntWdF/VpInr26dGgqUrxPWaX4GKR4n7JK8THY6ffJb7+YmSXE\nRd3MLCH9WNRP9XoCBUjxPmWV4mOQ4n3KKsXHYEffp757T93MzD64fjxTNzOzD6hvijrJe0j+mOQF\nkn/W6/nkgeQrJJ/t7th+ttfz6YUU8wo4t85r/+qLt1+6u57/BGtrQL8O4EkA90fE+Z5O7DqRfAXA\ndET023det0WqeQUGO7fOa3/rlzP1IwAuRMRLEdEE8B0A9/Z4Tnb9nNc0Oa99rF+K+kEAr637+fXu\ndTtdAPg+yae6G/gOmlTzCgx2bp3XPibtfGQf2F0RMUNyH4DHSb4YET/o9aQsF85tmnZ8XvvlTH0G\nwKF1P9/cvW5Hi4iZ7n8vATiNtT9bB0mSeQUGPrfOax/rl6L+JIDDJG8nWQNwHMAjPZ7TdSE5SnL8\n6mUAnwLwXG9nte2Syyvg3MJ57Wt98fZLRLRIfgnA9wCUAXwzIp7v8bSu134Ap0kCa4/ztyPisd5O\naXslmldgwHPrvPa3vvhKo5mZ5aNf3n4xM7McuKibmSXERd3MLCE9+6B0amoybjywT4qtVOsFz+b9\nRactxXXarQyj6r9PyxUtTSzpYz777HNvF7Ht2XCtGpMjNSmWoBRXrZbl49eq2rHXdKQodZ5AthxA\nHHd1dVUe8ZVLlwvJKwBMjo/Fvj27pdhOR3tsl+Zm5eOXSnoeStRixTAAwMjkLjm2Uh+W4gLaZ5pv\nvPEmLl++Is1WqhYk7wHwt1j7pPsbEfFXG25n9/ZjAJYAfC4izr3fmDce2IeHT31NOTx23/wrUhwz\nZCjEJx0ANBe0J97y7DvymKzov6jGbtgvxdWGR+Qxb7v1Q68WkdfJkRo+c9d/luZQFl+kB/dphQQA\nbj50kxzLVkOKq1T0XxTVoVH9+CXtl/Ubb+hfAX/gb/5XIXkFgH17duNv/vJPpXk0lpekuH//l3+W\n4gCgXtfzMFzTHtuqGAcAHzt2nxy7/0O/JsW12toJ4+//4R/Lx97ytKK7eM/fATgK4A4A95O8Y0PY\nUQCHu/9OAPi6PAPrJec1Tc7rAFP+VlQW77kXwMOx5kcApkjemPNcLV+jcF5T5LwOOKWoK4v3pLzA\nT6pqcF5T5LwOuG399gvJEyTPkjx75crcdh7aCrQ+r0vNLB8WW79bn9vZ+YVeT8cESlFXFu+RFviJ\niFMRMR0R01NTE1nnavlqooC8jmT44MkKkVtegffmdnJ8LNeJWjGUoq4s3vMIgM9yzScAzEbExZzn\navlahPOaIud1wG15WnWtxXtIPti9/SSAM1j7etQFrH1F6oHipmw5cl7T5LwOMOlv5Yg4g7Unwvrr\nTq67HAC+mO/UrGjOa5qc18HmZQLMzBLSu0+1IoBWUwpdXZ6X4hbfflM+/NyrL8qxszMvSXGNJf3b\nAbOLK3JsfVLrKL3z7qPymEUpl8vYJX4IfuP+PVLcSEVfJqCR4VtV6rLTu3YNyWPumdC7eoPa/Zod\n0Y9fJJaImtjV+ebPXpbiSuX8l1UAAJa0x7Y+rHcA18b0L3esNLXa1mlqdSBLB7zP1M3MEuKibmaW\nEBd1M7OEuKibmSXERd3MLCEu6mZmCXFRNzNLiIu6mVlCXNTNzBLSs47S1vIC3nr2CSn2F+e1TrIr\nb+p7ObZXluVYdLTOw9KwvjTp8qy+4e4vfv62FLfSWJTHLEq9VsNt4j6hUyPa5rxs6xsvd2r6PpZL\nS9pzQN1LFQAY2p6Ta7HaOdWuKX2P1iJFBFqrWqdkY0HrAq9WqvLxKxm6T9Vu4cqI3gFcG9Ff32oH\n6PzFV6W4jvi4Az5TNzNLiou6mVlCXNTNzBLiom5mlhAXdTOzhLiom5klZMuiTvIQyX8leZ7k8yS/\nvEnM3SRnST7d/ffVYqZrOao6r0lyXgec8j31FoA/iYhzJMcBPEXy8Yg4vyHuhxHx6fynaAVyXtPk\nvA6wLc/UI+JiRJzrXp4H8AKAg0VPzAq36rwmyXkdcJneUyd5G4CPAfi3TW7+JMlnSD5K8s4c5mbb\nxHlNk/M6mORlAkiOAfhHAF+JiI27+54DcEtELJA8BuC7AA5vMsYJACeAtQ165y++Jh27VNbatBfE\n1mQA6KzqreeVstZ6vrx4RR6z0dDbflttreX44qtay/F6+ed1FLXQ5luFFtfJsOFw6KEgtVbylZUl\neczFeX3ljXKlLsUNi5s9r5dHXrvjvJvbvTfsQogPcGe1JcWNjWbY+Lmk5QsAmisNKa5c15aqAIBS\nRc+tugzJ5Zd/IsW1xPsDiGfqJKtYe4J8KyL+aePtETEXEQvdy2ew9mHNL20VHxGnImI6IqYnRrQn\ntBXHeU1TXnnt3v5ubicn9LVPrHeUb78QwN8DeCEi/voaMQe6cSB5pDvuO3lO1ArhvKbJeR1gyt8T\nvwHgMwCeJfl097q/AHALAETESQD3AfgCyRaAZQDHQ10mzXplDM5ripzXAbdlUY+IJ4D3f1MzIh4C\n8FBek7JtsRBbvEHqvO5IzuuAc0epmVlCXNTNzBLiom5mlhAXdTOzhLiom5klxEXdzCwhet9rztrt\nDuZmF7VgcZmARlNv/V9d1lu/Jya1Nm1maFHPEtwRv0Ks72NfHAZQ7mj3be4Xs1Jclm9QR4Yd51fF\nHdpZ1l8mC8vicxpAp63FBsvymEUiiLLYKl8TX7NTk3qXaqzqrfKzTa1Nf3h4RB4zi+W3ZqS4lXlt\naZFo669un6mbmSXERd3MLCEu6mZmCXFRNzNLiIu6mVlCXNTNzBLiom5mlhAXdTOzhLiom5klpHcd\npa02Zq8sSLErbW0T2yybDo/W9OB6rSrFDVX0DYKb4sa8AORfvZNDQ/qYBWl3Ophf0DolG8ta/rN0\n/bU7GTrvqtoDm+VFIjbTAgCWxK7HhQUtrmiddguNK29JsRPD2mth/+Qu+fitht4FPjSivRbGMuy7\nGmK+AGD5orYJfHNR7CruaJu0A/rG06+QfJbk0yTPbnI7SX6N5AWSz5D8uDwD6xnnNU3O62DLchLy\nWxHx9jVuOwrgcPffrwP4eve/1v+c1zQ5rwMqr/fU7wXwcKz5EYApkjfmNLb1jvOaJuc1YWpRDwDf\nJ/kUyROb3H4QwGvrfn69e531N+c1Tc7rAFPffrkrImZI7gPwOMkXI+IHWQ/WfYKdAICpkXrW/93y\nl3tebxgbznuOll0ueQXem9u9uyfznKMVRDpTj4iZ7n8vATgN4MiGkBkAh9b9fHP3uo3jnIqI6YiY\nHqtr3yix4hSR1/Fh/7Lutbzy2h3j3dxOjI0WMV3L2ZZFneQoyfGrlwF8CsBzG8IeAfDZ7qfqnwAw\nGxEXc5+t5ankvCbJeR1wytsv+wGc5tpOPRUA346Ix0g+CAARcRLAGQDHAFwAsATggWKmazmqAHjC\neU2O8zrgtizqEfESgI9scv3JdZcDwBfznZoVrBkR0xuvdF53POd1wHmZADOzhPRumYBOB3PzWtvv\n7JK24ezQkP47au+hvXJsva59+Nehfvx6Re8nH6lp3yiZnJyQxyxKp9PBkrj5cqmmtZJ3avrTtFzS\nN2lutbSNypdX9Q3Ny6sZXlLi5uPtDC3iRYrVFaxcfFmK3T0xLsVNTugfvi5DX1qj09GWCRjqrMhj\nxiWt9R8Ayk3tNbAwry2VkWn5CznSzMz6nou6mVlCXNTNzBLiom5mlhAXdTOzhLiom5klxEXdzCwh\nLupmZglxUTczS4iLuplZQnq3TEA7MC/ukr7Q0Fp5h+v6rvPVst5OXhZjG0v6buNs663nQ+L96rT0\nNuqisETURrX2//KQtvzByGiGjTcytNTPXbmiBYZ++EXxuQoAFFu/29EfywSUCIxUtPPAipiHxbk5\n+firS3os21oeOvPX2sb1l7UyVMuSuATE2tpqSmCGY+uhZmbW71zUzcwS4qJuZpYQF3Uzs4S4qJuZ\nJUTZePrDJJ9e92+O5Fc2xNxNcnZdzFeLm7LlpO68Jsl5HXDKHqU/BvBRACBZBjAD4PQmoT+MiE/n\nOz0r0MrVvSyd16Q4rwMu69svvw3gpxGh7+tkO4HzmibndQBlLerHAfzDNW77JMlnSD5K8s7rnJdt\nL+c1Tc7rAJJ7pEjWAPwegD/f5OZzAG6JiAWSxwB8F8DhTcY4AeAEAIzWKphvaBtKLzeaUtxQTdvs\nFgDKFb2jtLGidYo2lrRNZIEMnWQAmqtap+hKK3vnYd553Ts5ij17b5COXa5Xpbj6kNahCgBXLotd\nogBKFe3pz3KGc59ylk2ytdhW6F2qV+WR1+4463I7hhK0nDWWtdfsUpZNtTN0YaOjvWYqVT23i3NL\ncqy6qfnYqLapfTnDczDLmfpRAOci4ucbb4iIuYhY6F4+A6BKcs8mcaciYjoipoerelG1QuWa14kR\nbRd3K9x157V7+7u5nRx1bneCLEX9flzjTzmSB8i1xQ5IHumO+871T8+2gfOaJud1QEl//5EcBfA7\nAD6/7roHASAiTgK4D8AXSLYALAM4HlneX7CecF7T5LwONqmoR8QigBs2XHdy3eWHADyU79SsaM5r\nmpzXweaOUjOzhLiom5klxEXdzCwhLupmZglxUTczS4iLuplZQtirr6eSfAvAZgsN7QGg7wa7M/Tj\nfbo1IvbmPajz2nOF5BW4Zm778TG4Xv14n+S89qyoXwvJs1eXDk1FivcpqxQfgxTvU1YpPgY7/T75\n7Rczs4S4qJuZJaQfi/qpXk+gACnep6xSfAxSvE9ZpfgY7Oj71HfvqZuZ2QfXj2fqZmb2AfVNUSd5\nD8kfk7xA8s96PZ88kHyF5LPdHdvP9no+vZBiXgHn1nntX33x9kt31/OfYG0N6NcBPAng/og439OJ\nXSeSrwCYjoh++87rtkg1r8Bg59Z57W/9cqZ+BMCFiHgpIpoAvgPg3h7Pya6f85om57WP9UtRPwjg\ntXU/v969bqcLAN8n+VR3A99Bk2pegcHOrfPax/Stz+2DuCsiZkjuA/A4yRcj4ge9npTlwrlN047P\na7+cqc8AOLTu55u71+1oETHT/e8lAKex9mfrIEkyr8DA59Z57WP9UtSfBHCY5O0kawCOA3ikx3O6\nLiRHSY5fvQzgUwCe6+2stl1yeQWcWzivfa0v3n6JiBbJLwH4HoAygG9GxPM9ntb12g/gNElg7XH+\ndkQ81tspba9E8woMeG6d1/7WF19pNDOzfPTL2y9mZpYDF3Uzs4S4qJuZJaRnH5QO16sxOVyXYit1\nLW7qhj3y8Un995n6ucNqY0kec3VpUY5tt9vamC0tDgAuXll4u4htz3ZNTcVNNx3QgqmGiYGZyRPo\nMX0Czz9/vpC8AsDoUDWmRofEaG3O7Qwf6ZUqVTl2bGq3FFcpZyiB1POQIVTyxhtv4PLlK9Ko0j0i\neQ+Av8XaJ93fiIi/2nA7u7cfA7AE4HMRce79xpwcruMzd39EOTz23XKbFPe7n/tvUhwAVGvaLwoA\naLdaUtzM8+97l9/j5/9PXyvoyuV5Ke6td67IY/7P0//n1SLyetNNB/Cdb31DmgNL2i/WkhgHAGvL\nksjBYlyGP2gzvZrVWH3MO+74aCF5BYCp0SE8eI+4y5uYs/mGXtXre/Sm1d+89w+kuCwngpWq/kul\nVNJypp6wHL//j/Rjb3nQtVfJ3wE4CuAOAPeTvGND2FEAh7v/TgD4ujwD6yXnNU3O6wBTfp0qi/fc\nC+DhWPMjAFMkb8x5rpavUTivKXJeB5xS1JXFe1Je4CdVNTivKXJeB9y2fvuF5AmSZ0meXWqubueh\nrUDr83r5sv6+vvW/9bldbPg1uxMoRV1ZvEda4CciTkXEdERMj9T0Dx2sEE0UkNddu6Zyn6hlklte\ngffmdnTIr9mdQCnqyuI9jwD4LNd8AsBsRFzMea6Wr0U4rylyXgfcll9pvNbiPSQf7N5+EsAZrH09\n6gLWviL1QHFTthw5r2lyXgeY9D31iDiDtSfC+utOrrscAL6Y79SsaM5rmpzXweZlAszMEtLb9dQ7\nHSlsaV7rqGw2luVDVzN8UNtqNqS4K2/8TB6z3dG6VAGgVNK67qrVDN2UfaCY7vssS0kXMIMiDt8n\ny2NHAC1xyQp1aY2o1OTj336n1oEOAOWqNu7KivbaBoDmalOOzfu51W5rtRLwmbqZWVJc1M3MEuKi\nbmaWEBd1M7OEuKibmSXERd3MLCEu6mZmCXFRNzNLiIu6mVlCetdRGoFOU+uqXG2sSHHNht4dNjox\nIcfOv3NJilv4hRYHAMsZNp5ebmn3q1XRu86KpJ4p9Hw/Z1WWjs4sdyq04P7oJwWAQCe051i7o923\nyRtvlo+++8ChrYO61M7XTiNDl2iG3OpPGW3Qjth9D/hM3cwsKS7qZmYJcVE3M0uIi7qZWUJc1M3M\nEuKibmaWkC2LOslDJP+V5HmSz5P88iYxd5OcJfl0999Xi5mu5ajqvCbJeR1wyvfUWwD+JCLOkRwH\n8BTJxyPi/Ia4H0bEp/OfohXIeU2T8zrAtjxTj4iLEXGue3kewAsADhY9MSvcqvOaJOd1wGV6T53k\nbQA+BuDfNrn5kySfIfkoyTtzmJttE+c1Tc7rYJKXCSA5BuAfAXwlIuY23HwOwC0RsUDyGIDvAji8\nyRgnAJz2r3plAAAJL0lEQVQAgPHhGkpV7XdKR9ykOTK00rZb+sbPb73yUymuMb8gj9lc1Y/faIqb\n/VaH5TGvyjuvB/bvw/LSknbwkpZ/inFrsfrm2+q4ZJYxizh+9u8z5JHX7jjv5nZipC4vWdAuaRu7\n7zr4IXFEIKj36TdXxPb/DGNmadVXxy2Jzxd1I29APFMnWcXaE+RbEfFPmxxwLiIWupfPYO3Dmj2b\nxJ2KiOmImB6p9W7ZGVtTRF53TU0WPm97f3nltXv7f7xm637N7gTKt18I4O8BvBARf32NmAPdOJA8\n0h33nTwnaoVwXtPkvA4w5VfvbwD4DIBnST7dve4vANwCABFxEsB9AL5AsgVgGcDxyPL3gvXCGJzX\nFDmvA27Loh4RT2CL9SEj4iEAD+U1KdsWCxHvv/ar87ojOa8Dzh2lZmYJcVE3M0uIi7qZWUJc1M3M\nEuKibmaWEBd1M7OE9KxFrNMJLCw3pNjRSk0cVf+q7eLsFTn27Z+9LMUtLy3LY3Y6q3qseL/2/cqm\nnd7X8GiGWN3i0hLOPXVOiq3V61rckL78QX1oSI6tibH1uj5mlthqTbz/YlzRSKJUEUtGfUIKK9dH\n5eMvLIrLTwCoNMRlAt7/29rvUaro58CVirZMQq2W/3m1z9TNzBLiom5mlhAXdTOzhLiom5klxEXd\nzCwhLupmZglxUTczS4iLuplZQlzUzcwS0ruO0gisiJsvD4sbvraaWocqAFye0bpEASAai1pchs2s\n2y29o7Q+sUuKO3Dbr8pjFqW1uoo337goxY6MjWlx41ocAAyP6h2Kw+Lzr93WNxzuZNg/KMRzqlKG\njbeLRJZRrWu5WKloXcDLcucn0Ma8HCt3K4txADBU1mPLZW1D6YraoZtBfzxbzMwsF1JRJ/kKyWdJ\nPk3y7Ca3k+TXSF4g+QzJj+c/Vcub85om53WwZTn3/62IePsatx0FcLj779cBfL37X+t/zmuanNcB\nldfbL/cCeDjW/AjAFMkbcxrbesd5TZPzmjC1qAeA75N8iuSJTW4/COC1dT+/3r3uPUieIHmW5NnG\najv7bC1vued1McPyw1aYXPIKbMhthg81rXfUt1/uiogZkvsAPE7yxYj4QdaDRcQpAKcAYM/4cIbv\nCVhBcs/rwRv3O6+9l0tegQ253TPl3O4A0pl6RMx0/3sJwGkARzaEzAA4tO7nm7vXWR9zXtPkvA62\nLYs6yVGS41cvA/gUgOc2hD0C4LPdT9U/AWA2IrQvK1uvlJzXJDmvA055+2U/gNMkr8Z/OyIeI/kg\nAETESQBnABwDcAHAEoAHipmu5agC4AnnNTnO64DbsqhHxEsAPrLJ9SfXXQ4AX8x3alawZkRMb7zS\ned3xnNcB17NlAiICLfEbMMvzc1Lcucf+t3z83ZMjcuz4iNbyXG6Oy2POZ/iWyMiBQ1sHAahk2PS4\nMBEohfYtiTq1uGroyy8wCvgsL8OYLOSjRH1z5CKRQFVsa6+WtDmvruhLe1QybMDd/Utl6zErWjs/\nAJQzxFar4uMkxqn3B/AyAWZmSXFRNzNLiIu6mVlCXNTNzBLiom5mlhAXdTOzhLiom5klxEXdzCwh\nLupmZglxUTczS0jPlgmolsvYu2tSih2pV6W49tKifPzLDb1Nvyb2fo+ILb8AsGtKu+8AUB7Vlilo\nLC/IYxZlqEJ8eHdNit0txi0szcrHX17V26kr4xNSXLWiPf8AoJLhOaC23Ks70xetVirj1jFxKYyq\ntmRFu3GtHfd+2cqwnodqTXt91Wr6mPW6vkxBrarF1mraa6AkLrsA+EzdzCwpLupmZglxUTczS4iL\nuplZQlzUzcwSouxR+mGST6/7N0fyKxti7iY5uy7mq8VN2XJSd16T5LwOOGU7ux8D+CgAkCxjbdfx\n05uE/jAiPp3v9KxAK1e3PXNek+K8Drisb7/8NoCfRsSrRUzGesZ5TZPzOoCyFvXjAP7hGrd9kuQz\nJB8leed1zsu2l/OaJud1AMntbyRrAH4PwJ9vcvM5ALdExALJYwC+C+DwJmOcAHACAHaNDeP2Ww9I\nxx4Su64qZf13VGNR7z4tsSPFtZr6mBMTWpcoAIxUVqW40fKSPOZVeed1365xTNS1PLRXtfu18PYv\npDgAaFLPwc23/Ccpbnhc7/6t1vTNv6tVrZuxVM7e+J1HXrvjvJvbveNjGC5pHaWjdW1j93Jd75Z9\nZ+GSHNvZf5MUN5Rhs/bhIT22LsbW61ptI/XaluVM/SiAcxHx8403RMRcRCx0L58BUCW5Z5O4UxEx\nHRHTY8PanbHC5ZrXSXFJAyvcdee1e/u7uZ0YcW53gixF/X5c4085kgdIsnv5SHfcd65/erYNnNc0\nOa8DSvq7juQogN8B8Pl11z0IABFxEsB9AL5AsgVgGcDxiNBWwbKecV7T5LwONqmoR8QigBs2XHdy\n3eWHADyU79SsaM5rmpzXweaOUjOzhLiom5klxEXdzCwhLupmZglxUTczS4iLuplZQtirr6eSfAvA\nZgsN7QGg70a7M/Tjfbo1IvbmPajz2nOF5BW4Zm778TG4Xv14n+S89qyoXwvJs1eXDk1FivcpqxQf\ngxTvU1YpPgY7/T757Rczs4S4qJuZJaQfi/qpXk+gACnep6xSfAxSvE9ZpfgY7Oj71HfvqZuZ2QfX\nj2fqZmb2AfVNUSd5D8kfk7xA8s96PZ88kHyF5LPdHdvP9no+vZBiXgHn1nntX33x9kt31/OfYG0N\n6NcBPAng/og439OJXSeSrwCYjoh++87rtkg1r8Bg59Z57W/9cqZ+BMCFiHgpIpoAvgPg3h7Pya6f\n85om57WP9UtRPwjgtXU/v969bqcLAN8n+VR3A99Bk2pegcHOrfPax7JvU25Z3BURMyT3AXic5IsR\n8YNeT8py4dymacfntV/O1GcAHFr3883d63a0iJjp/vcSgNNY+7N1kCSZV2Dgc+u89rF+KepPAjhM\n8naSNQDHATzS4zldF5KjJMevXgbwKQDP9XZW2y65vALOLZzXvtYXb79ERIvklwB8D0AZwDcj4vke\nT+t67QdwmiSw9jh/OyIe6+2UtleieQUGPLfOa3/ri680mplZPvrl7RczM8uBi7qZWUJc1M3MEuKi\nbmaWEBd1M7OEuKibmSXERd3MLCEu6mZmCfn/srbPX1x25p8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1167b5358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "showActivationPortion(activateArray, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': 2, 'stride': 2}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['hparam_pool1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
