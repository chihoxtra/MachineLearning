{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.30f'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This is my first attempt to put together a ConvNet without using TensorFlow or Keras\"\"\"\n",
    "\"\"\"\n",
    "This program uses ConvNet to learn to recognize images of hand gestures showing numbers. \n",
    "There are 1080 x 64 x 64 x 3 training set. Here I used 3 conv net layers and 3 dense/FC layers\n",
    "Output is softmax. I tried to vectorize as much as I could so that the progam runs faster.\n",
    "contact: samuelpun@gmail.com\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as Image\n",
    "from matplotlib import gridspec\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "\n",
    "#%matplotlib inline\n",
    "#plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "#plt.rcParams['image.interpolation'] = 'nearest'\n",
    "#plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%precision 30\n",
    "\n",
    "#np.random.seed(3) #FC_dW2 -> 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yToVector(y, C=6):\n",
    "    # convert the Y value into a vector of dimension C x 1\n",
    "    m = y.shape[0]\n",
    "    y = np.eye(6)[y].T.reshape(C,m) \n",
    "    #each row of eye matrix corresponds to the row vector of Y and [Y} specify which row to take\n",
    "    #and then transpose it to a column vector and reshape it\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData ():\n",
    "    # Training data\n",
    "    #read for h5 datafile and convert into dictionary\n",
    "    train_dataset = h5py.File('convdata/train_signs.h5', 'r')\n",
    "    #for keys in test_dataset: print(keys) #print all the keys from the dict var\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_x_orig = train_set_x_orig/255 #normalize X\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    train_set_y_orig = yToVector(train_set_y_orig) #vectorize y\n",
    "    list_classes = np.array(train_dataset[\"list_classes\"][:]) # your train set labels  \n",
    "    \n",
    "    # Test Data\n",
    "    test_dataset = h5py.File('convdata/test_signs.h5', 'r')\n",
    "    #for keys in test_dataset: print(keys) #print all the keys from the dict var\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your train set features\n",
    "    test_set_x_orig = test_set_x_orig/255 #normalize X\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your train set labels\n",
    "    test_set_y_orig = yToVector(test_set_y_orig) #vectorize y \n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, list_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: random_mini_batches\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :, :, :]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(m/mini_batch_size) \n",
    "    # number of mini batches of size mini_batch_size in your partitionning\n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : (k+1) * mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[(num_complete_minibatches * mini_batch_size) : m,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[:,(num_complete_minibatches * mini_batch_size) : m]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showActivationInput(X, activateArray, params, layerChoice = 'CN_A2'):\n",
    "    #activateArray is a list of dictionary of activation\n",
    "    \n",
    "    noOfWindows = 9 #no of windows to be displayed\n",
    "    ImagePortionList = []\n",
    "    layerNo = int(layerChoice[-1]) #extract layer number\n",
    "\n",
    "    # Randomly pick the channel\n",
    "    m, nH, nW, nC = activateArray[0][layerChoice].shape\n",
    "    neuro_pos = (random.randint(0,nH-1),random.randint(0,nW-1)) #random pick a neuron     \n",
    "    neuro_c = (random.randint(0,nC-1)) # randomly pick a channel\n",
    "\n",
    "    vs, ve, hs, he = locateActivationRegion(layerNo, params, neuro_pos)\n",
    "        \n",
    "    input_padded = addPadding(X, params['hparam_conv1']['pad'])\n",
    "\n",
    "    maxarg = np.argsort(np.absolute(activateArray[0][layerChoice]), axis=0)\n",
    "        \n",
    "    for i in range(noOfWindows):\n",
    "        max_sample = maxarg[:,neuro_pos[0],neuro_pos[1],neuro_c][-(i+1)]\n",
    "        imageportion = input_padded[max_sample,vs:ve,hs:he,:]\n",
    "        ImagePortionList.append(imageportion)\n",
    "        \n",
    "        \n",
    "    # Plot the image list in a 9 x 9 grid\n",
    "    nrow = 3\n",
    "    ncol = 3\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6)) \n",
    "\n",
    "    gs = gridspec.GridSpec(nrow, ncol, width_ratios=[1, 1, 1],\n",
    "             wspace=0.0, hspace=0.0, top=0.7, bottom=0.05, left=0.18, right=0.845) \n",
    "\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            im = ImagePortionList[i*nrow+j]\n",
    "            ax= plt.subplot(gs[i,j])\n",
    "            ax.imshow(im)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.grid(False)\n",
    "\n",
    "    #plt.tight_layout() # do not use this!!\n",
    "    plt.show()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def locateActivationRegion(layerNo, params, neuro_pos):\n",
    "    \n",
    "    #layerNo = 2\n",
    "    \n",
    "    pool_vstart = neuro_pos[0]\n",
    "    pool_vend = neuro_pos[0]    \n",
    "    pool_hstart = neuro_pos[1]\n",
    "    pool_hend = neuro_pos[1]    \n",
    "    \n",
    "    for l in reversed(range(layerNo)): \n",
    "    \n",
    "        f, f, nC_prev, nC = params['CN_W' + str(l+1)].shape\n",
    "        p = params['hparam_conv' + str(l+1)]['pad']\n",
    "        s = params['hparam_conv' + str(l+1)]['stride']\n",
    "        conv_vstart = pool_vstart*s\n",
    "        conv_vend = pool_vend*s+f\n",
    "        conv_hstart = pool_hstart*s\n",
    "        conv_hend = pool_hend*s+f\n",
    "\n",
    "        if l > 0:\n",
    "            f = params['hparam_pool' + str(l)]['f']\n",
    "            s = params['hparam_pool' + str(l)]['stride']\n",
    "\n",
    "            pool_vstart = conv_vstart*s\n",
    "            pool_vend = conv_vend*s\n",
    "            pool_hstart = conv_hstart*s\n",
    "            pool_hend = conv_hend*s    \n",
    "        else:\n",
    "            return conv_vstart, conv_vend, conv_hstart, conv_hend\n",
    "    \n",
    "    return pool_vstart, pool_vend, pool_hstart, pool_hend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showFilters(params, key='CN_W1'):\n",
    "    \n",
    "    p = params[key]\n",
    "    \n",
    "    fig, axarr = plt.subplots(1, p.shape[3],  figsize=(16, 12))\n",
    "\n",
    "    for i in range(p.shape[3]):\n",
    "        pgray = np.zeros([p.shape[0], p.shape[1], p.shape[2]])\n",
    "        pgray = rgb2gray(p[:,:,:,i])\n",
    "        axarr[i].set_title(key)\n",
    "        axarr[i].imshow(pgray, cmap = plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "\n",
    "    gray = np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showImage(imageArr, title = ''):\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.imshow(imageArr,)\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(cost_array, stitle):\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    i = cost_array.shape[0]\n",
    "    \n",
    "    plt.plot(np.arange(0,i), cost_array,'-')\n",
    "    plt.title(stitle)\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 5), dpi=100)    \n",
    "    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padToKeepDim(inDim, fDim, stride=1):\n",
    "    outdim = inDim\n",
    "    p = int((((outdim - 1) * stride) + fDim - inDim)/2)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calOutDim(inDim, fDim, padding, stride):\n",
    "\n",
    "    outDim = int((inDim + 2*padding - fDim)/stride) +  1\n",
    "    \n",
    "    return outDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addPadding(arr, p):\n",
    "    m, h, w, c = arr.shape\n",
    "    \n",
    "    padded = np.zeros([m, (h + 2*p) , (w + 2*p), c])\n",
    "    padded[ :, p:(p+h), p:(p+w), :] = arr[:,:,:,:]\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "\n",
    "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
    "    s = W * a_slice_prev\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = float(Z + b)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input2Col(A_prev, f, nC_prev, s, p, nH, nW):\n",
    "\n",
    "    m = A_prev.shape[0]\n",
    "    #print(A_prev.shape)\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. \n",
    "    col_A_prev = np.zeros([m, f, f, nC_prev, nH*nW])\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    if p == 0:\n",
    "        A_prev_pad = A_prev\n",
    "    else:\n",
    "        A_prev_pad = addPadding(A_prev, p)\n",
    "    \n",
    "    for i in range(m):                  # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]    # Select ith training example's padded activation\n",
    "        #print(\"conv forward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        for h in range(nH):                 # loop over vertical axis of the output volume\n",
    "            for w in range(nW):             # loop over horizontal axis of the output volume\n",
    "                \n",
    "                a_prev_slice = a_prev_pad[h*s : h*s + f, w*s : w*s + f , :]\n",
    "                #if i ==0 and h ==0 and w ==0:\n",
    "                    #print(a_prev_slice[:,:,0])\n",
    "                index = h * nW + w\n",
    "                col_A_prev[i, :, :, :, index] = a_prev_slice\n",
    "        \n",
    "    return col_A_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward2(A_prev, W, b, hparameters):\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    \n",
    "    #retrieve the filter dimension\n",
    "    f, f, nC_prev, noOffilters = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" \n",
    "    s = hparameters['stride']\n",
    "    p = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume \n",
    "    nH = calOutDim(nH_prev, f, p, s)\n",
    "    nW = calOutDim(nW_prev, f, p, s)\n",
    "    \n",
    "    #column-ize the input of X\n",
    "    col_A_prev = input2Col(A_prev, f, nC_prev, s, p, nH, nW)\n",
    "    \n",
    "    a = col_A_prev.transpose(0,4,1,2,3)\n",
    "    \n",
    "    Z = np.zeros([m, nH, nW, noOffilters])\n",
    "                         \n",
    "    for c in range(W.shape[3]):\n",
    "        thisW = W[:,:,:,c]\n",
    "        thisb = b[:,:,:,c]\n",
    "        \n",
    "        thisproduct = a*thisW\n",
    "        thisZ = np.sum(thisproduct, axis=(2,3,4)) + thisb\n",
    "        \n",
    "        thisSumT =  thisZ.transpose(1,2,0)\n",
    "        thisConv = thisSumT.reshape(m,nH,nW)\n",
    "                         \n",
    "        Z[:,:,:,c] = thisConv\n",
    "    \n",
    "    cache = (A_prev, W, b, hparameters, col_A_prev)\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, nH, nW, noOffilters))\n",
    "        \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, nC_prev, noOfFilters) = W.shape\n",
    "    \n",
    "    # b would be in shape (1,1,1,noOfFilters)\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" \n",
    "    s = hparameters['stride']\n",
    "    p = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume \n",
    "    nH = calOutDim(nH_prev, f, p, s)\n",
    "    nW = calOutDim(nW_prev, f, p, s)\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. \n",
    "    Z = np.zeros([m, nH, nW, noOfFilters])\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = addPadding(A_prev, p)\n",
    "    \n",
    "    for i in range(m):                  # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]    # Select ith training example's padded activation\n",
    "        #print(\"conv forward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        for h in range(nH):                 # loop over vertical axis of the output volume\n",
    "            for w in range(nW):             # loop over horizontal axis of the output volume\n",
    "                for c in range(noOfFilters):        # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad \n",
    "                    a_slice_prev = a_prev_pad[h*s : h*s + f, w*s : w*s + f ,:]\n",
    "                    \n",
    "                    # For each slice, convolve the slice with the each filter W and b\n",
    "                    # ie fill the output matrix across filter using each slice\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
    "                                        \n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, nH, nW, noOfFilters))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_forward(Z, hparameters, mode = \"max\"):\n",
    "\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, nH_prev, nW_prev, nC_prev) = Z.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    s = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    nH = calOutDim(nH_prev, f, 0, s)\n",
    "    nW = calOutDim(nW_prev, f, 0, s)\n",
    "    nC = nC_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    PooledZ = np.zeros((m, nH, nW, nC))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        #print(\"pool forward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        for h in range(nH):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(nW):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (nC):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c.\n",
    "                    z_prev_slice = Z[i,h*s:h*s + f, w*s:w*s + f,c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. \n",
    "                    # Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        PooledZ[i, h, w, c] = np.max(z_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        PooledZ[i, h, w, c] = np.mean(z_prev_slice)\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (Z, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(PooledZ.shape == (m, nH, nW, nC))\n",
    "    \n",
    "    return PooledZ, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_forward2(Z, hparameters, mode = \"max\"):\n",
    "\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, nH_prev, nW_prev, nC_prev) = Z.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    s = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    nH = calOutDim(nH_prev, f, 0, s)\n",
    "    nW = calOutDim(nW_prev, f, 0, s)\n",
    "    nC = nC_prev\n",
    "    \n",
    "    col_Z = input2Col(Z, f, nC_prev, s, 0, nH, nW)\n",
    "    \n",
    "    # Compute the pooling operation on the column\n",
    "\n",
    "    if mode == \"max\":\n",
    "        PooledColZ = np.max(col_Z, axis=(1,2))\n",
    "        #maskColZ = np.argmax(col_Z, axis=(1,2))\n",
    "    elif mode == \"average\":\n",
    "        PooledColZ = np.mean(col_Z, axis=(1,2))\n",
    "        \n",
    "    PooledZ = PooledColZ.transpose(0,2,1).reshape(m, nH, nW, nC)\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (Z, hparameters, col_Z)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(PooledZ.shape == (m, nH, nW, nC))\n",
    "    \n",
    "    return PooledZ, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Take dZ which is m, nH, nW, nC_prev and calculate:\n",
    "    dA (for the slice) m, f, f, nC_prev by adding W along nH, nW and for each corresponding slot, muliply by  \n",
    "    corresponding right dZ\n",
    "    dW same calculation except adding up the right slice of A_prev instead of W, then for each one \n",
    "    muliply by the corresponding dZ\n",
    "    dB just add up all the dZ to form a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, nC_prev, noOfFilters) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    s = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros(list(A_prev.shape))                     \n",
    "    dW = np.zeros(list(W.shape))   \n",
    "    db = np.zeros(list(b.shape) )  \n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = addPadding(A_prev, pad)\n",
    "    dA_prev_pad = addPadding(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]\n",
    "        da_prev_pad = dA_prev_pad[i,:,:,:]\n",
    "        \n",
    "        #print(\"conv backward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[h*s:h*s+f,w*s: w*s+f,:]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    #scan out the da\n",
    "                    da_prev_pad[h*s : h*s+f, w*s : w*s+f, :] += W[:,:,:,c]*dZ[i,h,w,c]\n",
    "                    #repeatedly darken the dW\n",
    "                    dW[:,:,:,c] += a_slice*dZ[i,h,w,c]  #note that DZ[i,h,w,c] is a scalar\n",
    "                    #add all up\n",
    "                    db[:,:,:,c] += dZ[i,h,w,c] #broadcast single value to 1,1,1\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad,pad:-pad,:] #unpad -pad is padth index from the row/col end\n",
    "\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    db = 1./m * db\n",
    "    dW = 1./m * dW\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward2(dZ, cache):\n",
    "    \"\"\"\n",
    "    Take dZ which is m, nH, nW, nC and calculate:\n",
    "    dA (for the slice) m, f, f, nC_prev by adding W along nH, nW and for each corresponding slot, muliply by  \n",
    "    corresponding right dZ\n",
    "    dW same calculation except adding up the right slice of A_prev instead of W, then for each one \n",
    "    muliply by the corresponding dZ\n",
    "    dB just add up all the dZ to form a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters, col_A_prev_pad) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, nC_prev, noOfFilters) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    s = hparameters['stride']\n",
    "    p = hparameters['pad']\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, nH, nW, nC) = dZ.shape\n",
    "    #nHW = nH*nW\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros(list(A_prev.shape))                     \n",
    "    dW = np.zeros(list(W.shape))   \n",
    "    db = np.zeros(list(b.shape))  \n",
    "\n",
    "    # add pad to dA_prev for matching the dimension \n",
    "    dA_prev_pad = addPadding(dA_prev, p)\n",
    "    \n",
    "    # Part I: Compute: dA_prev\n",
    "    # Transform A_prev into a colm matrix, including padding\n",
    "    #col_A_prev_pad = input2Col(A_prev, f, nC_prev, s, p, nH, nW)\n",
    "    # shape = m, f, f, nC_prev, nH * nW\n",
    "    \n",
    "    # Columize dZ, W\n",
    "    dZ_squeezed = dZ.reshape(m, nH*nW, nC)\n",
    "    W_squeezed = W.reshape(f*f*nC_prev ,noOfFilters).transpose(1,0)\n",
    "    \n",
    "    # dot products between dZ and @\n",
    "    dotprod = np.zeros([nH*nW, m, f*f*nC_prev])\n",
    "    \n",
    "    #print(dotproducts_arr.shape)\n",
    "    for colslot in range(nH*nW):\n",
    "        dotprod[colslot,:,:] = np.dot(dZ_squeezed[:,colslot,:], W_squeezed[:,:])\n",
    "    \n",
    "    #print(dotprod.shape)\n",
    "    dotprod_unroll = dotprod.transpose(1,0,2).reshape(m, nH*nW, f, f, nC_prev)\n",
    "\n",
    "    for colslot in range(nH*nW):\n",
    "        vstart = int(colslot/nH)\n",
    "        hstart = colslot - vstart*nH\n",
    "        #assign the corresponding \"slot\" of col matrix to the DZ canvas\n",
    "        dA_prev_pad[:,vstart*s:vstart*s+f,hstart*s:hstart*s+f,:] += dotprod_unroll[:,colslot,:,:,:]\n",
    "    \n",
    "    dA_prev = dA_prev_pad[:,p:-p,p:-p,:]\n",
    "    \n",
    "    # Part II: Compute: dW\n",
    "    # reshape col_A_prev_pad\n",
    "    col_A_prev_pad_squeezed = col_A_prev_pad.transpose(1,2,3,0,4).reshape(f, f, nC_prev, m * nH*nW)\n",
    "    \n",
    "    # reshape dZ\n",
    "    col_dZ_squeezed = dZ.reshape(m * nH*nW, nC)\n",
    "    \n",
    "    dW = 1./m * np.dot(col_A_prev_pad_squeezed,col_dZ_squeezed)\n",
    "    \n",
    "    db = 1./m * np.sum(dZ, axis=(0,1,2))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    #take the previous input layer to identify which \"slot\" the backward prop should be allocated to\n",
    "    \n",
    "    mask = (x == np.max(x))\n",
    "    if np.sum(mask) > 1:\n",
    "        print(\"More than one local maxima\")\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute_value(da, shape):\n",
    "    \"\"\"\n",
    "    distribute the value of da to the corresponding slice of Z and pass backward\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (nH, nW) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = 1/(nH * nW)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    z = np.ones([nH, nW])*da*average\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_backward(pooled_dZ, cache, mode = \"max\"):\n",
    "    #backward prop for the pooling layer: max or average\n",
    "    \n",
    "    # Retrieve information from cache for identifying slots to pass the backward prop\n",
    "    (Z, hparameters, _) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" to \"locate\" previous slice\n",
    "    s = hparameters['stride']\n",
    "    f = hparameters['f']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, nH_Z, nW_Z, nC_Z = Z.shape\n",
    "    m, nH, nW, nC = pooled_dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dZ = np.zeros([m, nH_Z, nW_Z, nC_Z])\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev\n",
    "        z = Z[i,:,:,:]\n",
    "        \n",
    "        #print(\"pool backward: \" +  str(i+1) + \"th training data, out of \" + str(m), end='\\r', flush=True)\n",
    "        \n",
    "        for h in range(nH):                   # loop on the vertical axis of the current layer\n",
    "            for w in range(nW):               # loop on the horizontal axis of the current layer\n",
    "                for c in range(nC):           # loop over the channels of the current layer\n",
    "                    \n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        z_slice = z[h*s : h*s+f, w*s : w*s+f, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(z_slice)\n",
    "                        #mask =  mask/np.sum(mask)  #just in case there are more than one maxima\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dZ[i, h*s: h*s+f, w*s: w*s+f, c] += pooled_dZ[i,h,w,c]*mask\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        pooled_dz = pooled_dZ[i,h,w,c] #get the current slow of dA\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dZ. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dZ[i, h*s: h*s+f, w*s: w*s+f, c] += distribute_value(pooled_dz, shape)\n",
    "                        \n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    #print(dZ.shape)\n",
    "    #print(np.sum(dZ == 0))\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward2(pooled_dZ, cache, mode = \"max\"):\n",
    "    #backward prop for the pooling layer: max or average\n",
    "    \n",
    "    # Retrieve information from cache for identifying slots to pass the backward prop\n",
    "    (Z, hparameters, col_Z) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" to \"locate\" previous slice\n",
    "    s = hparameters['stride']\n",
    "    f = hparameters['f']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, nH_Z, nW_Z, nC_Z = Z.shape\n",
    "    m, nH, nW, nC = pooled_dZ.shape\n",
    "    m, f, f, nC, nHW = col_Z.shape\n",
    "    \n",
    "    #convert dZ to column\n",
    "    #col_pooled_dZ = pooled_dZ.reshape(m,nH,nW,nC,1,1).transpose(0,4,5,1,2,3).reshape(m,1,1,nC,nHW)\n",
    "    col_pooled_dZ = pooled_dZ.transpose(0,3,1,2).reshape(m,nC,nHW).reshape(m,1,1,nC,nHW)\n",
    "    \n",
    "    if mode == \"max\":   \n",
    "        # Create mask for colZ\n",
    "        maskZ = (np.amax(col_Z, axis=(1,2), keepdims=True) == col_Z)\n",
    "        \n",
    "        # Calculate col dZ\n",
    "        col_dZ = col_pooled_dZ * maskZ\n",
    "    else:\n",
    "        col_dZ = col_pooled_dZ * (1/(f*f))\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dZ = np.zeros([m, nH_Z, nW_Z, nC_Z])\n",
    "    \n",
    "    #find the corresponding h and w from a columized matrix\n",
    " \n",
    "    for colslot in range(nHW):\n",
    "        vstart = int(colslot/nH)\n",
    "        hstart = colslot - vstart*nH\n",
    "        #assign the corresponding \"slot\" of col matrix to the DZ canvas\n",
    "        dZ[:,vstart*s:vstart*s+f,hstart*s:hstart*s+f,:] += col_dZ[:,:,:,:,colslot]\n",
    "        \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    #print(dZ.shape)\n",
    "    #print(np.sum(dZ == 0))\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSavedParams():\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    with open('convNet3.json', 'r') as infile:\n",
    "        param_data = json.load(infile)\n",
    "    \n",
    "    for key in param_data:\n",
    "        if isinstance(param_data[key], list):\n",
    "            params[key] = np.asarray(param_data[key])\n",
    "        else:\n",
    "            params[key] = param_data[key]\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initConvAndPoolLayer(params, name, noOfFilter, f, s, inDim, prev_nc, PoolHParams, keepDims = True, p = None):\n",
    "    \n",
    "    if keepDims == True: #override the value of p1 if KeepDims == True\n",
    "        p = padToKeepDim(inDim, f, s)\n",
    "        z_out_Dim = inDim\n",
    "    else:\n",
    "        z_out_Dim = calOutDim(inDim, f, p, s)\n",
    "    \n",
    "    smooth_factor = np.sqrt(2/(f*f)) \n",
    "    params['CN_W' + str(name)] = np.random.randn(f,f,prev_nc,noOfFilter) * smooth_factor\n",
    "    params['CN_b' + str(name)] = np.zeros([1,1,1,noOfFilter]) + 0.001\n",
    "    params['hparam_conv' + str(name)] = {'stride': s, 'pad': p}\n",
    "    params['hparam_pool' + str(name)] = PoolHParams\n",
    "    aDim = calOutDim(z_out_Dim, params['hparam_pool'+ str(name)]['f'], 0, params['hparam_pool'+ str(name)]['stride'])\n",
    "    \n",
    "    return aDim, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(inDim, FCLayers_dim, hparams):\n",
    "    params = {}\n",
    "    smooth_grad = hparams['smooth_grad']\n",
    "    loadFile = hparams['loadFile']\n",
    "    noOfCNLayers = 3  # as defined as no of repeating CN pattern\n",
    "    \n",
    "    if loadFile == True:\n",
    "        params = loadSavedParams()\n",
    "        print(\"saved parameters loaded!\")\n",
    "    else:\n",
    "        noOfFilter1 = 6\n",
    "        noOfFilter2 = 16\n",
    "        noOfFilter3 = 32        \n",
    "        #initConvAndPoolLayer(params, name, noOfFilter, f, s, inDim, prev_nc, PoolHParams, keepDims = True, p = None)\n",
    "        a1Dim, params = initConvAndPoolLayer(params, '1', noOfFilter1, 3, 1, inDim, 3, {'stride': 2, 'f': 2})\n",
    "        a2Dim, params = initConvAndPoolLayer(params, '2', noOfFilter2, 3, 1, a1Dim, noOfFilter1, {'stride': 2, 'f': 2})\n",
    "        a3Dim, params = initConvAndPoolLayer(params, '3', noOfFilter3, 3, 1, a2Dim, noOfFilter2, {'stride': 2, 'f': 2})\n",
    "        \n",
    "        ### FC layers dimension ###\n",
    "        FCLayers_dim[0] = a3Dim*a3Dim*noOfFilter3\n",
    "        \n",
    "        for l in range(1, len(FCLayers_dim)):\n",
    "            if smooth_grad == True:\n",
    "                smooth_gradient_adj = np.sqrt(2/FCLayers_dim[l-1])    # to avoid vanishing or exploding grads\n",
    "            else:\n",
    "                smooth_gradient_adj = 1\n",
    "            params['FC_W' + str(l)] = np.random.randn(FCLayers_dim[l],FCLayers_dim[l-1]) * smooth_gradient_adj\n",
    "            params['FC_b' + str(l)] = np.zeros([FCLayers_dim[l],1]) + 0.001\n",
    "    \n",
    "    return params, FCLayers_dim, noOfCNLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    cache = (A, W, b)           #linear_cache\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    a = np.maximum(0,Z)\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_forward(Z):\n",
    "    \n",
    "    #Zshift = Z - np.max(Z)\n",
    "    t = np.exp(Z)\n",
    "    a = np.divide(t, (np.sum(t, axis=0, keepdims=True)))\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flattenArray(arr):\n",
    "    m = arr.shape[0]\n",
    "    n = arr.shape[1]*arr.shape[2]*arr.shape[3]\n",
    "    a = np.zeros([m, n])\n",
    "    for i in range(m):\n",
    "        a[i,:] = arr[i,:,:,:].reshape(n,)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, FCLayers_dim, params):\n",
    "    \"\"\"    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    FC_caches = []\n",
    "    CN_caches = {}\n",
    "    activation = {}\n",
    "    L = len(FCLayers_dim)                  # number of layers in the FC network\n",
    "    dropOutRate = hparams['dropOutRate']\n",
    "    \n",
    "    A_CN = X\n",
    "    \n",
    "    for l in range(noOfCNLayers):\n",
    "        A_prev = A_CN\n",
    "        Z, CN_caches['cache_conv' + str(l+1)] = conv_forward2(A_prev, params['CN_W' + str(l+1)], \n",
    "                                            params['CN_b'+ str(l+1)], params['hparam_conv'  + str(l+1)])\n",
    "        A_CN, CN_caches['cache_pool' + str(l+1)] = pool_forward2(Z,params['hparam_pool'+str(l+1)], \"max\")\n",
    "        #A_CN, CN_caches['cache_relu' + str(l+1)] = relu(PooledZ)\n",
    "        \n",
    "        if dropOutRate > 0.0: A_CN=(np.random.randn(*(A_CN.shape)) > dropOutRate)*A_CN\n",
    "        \n",
    "        activation['CN_A' + str(l+1)] = A_CN\n",
    "        \n",
    "        CN_caches['LastCNAshape'] = A_CN.shape\n",
    "    \n",
    "    \n",
    "    A_unrolled = unrollMatrix(A_CN)\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    A = A_unrolled\n",
    "    for l in range(1, L - 1):\n",
    "        A_prev = A\n",
    "                \n",
    "        A, FC_cache = linear_activation_forward(A_prev, params['FC_W' + str(l)], params['FC_b' + str(l)], \"relu\")\n",
    "        \n",
    "        if dropOutRate > 0.0: A_CN=(np.random.randn(*(A.shape)) > dropOutRate)*A\n",
    "        \n",
    "        activation['FC_A' + str(l)] = A\n",
    "        \n",
    "        FC_caches.append(FC_cache)\n",
    "    \n",
    "    # Implement LINEAR -> softmax. Add \"cache\" to the \"caches\" list.\n",
    "    \n",
    "    AL, FC_cache = linear_activation_forward(A, params['FC_W' + str(L-1)], params['FC_b' + str(L-1)], \"softmax\")   \n",
    "    \n",
    "    activation['FC_A' + str(L-1)] = AL\n",
    "    \n",
    "    FC_caches.append(FC_cache)          # (linear_cache, z_activation_cache) \n",
    "        \n",
    "    assert(AL.shape == (params['FC_W' + str(L-1)].shape[0],X.shape[0]))\n",
    "    \n",
    "    return AL, FC_caches, CN_caches, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    ####def linear_activation_forward(A_prev, W, b, activation):    \n",
    "    \n",
    "    if activation == \"softmax\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b)   \n",
    "        A, z_activation_cache = softmax_forward(Z)\n",
    "\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b)\n",
    "        A, z_activation_cache = relu(Z)\n",
    "            \n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    FC_cache = (linear_cache, z_activation_cache)         #linear_cache is A, W, b, activation_cache is Z\n",
    "\n",
    "    return A, FC_cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, FCLayers_dim, params, _lambda):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    Y = np.array(Y, dtype=float)     # to avoid division by zero\n",
    "    SumSqW = 0                       # for regularization\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "\n",
    "    #cost = (1/m)*np.sum(-(Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    \n",
    "    \"\"\"\n",
    "    if np.sum(AL <= 0) > 0:                    #check if there is any instances, true = 1\n",
    "        AL[AL <= 0] = 1e-10\n",
    "        print(\"AL below zeros detected\")\n",
    "        \n",
    "    if np.sum(AL >= 1.0) > 0:\n",
    "        sub = 1.0 - 1e-10\n",
    "        AL[AL >= 1.0] = sub      #make it just slightly smaller than 1\n",
    "        print(\"(1 - AL) below zeros detected\")\n",
    "    \"\"\"\n",
    "\n",
    "    #logprobs = np.multiply(-np.log(AL),Y) + np.multiply(-np.log(1.0 - AL), 1.0 - Y)\n",
    "    #logprobs = (-np.log(AL) * Y) + (-np.log(1.0 - AL) * (1.0 - Y))\n",
    "    logprobs = -Y*np.log(AL)\n",
    "    \n",
    "    #print(logprobs.shape)\n",
    "    \n",
    "    ### Regularization ###\n",
    "    L = len(FCLayers_dim)\n",
    "    if _lambda > 0:\n",
    "        for l in range(L-1): \n",
    "            SumSqW = SumSqW + np.sum(np.square(params[\"FC_W\" + str(l + 1)]))\n",
    "            L2_reg = (1./(2 * m)) * _lambda * SumSqW\n",
    "    else:\n",
    "        L2_reg = 0\n",
    "    \n",
    "    cost = 1./m * np.sum(logprobs) + L2_reg\n",
    "        \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollMatrix(theVector, A2shape):\n",
    "\n",
    "    rolledMatrix = theVector.transpose(1,0).reshape(list(A2shape))\n",
    "    \n",
    "    assert(rolledMatrix.shape == A2shape)\n",
    "    \n",
    "    return rolledMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unrollMatrix(A):\n",
    "    m = A.shape[0]\n",
    "    \n",
    "    unrolledA = A.transpose(1,2,3,0).reshape(-1,m)\n",
    "    \n",
    "    return unrolledA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FC_model_backward(AL, Y, FCLayers_dim, FC_caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(FCLayers_dim) - 1 # the number of layers\n",
    "    \n",
    "    m = AL.shape[1] # A or Z retains the dimension of number of training examples m\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    noOfClass = Y.shape[0]\n",
    "    \n",
    "    dAL = -1 * np.divide( Y, AL )   #note that dAL doesnt depend on the actual cost and m\n",
    "    \n",
    "    current_cache = FC_caches[L-1]   # contains of linear cache (A, W, b,) and activation cache (Z)\n",
    "    \n",
    "    ### first backpropagation :-> softmax\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, 'softmax')\n",
    "    \n",
    "    grads[\"FC_dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"FC_dW\" + str(L)] = dW_temp\n",
    "    grads[\"FC_db\" + str(L)] = db_temp\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"FC_dA\" + str(l+1)], FC_caches[l] , 'relu')\n",
    "        \n",
    "        grads[\"FC_dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"FC_dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"FC_db\" + str(l+1)] = db_temp\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    #dJ_dZ = dJ_dA * dA_dZ\n",
    "    #dA_dZ = 0 when Z <=0\n",
    "    #dA_dZ = 1 when Z > 0\n",
    "    #dJ_dZ = 0 when z <=0; = dJ_dA when Z > 0\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0 \n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_backward_extended(dA, cache):\n",
    "    \n",
    "    Z = cache    \n",
    "    a, _ = softmax_forward(Z)\n",
    "    needVerify = True\n",
    "    useForLoop = False\n",
    "    \n",
    "    assert(Z.shape == dA.shape)\n",
    "    \n",
    "    noOfClass, m = Z.shape\n",
    "    dJdZ = np.zeros([noOfClass, m])\n",
    "    \n",
    "    for k in range(m):  #reconstruct dJdZMatrix per training example\n",
    "        dAdZMatrix = np.zeros((noOfClass, noOfClass))\n",
    "        \n",
    "        dAdZMatrix = -np.outer(a[:, k], a[:, k]) + np.diag(a[:, k].flatten())\n",
    "        \n",
    "        if useForLoop == True:\n",
    "            dAdZ_forLoop = np.zeros((noOfClass, noOfClass))\n",
    "        \n",
    "            for i in range(noOfClass):\n",
    "                for j in range(noOfClass):\n",
    "                    dAdZ_forLoop[i, j] = a[i, k] * ((i == j) - a[j, k])\n",
    "            \n",
    "            #dAdZMatrix = dAdZ_forLoop\n",
    "        \n",
    "        if needVerify == True and useForLoop == True:\n",
    "            if (np.sum(dAdZ_forLoop) - np.sum(dAdZMatrix)) > 1e-15:\n",
    "                print(\"difference between dAdZ_forLoop and Matrix is too big\")\n",
    "            else:\n",
    "                print(\"both method of backward softmax return similar results\")\n",
    "        \n",
    "            assert(dAdZMatrix.shape ==  dAdZ_forLoop.shape)\n",
    "        \n",
    "        assert(dAdZMatrix.shape == (noOfClass,noOfClass))\n",
    "\n",
    "        new_vector = np.sum ( (dA[:,k].reshape(noOfClass,1) * dAdZMatrix).T, axis=1, keepdims=True)\n",
    "    \n",
    "        if k == 0:\n",
    "            dJdZMatrix = new_vector\n",
    "\n",
    "        else:\n",
    "            dJdZMatrix = np.concatenate((dJdZMatrix, new_vector), axis=1)\n",
    "    \n",
    "    #hardcoded answer\n",
    "    hardCoded_dJdZa = a + dA*a\n",
    "    if np.sum(dJdZMatrix - hardCoded_dJdZa) > 1e-10:\n",
    "        print(\"There is an error in dJdZ first step of softmax backward\")\n",
    "    \n",
    "    return dJdZMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]   # n x m\n",
    "\n",
    "    dW = 1./m * np.dot(dZ, A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    dA_prev = np.dot(W.T, dZ) \n",
    "    \n",
    "    assert (db.shape == b.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectDeadNodes(actHistory, threhold = 0.0, selected_m = 0):\n",
    "    \n",
    "    # is a list of dict. List len = no of steps, dict contains activation of different layers\n",
    "    \n",
    "    total_act = {}\n",
    "    key_list = actHistory[0].keys()\n",
    "    \n",
    "    # add up all the values across different steps\n",
    "    for i in range(len(actHistory)):\n",
    "        for k in key_list:\n",
    "            if i == 0:\n",
    "                total_act[k] = actHistory[i][k]\n",
    "            else:\n",
    "                total_act[k] += actHistory[i][k]\n",
    "    \n",
    "    #calculate deadnodes rates for each activation\n",
    "    for key in total_act:\n",
    "        if 'CN' in key: #conv m x nH x nW x nC\n",
    "            sumOverAllData = np.sum(total_act[key], axis=0)\n",
    "        else:\n",
    "            #print(total_act[key].shape)\n",
    "            sumOverAllData = np.sum(total_act[key].T, axis=0)\n",
    "            #print(sumOverAllData.shape)\n",
    "        deadrate = np.sum(sumOverAllData == 0)/sumOverAllData.reshape(-1,1).shape[0]\n",
    "        if deadrate >= threhold:\n",
    "            print(key + \" has deadrate of :\" + str(round(deadrate*100,2)) + \"%\")\n",
    "    \n",
    "    return total_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        \n",
    "        dZ = softmax_backward_extended(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "                \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_params(m, FCLayers_dim, params, grads, alpha, _lambda):\n",
    "    \n",
    "    for l in range(2 ):\n",
    "        \n",
    "        params[\"CN_W\" + str(l+1)] = params[\"CN_W\" + str(l+1)] - alpha * (grads[\"CN_dW\" + str(l+1)]  \n",
    "                                                 + (params[\"CN_W\" + str(l+1)] * (_lambda/m))) \n",
    "        params[\"CN_b\" + str(l+1)] = params[\"CN_b\" + str(l+1)] - alpha * grads[\"CN_db\" + str(l+1)]\n",
    "    \n",
    "    L = len(FCLayers_dim)\n",
    "    \n",
    "    for l in range(L-1): \n",
    "\n",
    "        params[\"FC_W\" + str(l+1)] = params[\"FC_W\" + str(l+1)] - alpha * (grads[\"FC_dW\" + str(l+1)] \n",
    "                                                                         + (params[\"FC_W\" + str(l+1)] * (_lambda/m)) )  \n",
    "        params[\"FC_b\" + str(l+1)] = params[\"FC_b\" + str(l+1)] - alpha * grads[\"FC_db\" + str(l+1)]\n",
    "                \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict2vector(_dict, skip_term, include_term = \"\"):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    len_no_term = sum(1 for i in _dict if skip_term not in i and include_term in i)  \n",
    "    # find the length of vector without hparam*\n",
    "    \n",
    "    #this term stores list of gradients and its size for checking. To be used later to restall \"params\"\n",
    "    keyLabelsInfo = np.array(range(len_no_term*2), dtype='U32').reshape(len_no_term,2)\n",
    "    \n",
    "    count = 0\n",
    "        \n",
    "    dict_cache = {}\n",
    "    \n",
    "    for key, value in sorted(_dict.items()):\n",
    "        \n",
    "        if skip_term not in key and include_term in key:\n",
    "        \n",
    "            #Storing key names and dimenson\n",
    "            keyLabelsInfo[count, 0] = key\n",
    "            \n",
    "            shape_str = \"\"\n",
    "\n",
    "            for i in range(len(value.shape)):\n",
    "                shape_str = shape_str + str(value.shape[i])\n",
    "                \n",
    "                if i < (len(value.shape) - 1):\n",
    "                    shape_str = shape_str + \"-\"\n",
    "            keyLabelsInfo[count,1] = shape_str\n",
    "        \n",
    "            #storing a N x 1 dimensional value vector\n",
    "            new_vector = np.reshape(_dict[key], (-1,1))\n",
    "            thiskeylist = [key]*new_vector.shape[0]\n",
    "        \n",
    "            if count == 0:\n",
    "                dict_values = new_vector\n",
    "                keylist = thiskeylist\n",
    "\n",
    "            else:\n",
    "                dict_values = np.concatenate((dict_values, new_vector), axis=0)\n",
    "                keylist.extend(thiskeylist)\n",
    "                \n",
    "            count = count + 1\n",
    "        else:\n",
    "            dict_cache[key] = value\n",
    "            \n",
    "    assert(len(keylist) == len(dict_values))\n",
    "                \n",
    "    return keyLabelsInfo, keylist, dict_values, dict_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector2dict(keys_labels, param_values, params_cache):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    last_index = 0\n",
    "    params = {}\n",
    "    \n",
    "    for i in range(keys_labels.shape[0]):\n",
    "        \n",
    "        key = keys_labels[i][0]\n",
    "        dimlist = keys_labels[i][1].split('-')\n",
    "        \n",
    "        index_length = 1\n",
    "        this_shape = ()\n",
    "        \n",
    "        for i in range(len(dimlist)):\n",
    "            index_length = index_length * int(dimlist[i])\n",
    "            this_shape = this_shape + (int(dimlist[i]),)\n",
    "        \n",
    "        temp_array = param_values[last_index:last_index+index_length,0]\n",
    "        \n",
    "        temp_array = temp_array.reshape(*this_shape)\n",
    "        params[key] = temp_array\n",
    "        \n",
    "        last_index = last_index + index_length\n",
    "        \n",
    "    for k in params_cache:\n",
    "        params[k] = params_cache[k]\n",
    "\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(params, grads, X, Y, FCLayers_dim, hparams):\n",
    "\n",
    "    \n",
    "    # Set-up variables\n",
    "    epsilon = 1e-7\n",
    "    printdiff = True\n",
    "    randomCheck = True\n",
    "    \n",
    "    hparam_keylabels, pkeylist, param_values, params_cache = dict2vector(params, \"hparam\", \"\")\n",
    "    no_dA_grad_labels, gkeylist, no_dA_grad_values, grads_cache = dict2vector(grads, \"dA\", \"\")\n",
    "\n",
    "    num_parameters = param_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    assert (no_dA_grad_values.shape == param_values.shape)\n",
    "    \n",
    "    # random to display results of 50 gradient checks\n",
    "    if randomCheck == True:\n",
    "        randpos = list(range(len(no_dA_grad_values)))\n",
    "        random.shuffle(randpos)\n",
    "        \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        \n",
    "        if (randomCheck == True) and (i in randpos[0:50]):\n",
    "\n",
    "            thetaplus = np.copy(param_values)                           # Step 1\n",
    "        \n",
    "            if no_dA_grad_values[i] != 0:\n",
    "        \n",
    "                thetaplus[i][0] = thetaplus[i][0] + epsilon                 # Step 2\n",
    "\n",
    "                updated_params_plus = vector2dict(hparam_keylabels, thetaplus, params_cache)\n",
    "        \n",
    "                AL_plus, _ , _ , _ = L_model_forward(X, FCLayers_dim, updated_params_plus)\n",
    "                J_plus[i] = compute_cost(AL_plus, Y, FCLayers_dim, params, 0.0)     # Step 3\n",
    "\n",
    "\n",
    "                # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "\n",
    "                thetaminus = np.copy(param_values)                          # Step 1\n",
    "                thetaminus[i][0] = thetaminus[i][0] - epsilon               # Step 2        \n",
    "\n",
    "                updated_params_minus = vector2dict(hparam_keylabels, thetaminus, params_cache)\n",
    "            \n",
    "                AL_minus, _ , _ , _ = L_model_forward(X, FCLayers_dim, updated_params_minus)            \n",
    "                J_minus[i] = compute_cost(AL_minus, Y, FCLayers_dim, params, 0.0)   # Step 3\n",
    "\n",
    "        \n",
    "                # Compute gradapprox[i]\n",
    "                gradapprox[i] = (J_plus[i] - J_minus[i])/(2 * epsilon)\n",
    "    \n",
    "                # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "\n",
    "                numerator = np.linalg.norm(no_dA_grad_values[i] - gradapprox[i])                \n",
    "                denominator = np.linalg.norm(no_dA_grad_values[i]) + np.linalg.norm(gradapprox[i]) \n",
    "                difference = np.divide(numerator, denominator)                                      \n",
    "\n",
    "                if printdiff == True:\n",
    "                    if difference > 1e-7:\n",
    "                        print (\"\\033[93m\" + \"Gradient Check on param: \" + str(gkeylist[i]) \n",
    "                               + \". backward Prop error! difference = \" + str(difference) + \"\\033[0m\")\n",
    "                        #subprocess.call([\"afplay\", \"beep-08b.wav\"])\n",
    "                    else:\n",
    "                        print (\"\\033[92m\" + \"Gradient Check on param: \" + str(gkeylist[i]) \n",
    "                               + \". Backward Prop OKAY! difference = \" + str(difference) + \"\\033[0m\")\n",
    "                    print(\"grad value: \" + str(no_dA_grad_values[i]) + \"; grad approx: \" \n",
    "                          + str(gradapprox[i]) + \"; ratio: \" + str(no_dA_grad_values[i]/gradapprox[i]))\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveParams(params, printmsg = False):\n",
    "    \n",
    "    toSaveParam = {}\n",
    "    \n",
    "    for key in params:\n",
    "        if isinstance(params[key], np.ndarray):\n",
    "            toSaveParam[key] = params[key].tolist()\n",
    "        else:\n",
    "            toSaveParam[key] = params[key]\n",
    "    \n",
    "    with open('convNet3.json', 'w') as outfile:\n",
    "        json.dump(toSaveParam, outfile)\n",
    "        if printmsg == True:\n",
    "            print(\"parameters saved!\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showOutcome(X, Y, outcome, noOfexamples = 5, showWrongOnly = True):\n",
    "\n",
    "    if showWrongOnly == True:\n",
    "        wrongSlot = (np.argmax(outcome, axis=0) != np.argmax(Y, axis=0) )*1 # find the slot where prediction is wrong\n",
    "        posSlot = list(np.where(wrongSlot==1)[0])\n",
    "        statusstr = \" wrong only \"\n",
    "    else:\n",
    "        posSlot = list(range(0,outcome.shape[1]))\n",
    "        statusstr = \" \"\n",
    "        \n",
    "    #print(len(posSlot))\n",
    "    \n",
    "    if len(posSlot) > 0:\n",
    "         \n",
    "        fig, axarr = plt.subplots(1, noOfexamples, figsize=(16, 12))\n",
    "        \n",
    "        print(\"Randomly showing\" + str(statusstr) + \"predictions:\")\n",
    "    \n",
    "        for p in range(noOfexamples):\n",
    "            pos = random.randint(0,len(posSlot)-1)\n",
    "            i = posSlot[pos]\n",
    "            t = \"Y: \" + str(np.argmax(Y[:,i])) + \" ; predict: \" + str(np.argmax(outcome[:,i]))\n",
    "\n",
    "            axarr[p].set_title(t,  fontsize=14)\n",
    "            axarr[p].imshow(X[i,:,:,:])\n",
    " \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"There is no wrong prediction.\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calAccuracy(AL, Y):\n",
    "    \n",
    "    accuracy = (sum(np.argmax(Y, axis=0) == np.argmax(AL, axis=0))/Y.shape[1]) * 100\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(X, Y, params, mode = \"Training\"):\n",
    "    \n",
    "    activateArray = []\n",
    "    \n",
    "    #Conv and FC Forward\n",
    "    AL, _ , _ , activation = L_model_forward(X, FCLayers_dim, params)\n",
    "    activateArray.append(activation)\n",
    "    \n",
    "    accuracy = calAccuracy(AL, Y)\n",
    "    \n",
    "    print(str(mode) + \" Accuracy on this data set is: \" + str(accuracy) + \"%.\")\n",
    "    \n",
    "    return AL, activateArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CN_model_backward(grads, CN_caches):\n",
    "    \n",
    "    dA = grads['FC_dA0']\n",
    "    \n",
    "    for l in reversed(range(noOfCNLayers)):\n",
    "        dA_prev = dA\n",
    "        #CN_Pooled_dZ = relu_backward(dA_prev, CN_caches['cache_relu' + str(l+1)])       \n",
    "        \n",
    "        CN_dZ = pool_backward2(dA_prev, CN_caches['cache_pool' + str(l+1)], mode = \"max\")\n",
    "        dA, dW, db = conv_backward2(CN_dZ, CN_caches['cache_conv' + str(l+1)])\n",
    "        \n",
    "        grads['CN_dA' + str(l)] = dA\n",
    "        grads['CN_dW' + str(l+1)] = dW\n",
    "        grads['CN_db' + str(l+1)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, FCLayers_dim, FC_caches, CN_caches):\n",
    "    \n",
    "    # FC backward\n",
    "    grads = FC_model_backward(AL, Y, FCLayers_dim, FC_caches)\n",
    "    \n",
    "    #cache_conv1, cache_pool1, cache_conv2, cache_pool2, cache_relu1, cache_relu2, A2shape = CN_caches\n",
    "    \n",
    "    # Undo the unrolling\n",
    "    grads['FC_dA0'] = rollMatrix(grads['FC_dA0'], CN_caches['LastCNAshape'])\n",
    "    \n",
    "    #Conv backward\n",
    "    \n",
    "    grads = CN_model_backward(grads, CN_caches)\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerbar(achieved, total):\n",
    "\n",
    "    powerbar = [\" \"]\n",
    "    powerbar = powerbar*total\n",
    "    powerbar[0:achieved] = \"=\"*achieved\n",
    "    powerbar = ''.join(powerbar)\n",
    "    \n",
    "    return powerbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_model_run(dataBatches, params, hparams):\n",
    "\n",
    "    _lambda = hparams['_lambda']\n",
    "    alpha = hparams['alpha']\n",
    "    noOfIterations = hparams['noOfIterations']\n",
    "    checkGradient = hparams['checkGradient']\n",
    "    printCost = hparams['printCost']\n",
    "    minibatches = hparams['minibatches']\n",
    "    trackActHistory = hparams['trackActHistory']\n",
    "    \n",
    "    activationHistory = []  #a list of all the activation history: iterations x mini-batch size. \n",
    "    \n",
    "    if minibatches == True:\n",
    "        X = dataBatches[0][0]\n",
    "        Y = dataBatches[0][1]\n",
    "        dataBatches = random_mini_batches(X, Y, mini_batch_size = 64, seed = 0)\n",
    "        noOfbatches = len(dataBatches)\n",
    "    else:\n",
    "        noOfbatches = 1\n",
    "        \n",
    "    if printCost == True:\n",
    "        costArray = np.zeros([noOfIterations*noOfbatches,1]) # for tracking cost changes across steps\n",
    "\n",
    "    for i in range(noOfIterations):\n",
    "        \n",
    "        ###start of one epoch\n",
    "        for d in range(noOfbatches):\n",
    "            X = dataBatches[d][0]\n",
    "            Y = dataBatches[d][1]\n",
    "        \n",
    "            #Conv and FC Forward\n",
    "            AL, FC_caches, CN_caches, activation = L_model_forward(X, FCLayers_dim, params)\n",
    "        \n",
    "            #keep track of all activations after RELU/SIGMOID/TANH\n",
    "            if trackActHistory == True:\n",
    "                activationHistory.append(activation)\n",
    "\n",
    "            #Cost compute    \n",
    "            cost = compute_cost(AL, Y, FCLayers_dim, params, _lambda)\n",
    "                    \n",
    "            #FC and Conv backward\n",
    "            grads = L_model_backward(AL, Y, FCLayers_dim, FC_caches, CN_caches)\n",
    "        \n",
    "            assert(grads['CN_dA0'].shape == X.shape)\n",
    "        \n",
    "            if printCost == True:\n",
    "                costArray[i*noOfbatches+d,0] = cost\n",
    "\n",
    "            if checkGradient == True and (i+1)%1 == 0:\n",
    "                diff = gradient_check_n(params, grads, X, Y, FCLayers_dim, hparams)        \n",
    "        \n",
    "            #update params\n",
    "            params = update_params(X.shape[0], FCLayers_dim, params, grads, alpha, _lambda)\n",
    "        \n",
    "            # Print Update           \n",
    "            if minibatches == True:\n",
    "                bar = powerbar(d, noOfbatches-1)\n",
    "                status = \"Mini-batch: \" + \"|\" + bar + \"|\"\n",
    "            else:\n",
    "                status = ''\n",
    "\n",
    "            acc = calAccuracy(AL, Y)\n",
    "            print(status + \" Cost: \" + str(round(cost,7)) + \" at \" + str(i+1)\n",
    "                                    + \"th iters; accuracy: \" + str(round(acc,2)) + \"%   \", end='\\r',flush=True)\n",
    "        \n",
    "        ### end of one epoch \n",
    "        \n",
    "        if hparams['save2File'] == True:\n",
    "            saveParams(params, False)\n",
    "        \n",
    "        print('')        \n",
    "        \n",
    "    #print cost graph\n",
    "    if printCost == True:\n",
    "        plot_graph(costArray, \"cost change per iteration\")\n",
    "    \n",
    "\n",
    "    return params, grads, activationHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved parameters loaded!\n",
      "Mini-batch: |================| Cost: 7.617366 at 1th iters; accuracy: 100.0%    \n",
      "Mini-batch: |================| Cost: 7.6165103 at 2th iters; accuracy: 100.0%   \n",
      "Mini-batch: |================| Cost: 7.6157712 at 3th iters; accuracy: 100.0%   \n",
      "Test Accuracy on this data set is: 93.3333333333%.\n"
     ]
    }
   ],
   "source": [
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig, allClasses = loadData()\n",
    "minibatch_start = 0\n",
    "minibatch_end = 1080\n",
    "X = train_x_orig[minibatch_start:minibatch_end,:]\n",
    "Y = train_y_orig[:,minibatch_start:minibatch_end]\n",
    "#X = train_x_orig\n",
    "#Y = train_y_orig\n",
    "FCLayers_dim = [None, 120, 84, len(allClasses)]\n",
    "\n",
    "hparams = {}\n",
    "hparams['_lambda'] = 0.01\n",
    "hparams['alpha'] = 0.0001\n",
    "hparams['noOfIterations'] = 3\n",
    "hparams['save2File'] = False\n",
    "hparams['loadFile'] = True\n",
    "hparams['smooth_grad'] = True\n",
    "hparams['checkGradient'] = False\n",
    "hparams['printCost'] = False\n",
    "hparams['dropOutRate'] = 0.00\n",
    "hparams['trackActHistory'] = False\n",
    "hparams['minibatches'] = True\n",
    "np.random.seed(10) \n",
    "\n",
    "params, FCLayers_dim, noOfCNLayers = init_params(X.shape[1], FCLayers_dim, hparams)\n",
    "\n",
    "params, grads, actHistory = class_model_run([[X, Y]] , params, hparams)\n",
    "\n",
    "outcome, activateArray = make_prediction(test_x_orig, test_y_orig, params, \"Test\")\n",
    "#outcome, activateArray = make_prediction(train_x_orig, train_y_orig, params, \"Training\")\n",
    "\n",
    "#showOutcome(X, Y, outcome, showWrongOnly = True)\n",
    "#showFilters(params, 'CN_W1')\n",
    "#showFilters(params, 'CN_W2')\n",
    "#detectDeadNodes(activateArray, 0.8)\n",
    "#showActivationInput(train_x_orig, activateArray, params, 'CN_A2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
