{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Background:\n",
    "#I am a newbie on machine learing and I am doing this notebook to try out different methods/skills that I learned\n",
    "# most the optioins and parameters changes can be done at the bottom of the notebook by changing parameters in \n",
    "# global_var. \n",
    "# The basic purpose is to classify images of digits from 0-9 into 10 different classes. \n",
    "# Here are some of the functions we can use in this notebook: normalize data, separate training and testing data,\n",
    "# use of batch normalization, use softmax, use gradient check, use gradient descent or adams or momentum methods \n",
    "# for updating paramters, use of dropout, use of L2 regularization, plotgraph to show cost/gradient changes etc.\n",
    "\n",
    "#Key learnings:\n",
    "# [X_train.shape[0],40,20,5,1] \n",
    "# batchnorm = True, grad = adams, iter=1000, alpha = 0.01, lambda = 0 train acc = 100% test = 92.6%\n",
    "\n",
    "\n",
    "#- Gradient checks work better if other techniques like momentum/RMS are not implemented\n",
    "#- \"division by zero in log\" is catched and attempted to be solved by addiing epsilon to (1 - AL) if <= 0. \n",
    "#   By doing so at least the cost function would not become nan\n",
    "#- it seems that the cost function is easier to subject to exploding grad if alpha is larger\n",
    "#- if mini-batch is not applied, the value of adams is not certain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_array(a):\n",
    "    \n",
    "    n, m = a.shape\n",
    "\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    #a_mean = np.mean(a, axis = 1).reshape(a.shape[0],1)\n",
    "    #a_std = np.std(a, axis = 1).reshape(a.shape[0],1)\n",
    "    \n",
    "    ### mean of data set: no of columns = number of features\n",
    "    a_mean = (1./m) * np.sum(a, axis = 1).reshape(n,1)\n",
    "\n",
    "    ### difference between dataset and mean\n",
    "    a_mean_diff = (a - a_mean)\n",
    "    \n",
    "    ### square of the difference\n",
    "    a_sq = np.power( (a - a_mean) , 2 )\n",
    "    \n",
    "    ### variance\n",
    "    a_var = (1./m) * np.sum( a_sq , axis = 1).reshape(n,1)\n",
    "    \n",
    "    ### standard deviation with addition of epsilon to avoid div by zero\n",
    "    a_std = np.sqrt(a_var + epsilon) \n",
    "    \n",
    "    a_std_iver = 1./a_std\n",
    "    \n",
    "    a_norm = a_mean_diff * a_std_iver\n",
    "    \n",
    "    assert(a_norm.shape == a.shape)\n",
    "    \n",
    "    return a_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showrandomimage(X, Y, Ypredict, showWrongOnly = False):\n",
    "    \n",
    "    dim = int(np.sqrt(X.shape[0]))\n",
    "    \n",
    "    if showWrongOnly == True:\n",
    "        mask = np.where(Y != Ypredict.reshape(Y.shape))\n",
    "        Y = Y[:,mask[1]]\n",
    "        X = X[:,mask[1]]\n",
    "        Ypredict = Ypredict[:,mask[1]]\n",
    "    \n",
    "    i = np.random.randint(0,X.shape[1])\n",
    "    \n",
    "    print(\"Prediction: \" + str(Ypredict[:,i]) + \" ; Y: \" + str(Y[:,i]))\n",
    "    \n",
    "    arr = X[:,i].reshape(dim,dim).T\n",
    "    plt.imshow(arr, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(train_data_ratio):\n",
    "    \n",
    "    mat = scipy.io.loadmat('ex4data1.mat')\n",
    "    X_raw = np.array(mat['X']).T\n",
    "    Y_raw = np.array(mat['y']).T\n",
    "    \n",
    "    #X = np.random.rand(400,5000)       #X n=400, m=5000\n",
    "    #Y = np.round(np.random.rand(1,1000)*100)%2\n",
    "    \n",
    "    m = Y_raw.shape[1]\n",
    "    train_size = round(m * train_data_ratio)   # 80% as training set\n",
    "    test_size = m - train_size\n",
    "    \n",
    "    p = np.random.permutation(m)\n",
    "        \n",
    "    #X_train = normal_array(X_raw)\n",
    "    X_train = X_raw[:, p[ 0 : train_size ] ]\n",
    "    Y_train = Y_raw[:, p[ 0 : train_size ] ]\n",
    "    X_dev = X_raw[:, p[ train_size : train_size + 1 + test_size] ]\n",
    "    Y_dev = Y_raw[:, p[ train_size : train_size + 1 + test_size] ]\n",
    "    \n",
    "    assert(X_train.shape[1] + X_dev.shape[1] == m)\n",
    "    \n",
    "    return X_train, Y_train, X_dev, Y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: random_mini_batches\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "    # number of mini batches of size mini_batch_size in your partitionning\n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:,k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:,(num_complete_minibatches * mini_batch_size) : m]\n",
    "        mini_batch_Y = shuffled_Y[:,(num_complete_minibatches * mini_batch_size) : m]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareSoftMaxY(Y_train, numberOfClasses): \n",
    "    Y_all_class = np.zeros([numberOfClasses, Y_train.shape[1]])\n",
    "    Y_trainSoftMax = Y_train\n",
    "    Y_trainSoftMax[Y_train == 10] = 0\n",
    "        \n",
    "    # change Y to a numberOfClasses x m matrix with class of pos = 1\n",
    "    for i in range(Y_trainSoftMax.shape[1]):\n",
    "        Y_all_class[Y_trainSoftMax[0,i], i] = 1\n",
    "    \n",
    "    return Y_all_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims, global_var):\n",
    "\n",
    "    parameters = {}\n",
    "    momentumGrad = {}            # for moving average gradient\n",
    "    RMSGrad = {}\n",
    "    ActRecord = {}\n",
    "    batchNorm = global_var['batchNorm']\n",
    "    #checkAct = global_var['checkActivation']\n",
    "    \n",
    "    #if checkAct == True:\n",
    "    #    actStatus = {}\n",
    "    \n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        ### Avoid Gradient Vanish or Exploding\n",
    "        smooth_gradient_adj = np.sqrt(2/layer_dims[l-1])    # to avoid vanishing or exploding gradients\n",
    "        \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * smooth_gradient_adj\n",
    "       \n",
    "        ### for moving average gradient ###\n",
    "        momentumGrad['dW' + str(l)]  = np.zeros([layer_dims[l],layer_dims[l-1]])\n",
    "       \n",
    "        ### for RMS moving average gradient ###\n",
    "        RMSGrad['dW' + str(l)]  = np.zeros([layer_dims[l],layer_dims[l-1]])\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            ### Creating Gamma and Beta for Z normalization\n",
    "            parameters['G' + str(l)] = np.ones([layer_dims[l],1]) * smooth_gradient_adj \n",
    "            #parameters['G' + str(l)] = np.random.randn(layer_dims[l],1) * smooth_gradient_adj \n",
    "            parameters['B' + str(l)] = np.zeros([layer_dims[l],1])  \n",
    "        \n",
    "            momentumGrad['dG' + str(l)]  = np.zeros([layer_dims[l],1])\n",
    "            momentumGrad['dB' + str(l)]  = np.zeros([layer_dims[l],1])     \n",
    "        \n",
    "            RMSGrad['dG' + str(l)]  = np.zeros([layer_dims[l],1])\n",
    "            RMSGrad['dB' + str(l)]  = np.zeros([layer_dims[l],1])   \n",
    "        \n",
    "            assert(parameters['G' + str(l)].shape == (layer_dims[l], 1))\n",
    "            assert(parameters['B' + str(l)].shape == (layer_dims[l], 1))\n",
    "            assert(momentumGrad['dG' + str(l)].shape == (layer_dims[l], 1))\n",
    "            assert(momentumGrad['dB' + str(l)].shape == (layer_dims[l], 1))\n",
    "        else:\n",
    "            parameters['b' + str(l)] = np.zeros([layer_dims[l],1])\n",
    "            momentumGrad['db' + str(l)] = np.zeros([layer_dims[l],1]) \n",
    "            RMSGrad['db' + str(l)] = np.zeros([layer_dims[l],1])\n",
    "            \n",
    "            assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "            \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))   \n",
    "        \n",
    "        #if checkAct == True:\n",
    "            # set all initial status to false\n",
    "            #actStatus['a' + str(l)] = np.zeros(([layer_dims[l],1]) , dtype=bool) \n",
    "    \n",
    "    #if checkAct == True:\n",
    "        #global_var['act'] = actStatus\n",
    "        \n",
    "            \n",
    "    return parameters, momentumGrad, RMSGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_graph(cost_array, stitle):\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    i = cost_array.shape[0]\n",
    "    \n",
    "    plt.plot(np.arange(0,i), cost_array,'-')\n",
    "    plt.title(stitle)\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 5), dpi=100)    \n",
    "    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    cache = (A, W, b)           #linear_cache\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    a = np.maximum(0,Z)\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_forward(Z):\n",
    "    \n",
    "    Zshift = Z - np.max(Z)\n",
    "    t = np.exp(Zshift)\n",
    "    a = np.divide(t, (np.sum(t, axis=0, keepdims=True)))\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    a = 1./(1+np.exp(-Z))\n",
    "    \n",
    "    #assert(np.sum(a <= 0) == 0 and np.sum(a >= 1) == 0)\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_forward(Z, G, B):    \n",
    "    \"\"\"\n",
    "    Obsoleted \n",
    "    \"\"\"\n",
    "    n, m = Z.shape    # n is the number of features and m is the number of training examples\n",
    "    \n",
    "    epsilon = 1e-8    # to \n",
    "    \n",
    "    ### mean of data set: no of columns = number of features\n",
    "    Z_mean = np.mean(Z, axis = 1).reshape(n,1)\n",
    "    \n",
    "    ### difference between dataset and mean\n",
    "    Z_mean_diff = (Z - Z_mean)\n",
    "    \n",
    "    ### square of the difference\n",
    "    Z_sq = Z_mean_diff ** 2 \n",
    "    \n",
    "    ### variance\n",
    "    Z_var = np.mean( Z_sq , axis = 1).reshape(n,1)\n",
    "    \n",
    "    ### standard deviation with addition of epsilon to avoid div by zero\n",
    "    Z_std = np.sqrt(Z_var + epsilon) \n",
    "    \n",
    "    ### standard deviation with addition of epsilon to avoid div by zero\n",
    "    Z_std_iver = 1./Z_std\n",
    "    \n",
    "    ### normalized Z\n",
    "    Z_norm = Z_mean_diff * Z_std_iver\n",
    "    \n",
    "    ### ZGamma: easier for differentiation\n",
    "    ZGamma = G * Z_norm\n",
    "    \n",
    "    Z_tda = ZGamma + B\n",
    "        \n",
    "    \n",
    "    norm_cache = Z_norm, G, B, Z_mean, Z_std_iver, Z_var\n",
    "        \n",
    "    assert (Z_tda.shape == Z.shape)\n",
    "    \n",
    "    return Z_tda, norm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_forward_computational_graph(Z, G, B):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Reference: \n",
    "    https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "    But seems that gradient check does not add up\n",
    "    last check: 25 Sept 2017 19:00\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n, m = Z.shape    # n is the number of features and m is the number of training examples\n",
    "    \n",
    "    epsilon = 1e-8    # to \n",
    "    \n",
    "    ### mean of data set: no of columns = number of features\n",
    "    Z_mean = (1./m) * np.sum(Z, axis = 1).reshape(n,1)\n",
    "    \n",
    "    ### difference between dataset and mean\n",
    "    Z_mean_diff = (Z - Z_mean)\n",
    "    \n",
    "    ### square of the difference\n",
    "    Z_sq = Z_mean_diff ** 2 \n",
    "    \n",
    "    ### variance\n",
    "    Z_var = (1./m) * np.sum( Z_sq , axis = 1).reshape(n,1)\n",
    "    \n",
    "    ### standard deviation with addition of epsilon to avoid div by zero\n",
    "    Z_std = np.sqrt(Z_var + epsilon) \n",
    "    \n",
    "    ### standard deviation with addition of epsilon to avoid div by zero\n",
    "    Z_std_iver = 1./Z_std\n",
    "    \n",
    "    ### normalized Z\n",
    "    Z_norm = Z_mean_diff * Z_std_iver\n",
    "    \n",
    "    ### ZGamma: easier for differentiation\n",
    "    ZGamma = G * Z_norm\n",
    "    \n",
    "    Z_tda = ZGamma + B\n",
    "    \n",
    "    norm_cache = Z_norm, G, B, Z_mean_diff, Z_std_iver, Z_var\n",
    "    \n",
    "    #norm_cache = Z_norm, G, B, Z_mean, Z_std_iver, Z_var\n",
    "            \n",
    "    assert (Z_tda.shape == Z.shape)\n",
    "    \n",
    "    return Z_tda, norm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_backward_computational_graph(dZ_tda, activation_cache, norm_cache):\n",
    "    \"\"\"\n",
    "    Reference: \n",
    "    https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "    But seems that gradient check does not add up\n",
    "    last check: 25 Sept 2017 19:00\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Z = activation_cache\n",
    "    Z_norm, G, B, Z_mean_diff, Z_std_iver, Z_var = norm_cache\n",
    "    \n",
    "    \n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    n, m = dZ_tda.shape           \n",
    "    \n",
    "    #assuming we have the right dJ_dZ_tda\n",
    "    \n",
    "    #Z_tda = GammaZ + B\n",
    "    dGammaZ = dZ_tda * 1\n",
    "    dB = np.sum((dZ_tda * 1), axis = 1).reshape(n, 1)                    #dJdB\n",
    "    \n",
    "    #GammaZ = G * Z_norm\n",
    "    dG = np.sum((dGammaZ * Z_norm), axis = 1).reshape(n, 1)               #dJdG    \n",
    "    dZ_norm = dZ_tda * G                                                          #dJdZ_norm    \n",
    "    \n",
    "    \n",
    "    #Z_norm = Z_mean_diff * Z_std_iver\n",
    "    \n",
    "    dZ_mean_diff1 = dZ_norm * Z_std_iver                                                   #dJdZ_mean_diff\n",
    "    dZ_std_iver = np.sum((dZ_norm * Z_mean_diff), axis = 1).reshape(n, 1)         #dJdZ_std_iver\n",
    "        \n",
    "    dZ_std = ( -1./ (Z_std_iver ** 2) ) * dZ_std_iver                      #dJdstd\n",
    "    \n",
    "    #Z_std = np.power((Z_var +  epsilon), (1/2))\n",
    "    \n",
    "    dZ_var = (0.5 * (1./ np.sqrt(Z_var +  epsilon) * 1)) * dZ_std            #dJddZ_var\n",
    "    \n",
    "    #Z_var = 1./m * np.sum(Z_sq)\n",
    "    \n",
    "    dZ_sq =  (1./m * np.ones(Z.shape)) * dZ_var                                    #dJdZ_sq\n",
    "    \n",
    "    #Z_sq = Z_mean_diff**2\n",
    "    \n",
    "    dZ_mean_diff2 = (2 * Z_mean_diff) * dZ_sq                            #dJdZ_mean_diff\n",
    "    \n",
    "    dZ_mean_diff = dZ_mean_diff1 + dZ_mean_diff2\n",
    "    \n",
    "    #dZ_mean_diff = z - dZ_mean\n",
    "    \n",
    "    dZ1 = 1 * (dZ_mean_diff)                                      \n",
    "                       \n",
    "    dZ_mean = np.sum((dZ_mean_diff), axis = 1).reshape(n, 1) * -1        #dJdZ_mean\n",
    "    \n",
    "    #dZ_mean = (1/m) * np.sum(z)\n",
    "    \n",
    "    dZ2 = dZ_mean * 1./m * np.ones(dZ_tda.shape)                                 #dJdZ\n",
    "    \n",
    "    dZ = dZ1 + dZ2\n",
    "    \n",
    "    ##########\n",
    "        \n",
    "    assert (dZ_tda.shape == dZ.shape)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    assert (dG.shape == (Z.shape[0],1))\n",
    "    assert (dB.shape == (Z.shape[0],1))\n",
    "    \n",
    "    \n",
    "    return dZ, dG, dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_backward_complicated(dout, activation_cache, norm_cache):\n",
    "    \"\"\"\n",
    "    Reference:\n",
    "    https://wiseodd.github.io/techblog/2016/07/04/batchnorm/\n",
    "    Seems that this one works for gradient check\n",
    "    Last update: 25 Sept 2016 20:00\n",
    "    \"\"\"\n",
    "\n",
    "    X = activation_cache\n",
    "    X_norm, gamma, beta, Z_mean_diff, Z_std_iver, var = norm_cache\n",
    "\n",
    "    assert(X.shape == dout.shape)\n",
    "    \n",
    "    n, m = dout.shape\n",
    "    \n",
    "    mu = X - Z_mean_diff\n",
    "\n",
    "    std_inv = 1. / np.sqrt(var + 1e-8)\n",
    "\n",
    "    dX_norm = dout * gamma\n",
    "    \n",
    "    \n",
    "    dvar = np.sum(dX_norm * Z_mean_diff, axis=1).reshape(n,1) * -.5 * std_inv**3\n",
    "    dmu = np.sum(dX_norm * -std_inv, axis=1, keepdims=True) + dvar * np.mean(-2. * Z_mean_diff, axis=1, keepdims=True)\n",
    "\n",
    "    dX = (dX_norm * std_inv) + (dvar * 2 * Z_mean_diff / m) + (dmu / m)\n",
    "    dgamma = np.sum(dout * X_norm, axis=1, keepdims=True)\n",
    "    dbeta = np.sum(dout, axis=1, keepdims=True)\n",
    "\n",
    "    dX = dX.reshape(n,m)\n",
    "\n",
    "    \n",
    "    return dX, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_backward_thankGod(dJ_dZtda, activation_cache, norm_cache):\n",
    "\n",
    "    # retrieve cache(s)\n",
    "    Z = activation_cache\n",
    "    Znorm, Gamma, Beta, Zmeandiff, Z_std_iver, Zvar = norm_cache\n",
    "    \n",
    "    # needed variables Z, gamma, beta, Zmean, Zvar,\n",
    "    \n",
    "    # prepare variable(s)\n",
    "    n, m = dJ_dZtda.shape\n",
    "    eps = 1e-8\n",
    "    \n",
    "    #assert(np.sum(1./(np.sqrt(Zvar + eps))) == np.sum(Z_std_iver))\n",
    "    assert(Z.shape == dJ_dZtda.shape)\n",
    "    \n",
    "    #dZtda = Gamma * Znorm + Beta\n",
    "    dZtda_dZnorm = Gamma\n",
    "    \n",
    "    dJ_dZnorm = dJ_dZtda * dZtda_dZnorm   # 1\n",
    "    #(n. m)\n",
    "    \n",
    "    #Znorm = (Z - Zmean) *  1/((Zvar + eps)**1/2)\n",
    "    # we need to compute dJ_dZ by breaking into 3 paths:\n",
    "    # 1) dJ_dZvar\n",
    "    # 2) dJ_dZmu\n",
    "    # 3) dJ_dZc (assuming other interdependent variables like Zmu, Zvar are constant)\n",
    "    # then dJ_dZ (total) = dJ_dZc + dJ_dZvar + dJ_dZmu\n",
    "    \n",
    "    #compute the derivative of Znorm wrt \"Zvar\" assuming other variables are constant\n",
    "    dZnorm_dZvar = np.sum( Zmeandiff, axis = 1, keepdims=True) * (-0.5) * ( (1./np.sqrt(Zvar + eps)) **3 )\n",
    "    # (n, 1)\n",
    "    #dJ_dZvar = np.sum(dJ_dZnorm, axis = 1).reshape(n, 1)  * dZnorm_dZvar\n",
    "    #(n, 1) cannot do this because np.sum elements have to be processed together\n",
    "    dJ_dZvar = np.sum( dJ_dZnorm * Zmeandiff, axis = 1, keepdims=True) * (-0.5) * ( (1./np.sqrt(Zvar + eps)) **3 )\n",
    "    # (n, 1)\n",
    "    \n",
    "    \n",
    "    #compute the derivative of Znorm wrt \"Zmu\" assuming other variablesare constant\n",
    "    dZnorm_dZmu = np.sum( (-1./np.sqrt(Zvar + eps)) , \n",
    "                         axis = 1).reshape(n,1) + dJ_dZvar * (-2./m) * np.sum( Zmeandiff, axis = 1).reshape(n, 1) \n",
    "    #(n, 1)\n",
    "    \n",
    "    #dJ_dZmu = np.sum(dJ_dZnorm, axis = 1).reshape(n, 1)  * dZnorm_dZmu\n",
    "    #(n, 1) cannot do this because np.sum elements have to be processed together\n",
    "    \n",
    "    dJ_dZmu = np.sum(dJ_dZnorm * (-1./np.sqrt(Zvar + eps)) , \n",
    "                     axis = 1, keepdims = True) + dJ_dZvar * (-2./m) * np.sum( Zmeandiff , axis = 1, keepdims = True)\n",
    "    \n",
    "    #(n, 1)\n",
    "    \n",
    "    #compute the derivative of Znorm wrt \"Z\" assuming other variables are constant\n",
    "    dZnorm_dZc = 1./(np.sqrt(Zvar + eps)) * 1\n",
    "    #dJ_dZc = dJ_dZnorm * dZnorm_dZc\n",
    "    #(n, m)\n",
    "    \n",
    "    #we need to compute dZmu_dZvar and dZmu_dZ before adding them up\n",
    "    #Zvar = 1/m * np.sum( (Z - Zmu) ** 2)\n",
    "    #dZvar_dZ =  2 * (Z - Zmu)/m \n",
    "    #(n, m)\n",
    "    \n",
    "    #Zmu = 1/m * np.sum(Z)\n",
    "    #dZmu_dZ = 1/m\n",
    "    #(n, m)\n",
    "    \n",
    "    #then we compuete the total dJ_dZ\n",
    "\n",
    "    dJ_dZ = (dJ_dZnorm * dZnorm_dZc) + (dJ_dZvar * 2 * Zmeandiff/m ) + (dJ_dZmu / m)\n",
    "    #(n, m)\n",
    "    \n",
    "    \n",
    "    dJ_dGamma = np.sum(dJ_dZtda * Znorm, axis=1).reshape(n, 1)\n",
    "    dJ_dBeta = np.sum(dJ_dZtda * 1, axis=1).reshape(n, 1)\n",
    "    \n",
    "    return dJ_dZ, dJ_dGamma, dJ_dBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation, global_var, G = 0, B = 0):\n",
    "\n",
    "    ####def linear_activation_forward(A_prev, W, b, activation):    \n",
    "    \n",
    "    batchNorm = global_var['batchNorm']\n",
    "    dropOut = global_var['dropOut']\n",
    "    dropOutRate = 0.15\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b, G, B)\n",
    "            Ztida, norm_cache = batchnorm_forward_computational_graph(Z, G, B)\n",
    "            if global_var['useSoftMax'] == False:\n",
    "                A, z_activation_cache = sigmoid(Ztida) \n",
    "            else:\n",
    "                A, z_activation_cache = softmax_forward(Ztida)\n",
    "        else:\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b)   \n",
    "            if global_var['useSoftMax'] == False:\n",
    "                A, z_activation_cache = sigmoid(Z)\n",
    "            else:\n",
    "                A, z_activation_cache = softmax_forward(Z)\n",
    "\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b, G, B)\n",
    "            Ztida, norm_cache = batchnorm_forward_computational_graph(Z, G, B)\n",
    "            A, z_activation_cache = relu(Ztida)\n",
    "        else:\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b)\n",
    "            A, z_activation_cache = relu(Z)\n",
    "            \n",
    "        if dropOut == True and A.shape[0] >= 10:\n",
    "            d = np.random.rand(A.shape[0], A.shape[1])\n",
    "            A = A * (d > dropOutRate)\n",
    "            #print(\"dropout applied!\")\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    cache = (linear_cache, z_activation_cache)         #linear_cache is A, W, b, activation_cache is Z\n",
    "\n",
    "    if batchNorm == True:\n",
    "        return A, cache, norm_cache\n",
    "    else:\n",
    "        return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, layer_dims, parameters, global_var, i = -1):\n",
    "    \"\"\"    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    batchNorm = global_var['batchNorm']\n",
    "    isPredict = global_var['isPredict']\n",
    "    #checkAct = global_var['checkActivation']\n",
    "    \n",
    "    caches = []\n",
    "    norm_caches = []\n",
    "    A = X\n",
    "    L = len(layer_dims)                  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L - 1):\n",
    "        A_prev = A \n",
    "                \n",
    "        if batchNorm == True:\n",
    "            A, cache, norm_cache = linear_activation_forward(A_prev, parameters['W' + str(l)], 0, \"relu\", global_var, parameters['G' + str(l)], parameters['B' + str(l)])\n",
    "            norm_caches.append(norm_cache)\n",
    "        else:\n",
    "            A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\", global_var)\n",
    "        \n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    \n",
    "    if batchNorm == True:\n",
    "        AL, cache, norm_cache = linear_activation_forward(A, parameters['W' + str(L-1)], 0, \"sigmoid\", global_var, parameters['G' + str(L-1)], parameters['B' + str(L-1)])\n",
    "        norm_caches.append(norm_cache)\n",
    "    else:\n",
    "        AL, cache = linear_activation_forward(A, parameters['W' + str(L-1)], parameters['b' + str(L-1)], \"sigmoid\", global_var)\n",
    "        \n",
    "    \n",
    "    caches.append(cache)          # (linear_cache, z_activation_cache) \n",
    "        \n",
    "    assert(AL.shape == (parameters['W' + str(L-1)].shape[0],X.shape[1]))\n",
    "    \n",
    "    if batchNorm == True:       \n",
    "        return AL, caches, norm_caches\n",
    "    else:\n",
    "        return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, layer_dims, parameters, lambd, global_var):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    Y = np.array(Y, dtype=float)     # to avoid division by zero\n",
    "    SumSqW = 0                       # for regularization\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "\n",
    "    #cost = (1/m)*np.sum(-(Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    \n",
    "    if np.sum(AL <= 0) > 0:                    #check if there is any instances, true = 1\n",
    "        AL[AL <= 0] = 1e-7\n",
    "        print(\"AL below zeros detected\")\n",
    "        \n",
    "    if np.sum(AL >= 1) > 0:\n",
    "        sub = 1 - 1e-7\n",
    "        AL[AL > 1] = sub      #make it just slightly smaller than 1\n",
    "        print(\"(1 - AL) below zeros detected\")\n",
    "      \n",
    "    if global_var['useSoftMax'] == False:    \n",
    "        logprobs = np.multiply(-np.log(AL),Y) + np.multiply(-np.log(1 - AL), 1 - Y)\n",
    "    else:\n",
    "        logprobs = -1 * np.sum(Y * (np.log(AL)), axis=0, keepdims=True)\n",
    "    \n",
    "    ### Regularization ###\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(L-1): \n",
    "        SumSqW = SumSqW + np.sum(np.square(parameters[\"W\" + str(l + 1)]))\n",
    "        L2_reg = (1./(2 * m)) * lambd * SumSqW\n",
    "    \n",
    "    cost = 1./m * np.sum(logprobs) + L2_reg\n",
    "        \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    #dJ_dZ = dJ_dA * dA_dZ\n",
    "    #dA_dZ = 0 when Z <=0\n",
    "    #dA_dZ = 1 when Z > 0\n",
    "    #dJ_dZ = 0 when z <=0; = dJ_dA when Z > 0\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0 \n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache     # activation cache\n",
    "    \n",
    "    a = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    #dZ = np.multiply(np.multiply(a, (1-a)), dA)\n",
    "    dZ = dA * a * (1-a)       # dAL/dZ = a * (1-a)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_backward_extended(dA, cache):\n",
    "    \n",
    "    Z = cache    \n",
    "    a, _ = softmax_forward(Z)\n",
    "    needVerify = False\n",
    "    useForLoop = False\n",
    "    \n",
    "    assert(Z.shape == dA.shape)\n",
    "    \n",
    "    noOfClass, m = Z.shape\n",
    "    dJdZ = np.zeros([noOfClass, m])\n",
    "    \n",
    "    for k in range(m):\n",
    "        dAdZMatrix = np.zeros((noOfClass, noOfClass))\n",
    "        \n",
    "        if useForLoop == False:\n",
    "            dAdZMatrix = -np.outer(a[:, k], a[:, k]) + np.diag(a[:, k].flatten())\n",
    "        else:\n",
    "            dAdZ_forLoop = np.zeros((noOfClass, noOfClass))\n",
    "        \n",
    "            for i in range(noOfClass):\n",
    "                for j in range(noOfClass):\n",
    "                    dAdZ_forLoop[i, j] = a[i, k] * ((i == j) - a[j, k])\n",
    "            \n",
    "            dAdZMatrix = dAdZ_forLoop\n",
    "        \n",
    "        if needVerify == True and useForLoop == True:\n",
    "            if (np.sum(dAdZ_forLoop) - np.sum(dAdZMatrix)) > 1e-15:\n",
    "                print(\"difference between dAdZ_forLoop and Matrix is too big\")\n",
    "        \n",
    "            assert(dAdZMatrix.shape ==  dAdZ_forLoop.shape)\n",
    "        \n",
    "        assert(dAdZMatrix.shape == (noOfClass,noOfClass))\n",
    "\n",
    "        new_vector = np.sum ( (dA[:,k].reshape(noOfClass,1) * dAdZMatrix).T, axis=1, keepdims=True)\n",
    "    \n",
    "        if k == 0:\n",
    "            dJdZMatrix = new_vector\n",
    "\n",
    "        else:\n",
    "            dJdZMatrix = np.concatenate((dJdZMatrix, new_vector), axis=1)\n",
    "    \n",
    "    #hardcoded answer\n",
    "    dJdZa = a + dA*a\n",
    "    \n",
    "    #print(dJdZMatrix.shape)\n",
    "    \n",
    "    if np.sum(dJdZMatrix) - np.sum(dJdZa) > 1e-10:\n",
    "        print(\"difference between dJdZMatrix and hardcode calculation is too big\")\n",
    "    \n",
    "    return dJdZMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    t = np.exp(Z)\n",
    "    a = t/(np.sum(t, axis=0))\n",
    "    \n",
    "    dZ = a + dA * a\n",
    "    \n",
    "    assert (dZ.shape == dA.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache, batchNorm):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "       \n",
    "    dW = 1./m * np.dot(dZ, A_prev.T)   \n",
    "    dA_prev = np.dot(W.T, dZ)    \n",
    "        \n",
    "    if batchNorm ==  False:\n",
    "        db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        assert (db.shape == b.shape)\n",
    "        \n",
    "        \n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    \n",
    "    if batchNorm ==  True:\n",
    "        return dA_prev, dW\n",
    "    else:\n",
    "        return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, norm_cache, activation, batchNorm):\n",
    "    \n",
    "    batchNorm = global_var['batchNorm']\n",
    "    batchNormBackMethod = global_var['batchNormBackMethod']\n",
    "    useSoftMax = global_var['useSoftMax']\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "\n",
    "        if batchNorm == True:\n",
    "            dZ_tda = relu_backward(dA, activation_cache)\n",
    "            \n",
    "            if batchNormBackMethod == \"abstract\":\n",
    "                dZ, dG, dB = batchnorm_backward_thankGod(dZ_tda, activation_cache, norm_cache) \n",
    "            else:\n",
    "                dZ, dG, dB = batchnorm_backward_computational_graph(dZ_tda, activation_cache, norm_cache)\n",
    "                \n",
    "            dA_prev, dW = linear_backward(dZ, linear_cache, batchNorm)\n",
    "        else:\n",
    "            dZ = relu_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache, batchNorm)\n",
    "        \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            if useSoftMax == False:\n",
    "                dZ_tda = sigmoid_backward(dA, activation_cache)\n",
    "            else:\n",
    "                dZ_tda = softmax_backward_extended(dA, activation_cache)\n",
    "            \n",
    "            if batchNormBackMethod == \"abstract\":\n",
    "                dZ, dG, dB = batchnorm_backward_thankGod(dZ_tda, activation_cache, norm_cache) \n",
    "            else:\n",
    "                dZ, dG, dB = batchnorm_backward_computational_graph(dZ_tda, activation_cache, norm_cache)    \n",
    "             \n",
    "            dA_prev, dW = linear_backward(dZ, linear_cache, batchNorm)  \n",
    "\n",
    "        else:\n",
    "            if useSoftMax == False:\n",
    "                dZ = sigmoid_backward(dA, activation_cache)\n",
    "            else:\n",
    "                dZ = softmax_backward_extended(dA, activation_cache)\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache, batchNorm)\n",
    "                \n",
    "\n",
    "    if batchNorm == True:  \n",
    "        return dA_prev, dW, dG, dB\n",
    "    else:\n",
    "        return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictionary_to_vector_custom(parameters):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    keys_labels = np.array(range(len(parameters)*3), dtype='U8').reshape(len(parameters),3)\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for key, value in sorted(parameters.items()):\n",
    "        \n",
    "        #Storing key names and dimenson\n",
    "        keys_labels[count, 0] = key\n",
    "        keys_labels[count,1] = value.shape[0]\n",
    "        keys_labels[count,2] = value.shape[1]\n",
    "        \n",
    "        #storing a N x 1 dimensional value vector\n",
    "        new_vector = np.reshape(parameters[key], (-1,1))\n",
    "        \n",
    "        if count == 0:\n",
    "            param_values = new_vector\n",
    "\n",
    "        else:\n",
    "            param_values = np.concatenate((param_values, new_vector), axis=0)\n",
    "    \n",
    "                \n",
    "        count = count + 1\n",
    "        \n",
    "    return keys_labels, param_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_to_dictionary_custom(keys_labels, param_values):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    last_index = 0\n",
    "    parameters = {}\n",
    "    \n",
    "    for i in range(keys_labels.shape[0]):\n",
    "        \n",
    "        key = keys_labels[i][0]\n",
    "        dim0 = int(keys_labels[i][1])\n",
    "        dim1 = int(keys_labels[i][2])\n",
    "        index_length = (dim0 * dim1)\n",
    "        \n",
    "        temp_array = param_values[last_index:last_index+index_length,0]\n",
    "        \n",
    "        temp_array = temp_array.reshape(dim0, dim1)\n",
    "        parameters[key] = temp_array\n",
    "        \n",
    "        last_index = last_index + index_length\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradients_to_vector_custom(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    len_no_dA = sum(1 for i in gradients if 'dA' not in i)  # find the length of vector without dA*\n",
    "    \n",
    "    no_dA_grad_labels = np.array(range(len_no_dA*3), dtype='U8').reshape(len_no_dA,3)\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for key, value in sorted(gradients.items()):\n",
    "        \n",
    "        if 'dA' not in key:\n",
    "        \n",
    "            #Storing key names and dimenson\n",
    "            no_dA_grad_labels[count, 0] = key\n",
    "            no_dA_grad_labels[count,1] = value.shape[0]\n",
    "            no_dA_grad_labels[count,2] = value.shape[1]\n",
    "        \n",
    "            #storing a N x 1 dimensional value vector\n",
    "            new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        \n",
    "            if count == 0:\n",
    "                no_dA_grad_values = new_vector\n",
    "\n",
    "            else:\n",
    "\n",
    "                no_dA_grad_values = np.concatenate((no_dA_grad_values, new_vector), axis=0)\n",
    "                \n",
    "            count = count + 1\n",
    "        \n",
    "    \n",
    "    return no_dA_grad_labels, no_dA_grad_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, batchNorm, norm_caches = []):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    batchNorm = global_var['batchNorm']\n",
    "    useSoftMax = global_var['useSoftMax']\n",
    "\n",
    "    \n",
    "    m = AL.shape[1] # A or Z retains the dimension of number of training examples m\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    noOfClass = Y.shape[0]\n",
    "    \n",
    "    if useSoftMax == False:\n",
    "        dAL = ( - (np.divide( Y, AL ) - np.divide(1 - Y, 1 - AL )) ) # derivative of cost with respect to AL\n",
    "    else:\n",
    "        dAL = -1 * np.divide( Y, AL ) \n",
    "        \n",
    "\n",
    "    \n",
    "    current_cache = caches[L-1]   # contains of linear cache (A, W, b,) and activation cache (Z)\n",
    "    \n",
    "    if batchNorm == True:\n",
    "        current_norm_cache  = norm_caches[L-1]\n",
    "    else:\n",
    "        current_norm_cache = norm_caches\n",
    "    \n",
    "    #### MAKE norm_caches to append ####\n",
    "    \n",
    "    ### first backpropagation :-> sigmoid\n",
    "    if batchNorm == True:\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"dG\" + str(L)], grads[\"dB\" + str(L)] = linear_activation_backward(dAL, current_cache, current_norm_cache, 'sigmoid', global_var)\n",
    "        #print(np.sum(grads[\"dG\" + str(L)]))\n",
    "    else:\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, current_norm_cache, 'sigmoid', global_var)\n",
    "        #print(np.sum(grads[\"dW\" + str(L)]))\n",
    "\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            dA_prev_temp, dW_temp, dG_temp, dB_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], caches[l], norm_caches[l] , 'relu', global_var)\n",
    "        else:\n",
    "            dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], caches[l], norm_caches , 'relu', global_var)\n",
    "\n",
    "        \n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            grads[\"dG\" + str(l+1)] = dG_temp\n",
    "            grads[\"dB\" + str(l+1)] = dB_temp\n",
    "        else:\n",
    "            grads[\"db\" + str(l+1)] = db_temp\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, layer_dims, global_var):\n",
    "\n",
    "    \n",
    "    # Set-up variables\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    batchNorm = global_var['batchNorm']\n",
    "    printdiff = global_var['checkGradientPrintDiff']\n",
    "    dropOut = global_var['dropOut']\n",
    "    keys_labels, param_values = dictionary_to_vector_custom(parameters)\n",
    "    no_dA_grad_labels, no_dA_grad_values = gradients_to_vector_custom(gradients)\n",
    "\n",
    "    num_parameters = param_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    assert (no_dA_grad_values.shape == param_values.shape)\n",
    "    \n",
    "    if dropOut == True:\n",
    "        print(\"Warning Dropout is ON!\")\n",
    "    \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        #print(\"Testing \" + str(i) + \"th parameter...\")\n",
    "        \n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaplus = np.copy(param_values)                           # Step 1\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon                 # Step 2\n",
    "        if batchNorm == True:\n",
    "            AL_plus, _ , _ = L_model_forward(X, layer_dims, \n",
    "                                             vector_to_dictionary_custom(keys_labels, thetaplus), global_var)\n",
    "        else:\n",
    "            AL_plus, _  = L_model_forward(X, layer_dims, vector_to_dictionary_custom(keys_labels, thetaplus), \n",
    "                                          global_var)\n",
    "        J_plus[i] = compute_cost(AL_plus, Y, layer_dims, parameters, 0.0, global_var)     # Step 3\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaminus = np.copy(param_values)                          # Step 1\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon               # Step 2        \n",
    "\n",
    "        if batchNorm == True:\n",
    "            AL_minus, _ , _ = L_model_forward(X, layer_dims, \n",
    "                                              vector_to_dictionary_custom(keys_labels, thetaminus), global_var)\n",
    "        else:\n",
    "            AL_minus, _ = L_model_forward(X, layer_dims, vector_to_dictionary_custom(keys_labels, thetaminus), \n",
    "                                          global_var)            \n",
    "        J_minus[i] = compute_cost(AL_minus, Y, layer_dims, parameters, 0.0, global_var)   # Step 3\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute gradapprox[i]\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i])/(2 * epsilon)\n",
    "        #print(gradapprox.shape)\n",
    "        #print(no_dA_grad_values.shape)\n",
    "    \n",
    "        # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "\n",
    "        numerator = np.linalg.norm(no_dA_grad_values[i] - gradapprox[i])                          # Step 1'\n",
    "        denominator = np.linalg.norm(no_dA_grad_values[i]) + np.linalg.norm(gradapprox[i])        # Step 2'\n",
    "        difference = np.divide(numerator, denominator)                                            # Step 3'\n",
    "\n",
    "        if printdiff == True:\n",
    "            if difference > 1e-7:\n",
    "                print (\"\\033[93m\" + \"Gradient Check on \" + str(i) + \"th param: backward Prop error! difference = \" + str(difference) + \"\\033[0m\")\n",
    "                #subprocess.call([\"afplay\", \"beep-08b.wav\"])\n",
    "            else:\n",
    "                print (\"\\033[92m\" + \"Gradient Check on \" + str(i) + \"th param: Backward Prop OKAY! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(m, layer_dims, parameters, grads, momentumGrad, RMSGrad, alpha, lambd, i, global_var):\n",
    "\n",
    "    B1 = 0.9\n",
    "    B2 = 0.98\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    momentumGrad_corrected = {}\n",
    "    RMSGrad_corrected = {}\n",
    "\n",
    "    batchNorm = global_var['batchNorm']\n",
    "    update_method = global_var['update_method']\n",
    "        \n",
    "\n",
    "    for l in range(L-1): \n",
    "\n",
    "        \n",
    "        ### Update Velocity by using B1 and (!-B1) and Grads ###\n",
    "        momentumGrad[\"dW\" + str(l+1)] = B1 * momentumGrad[\"dW\" + str(l+1)] + ((1 - B1) * grads[\"dW\" + str(l+1)])\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            momentumGrad[\"dG\" + str(l+1)] = B1 * momentumGrad[\"dG\" + str(l+1)] + ((1 - B1) * grads[\"dG\" + str(l+1)])\n",
    "            momentumGrad[\"dB\" + str(l+1)] = B1 * momentumGrad[\"dB\" + str(l+1)] + ((1 - B1) * grads[\"dB\" + str(l+1)])\n",
    "        else:\n",
    "            momentumGrad[\"db\" + str(l+1)] = B1 * momentumGrad[\"db\" + str(l+1)] + ((1 - B1) * grads[\"db\" + str(l+1)])\n",
    "\n",
    "        \n",
    "        ### Calculate corrected Velocity     \n",
    "        momentumGrad_corrected[\"dW\" + str(l+1)] = np.divide(momentumGrad[\"dW\" + str(l+1)], (1 - B1**i))\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            momentumGrad_corrected[\"dG\" + str(l+1)] = np.divide(momentumGrad[\"dG\" + str(l+1)], (1 - B1**i))\n",
    "            momentumGrad_corrected[\"dB\" + str(l+1)] = np.divide(momentumGrad[\"dB\" + str(l+1)], (1 - B1**i))        \n",
    "        else:\n",
    "            momentumGrad_corrected[\"db\" + str(l+1)] = np.divide(momentumGrad[\"db\" + str(l+1)], (1 - B1**i))        \n",
    "           \n",
    "        \n",
    "        ### Update RMS using B2 and Grads ###\n",
    "        RMSGrad[\"dW\" + str(l+1)] = B2 * RMSGrad[\"dW\" + str(l+1)] + np.multiply((1 - B2) , np.power( grads[\"dW\" + str(l+1)], 2))\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            RMSGrad[\"dG\" + str(l+1)] = B2 * RMSGrad[\"dG\" + str(l+1)] + np.multiply((1 - B2) , np.power( grads[\"dG\" + str(l+1)], 2))\n",
    "            RMSGrad[\"dB\" + str(l+1)] = B2 * RMSGrad[\"dB\" + str(l+1)] + np.multiply((1 - B2) , np.power( grads[\"dB\" + str(l+1)], 2))\n",
    "        else:\n",
    "            RMSGrad[\"db\" + str(l+1)] = B2 * RMSGrad[\"db\" + str(l+1)] + np.multiply((1 - B2) , np.power( grads[\"db\" + str(l+1)], 2))\n",
    "            \n",
    "        \n",
    "        ### Calculate corrected RMSVelocity\n",
    "        RMSGrad_corrected[\"dW\" + str(l+1)] = np.divide(RMSGrad[\"dW\" + str(l+1)], (1 - B2**i))\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            RMSGrad_corrected[\"dG\" + str(l+1)] = np.divide(RMSGrad[\"dG\" + str(l+1)], (1 - B2**i))\n",
    "            RMSGrad_corrected[\"dB\" + str(l+1)] = np.divide(RMSGrad[\"dB\" + str(l+1)], (1 - B2**i))\n",
    "        else:\n",
    "            RMSGrad_corrected[\"db\" + str(l+1)] = np.divide(RMSGrad[\"db\" + str(l+1)], (1 - B2**i))\n",
    "                \n",
    "        ### UPDATE PARAMETERS ####\n",
    "        \n",
    "        if update_method == \"grads\":\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - alpha * (grads[\"dW\" + str(l+1)] + (parameters[\"W\" + str(l+1)] * (lambd/m)) )  \n",
    "            \n",
    "            if batchNorm == True:\n",
    "                parameters[\"G\" + str(l+1)] = parameters[\"G\" + str(l+1)] - alpha * (grads[\"dG\" + str(l+1)] + (parameters[\"G\" + str(l+1)] * (lambd/m)))\n",
    "                parameters[\"B\" + str(l+1)] = parameters[\"B\" + str(l+1)] - alpha * grads[\"dB\" + str(l+1)]\n",
    "            else:\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - alpha * grads[\"db\" + str(l+1)]\n",
    "                \n",
    "            \n",
    "        elif update_method == \"momentum\":\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - alpha * ( momentumGrad_corrected[\"dW\" + str(l+1)] + (parameters[\"W\" + str(l+1)] * (lambd/m)) )\n",
    "           \n",
    "            if batchNorm == True:\n",
    "                parameters[\"G\" + str(l+1)] = parameters[\"G\" + str(l+1)] - alpha * (momentumGrad_corrected[\"dG\" + str(l+1)] + (parameters[\"G\" + str(l+1)] * (lambd/m)))\n",
    "                parameters[\"B\" + str(l+1)] = parameters[\"B\" + str(l+1)] - alpha * momentumGrad_corrected[\"dB\" + str(l+1)]               \n",
    "            else:\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - alpha * momentumGrad_corrected[\"db\" + str(l+1)]               \n",
    "                 \n",
    "        \n",
    "        elif update_method == \"adams\":\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - alpha * ( np.divide(momentumGrad_corrected[\"dW\" + str(l+1)] , (np.sqrt(RMSGrad_corrected[\"dW\" + str(l+1)]) + epsilon)) + (parameters[\"W\" + str(l+1)] * (lambd/m)) )\n",
    "            \n",
    "            if batchNorm == True:\n",
    "                parameters[\"G\" + str(l+1)] = parameters[\"G\" + str(l+1)]- alpha * (np.divide(momentumGrad_corrected[\"dG\" + str(l+1)] , (np.sqrt(RMSGrad_corrected[\"dG\" + str(l+1)]) + epsilon)) + (parameters[\"G\" + str(l+1)] * (lambd/m)))\n",
    "                parameters[\"B\" + str(l+1)] = parameters[\"B\" + str(l+1)] - alpha * np.divide(momentumGrad_corrected[\"dB\" + str(l+1)] , (np.sqrt(RMSGrad_corrected[\"dB\" + str(l+1)]) + epsilon))        \n",
    "            else:\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - alpha * np.divide(momentumGrad_corrected[\"db\" + str(l+1)] , (np.sqrt(RMSGrad_corrected[\"db\" + str(l+1)]) + epsilon))        \n",
    "                \n",
    "            \n",
    "    return parameters, momentumGrad, RMSGrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(X, Y, numberOfClasses, layer_dims, final_params, global_var, dset):\n",
    "    \n",
    "    batchNorm = global_var['batchNorm']\n",
    "    useSoftMax = global_var['useSoftMax']\n",
    "    \n",
    "    predictions = np.zeros([1, Y.shape[1] ])\n",
    "    probability = np.zeros([numberOfClasses , Y.shape[1] ]) \n",
    "    \n",
    "    Y[Y == 10] = 0\n",
    "    \n",
    "    if useSoftMax == False:\n",
    "        \n",
    "        for p in range(numberOfClasses):\n",
    "            if batchNorm == True:\n",
    "                probability[p,:], caches, z_norm_caches = L_model_forward(X, layer_dims, \n",
    "                                                                      final_params[\"param\" + str(p)], global_var)\n",
    "            else:\n",
    "                probability[p,:], caches = L_model_forward(X, layer_dims, final_params[\"param\" + str(p)], global_var)\n",
    "    else:\n",
    "        if batchNorm == True:\n",
    "            probability, caches, z_norm_caches = L_model_forward(X, layer_dims, \n",
    "                                                                 final_param_all_class[\"finalparam\"], global_var)\n",
    "        else:\n",
    "            probability, caches = L_model_forward(X, layer_dims, final_param_all_class[\"finalparam\"], global_var)\n",
    "    \n",
    "    predictions = np.argmax(probability,axis=0)\n",
    "    predictions = predictions.T\n",
    "            \n",
    "    print(dset + \" accruracy: is \" + str(np.sum(predictions == Y)/Y.shape[1]*100) + \"%\")\n",
    "    \n",
    "    return predictions, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_class_model_run(X, Y, k, layer_dims, iterations, alpha, lambd, global_var):\n",
    "    \n",
    "    initial_parameters, momentumGrad, RMSGrad = initialize_parameters_deep(layer_dims, global_var)\n",
    "    \n",
    "    parameters = initial_parameters\n",
    "    \n",
    "    print_cost = global_var['print_cost']\n",
    "    checkGradient = global_var['checkGradient']\n",
    "    update_method = global_var['update_method']\n",
    "    batchNorm = global_var['batchNorm']\n",
    "    useSoftMax = global_var['useSoftMax']\n",
    "    checkTime = global_var['checkTime']\n",
    "    timerStart = False\n",
    "    td = []\n",
    "    \n",
    "\n",
    "    cost_array = np.zeros([iterations,1])\n",
    "    gradient_mean_array = np.zeros([iterations,3])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        if checkTime == True and i == round(iterations/10):\n",
    "            ts = time.time()\n",
    "            timerStart = True\n",
    "        \n",
    "        ### ONE EPOCH STARTS ###\n",
    "        \n",
    "        # Forward Propagation\n",
    "        if batchNorm == True:\n",
    "            AL, caches, norm_caches = L_model_forward(X, layer_dims, parameters, global_var, i)\n",
    "                \n",
    "        else:\n",
    "            AL, caches = L_model_forward(X, layer_dims, parameters, global_var, i)\n",
    "                \n",
    "        #Cost compute    \n",
    "        cost = compute_cost(AL, Y, layer_dims, parameters, lambd, global_var)\n",
    "            \n",
    "        cost_array[i, 0] = cost   \n",
    "            \n",
    "        if i % 100 == 0 and print_cost == True:\n",
    "            if useSoftMax == False:\n",
    "                print(\"Cost for class \" + str(k) + \" on the \" + str(i+1) + \"th iterations: \" + str(cost))\n",
    "            else:\n",
    "                print(\"SoftMax cost on \" +  str(i+1) + \"th iterations: \" + str(cost))\n",
    "                   \n",
    "        #Backward Propation\n",
    "            \n",
    "        if batchNorm == True:\n",
    "            grads = L_model_backward(AL, Y, caches, global_var, norm_caches)\n",
    "        else:\n",
    "            grads = L_model_backward(AL, Y, caches, global_var)\n",
    "\n",
    "            \n",
    "        gradient_mean_array[i, 0] = np.std(grads['dW1'])\n",
    "            \n",
    "        if batchNorm == True:\n",
    "            gradient_mean_array[i, 1] = np.mean(grads['dG1'])\n",
    "            gradient_mean_array[i, 2] = np.mean(grads['dB1'])\n",
    "\n",
    "        ### Conduct Gradient Checks\n",
    "        if i % 500 == 0 and checkGradient == True:\n",
    "            diff = gradient_check_n(parameters, grads, X, Y, layer_dims, global_var)\n",
    "                \n",
    "        ### Update Parameters ###    \n",
    "        parameters, momentumGrad, RMSGrad = update_parameters(Y.shape[1] , layer_dims, \n",
    "                                                              parameters, grads, momentumGrad, \n",
    "                                                              RMSGrad, alpha, lambd, i + 1, global_var)\n",
    "        if timerStart == True:\n",
    "            te = time.time()\n",
    "            td.append(te-ts)\n",
    "            timerStart = False\n",
    "        \n",
    "        ### ONE EPOCH ENDS ###\n",
    "    \n",
    "    if checkTime == True:\n",
    "        at = sum(td)/len(td)\n",
    "        print(\"Average time per EPOCH is: \" + str(at))\n",
    "        \n",
    "    return parameters, grads, cost_array, gradient_mean_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_training(numberOfClasses, X_train, Y_train, layer_dims, iterations, alpha, lambd, global_var):\n",
    "    \n",
    "    final_param_all_class = {}\n",
    "    global_grads = {}\n",
    "    global_var['isPredict'] = False\n",
    "    \n",
    "    if global_var['useSoftMax'] == False:\n",
    "        \n",
    "        for k in range(numberOfClasses):\n",
    "            \n",
    "            if k == 0:\n",
    "                Y_class = (Y_train==10)*Y_train\n",
    "            else: \n",
    "                Y_class = (Y_train==k)*Y_train    \n",
    "            Y_class[ Y_class > 0 ] = 1\n",
    "        \n",
    "             \n",
    "            parameters, grads, cost_array, grad_mean_array = single_class_model_run(X_train, Y_class, k, layer_dims, \n",
    "                                                                                iterations, alpha, lambd, global_var)\n",
    "            if global_var['plotGraph'] == True:\n",
    "                if global_var['batchNorm'] == True:\n",
    "                    plot_graph(grad_mean_array[:,0], 'Mean of dW1 per iteration')\n",
    "                    plot_graph(grad_mean_array[:,1], 'Mean of dG2 per iteration')\n",
    "                    plot_graph(grad_mean_array[:,1], 'Mean of dB2 per iteration')                \n",
    "                else:\n",
    "                    plot_graph(cost_array, (\"Cost function change per iteration for class \" +  str(k)))\n",
    "        \n",
    "            final_param_all_class[\"param\" + str(k)] = parameters\n",
    "            global_grads[\"grad\" + str(k)] = grads\n",
    "        \n",
    "    else:\n",
    "        Y_all_class = prepareSoftMaxY(Y_train, numberOfClasses)\n",
    "        # numberOfClasses x m\n",
    "        \n",
    "        # change the last layer to numberOfClasses nodes (instead of one)\n",
    "        layer_dims[-1] = numberOfClasses\n",
    "        \n",
    "        parameters, grads, cost_array, grad_mean_array = single_class_model_run(X_train, Y_all_class, 0, \n",
    "                                                                                layer_dims, iterations, \n",
    "                                                                                alpha, lambd, global_var)\n",
    "        if global_var['plotGraph'] == True:\n",
    "            if global_var['batchNorm'] == True:\n",
    "                plot_graph(grad_mean_array[:,0], 'Mean of dW1 per iteration')\n",
    "                plot_graph(grad_mean_array[:,1], 'Mean of dG2 per iteration')\n",
    "                plot_graph(grad_mean_array[:,1], 'Mean of dB2 per iteration')                \n",
    "            else:\n",
    "                plot_graph(cost_array, \"SoftMax cost function change per iteration\")\n",
    "        \n",
    "        final_param_all_class[\"finalparam\"] = parameters\n",
    "        global_grads[\"finalparam\"] = grads\n",
    "\n",
    "    \n",
    "    global_var['isPredict'] = True\n",
    "    \n",
    "    return final_param_all_class, global_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax cost on 1th iterations: 2.39856816838\n",
      "\u001b[93mGradient Check on 0th param: backward Prop error! difference = 0.999555654299\u001b[0m\n",
      "\u001b[93mGradient Check on 1th param: backward Prop error! difference = 0.999555654293\u001b[0m\n",
      "\u001b[93mGradient Check on 2th param: backward Prop error! difference = 0.999555653437\u001b[0m\n",
      "\u001b[93mGradient Check on 3th param: backward Prop error! difference = 0.999555654294\u001b[0m\n",
      "\u001b[93mGradient Check on 4th param: backward Prop error! difference = 0.999555654308\u001b[0m\n",
      "\u001b[93mGradient Check on 5th param: backward Prop error! difference = 0.999555654315\u001b[0m\n",
      "\u001b[93mGradient Check on 6th param: backward Prop error! difference = 0.999555654297\u001b[0m\n",
      "\u001b[93mGradient Check on 7th param: backward Prop error! difference = 0.999555654291\u001b[0m\n",
      "\u001b[93mGradient Check on 8th param: backward Prop error! difference = 0.999555654282\u001b[0m\n",
      "\u001b[93mGradient Check on 9th param: backward Prop error! difference = 0.99955565429\u001b[0m\n",
      "\u001b[93mGradient Check on 10th param: backward Prop error! difference = 0.999555654297\u001b[0m\n",
      "\u001b[93mGradient Check on 11th param: backward Prop error! difference = 0.999555654304\u001b[0m\n",
      "\u001b[93mGradient Check on 12th param: backward Prop error! difference = 0.999555654302\u001b[0m\n",
      "\u001b[93mGradient Check on 13th param: backward Prop error! difference = 0.999555654288\u001b[0m\n",
      "\u001b[93mGradient Check on 14th param: backward Prop error! difference = 0.999555654296\u001b[0m\n",
      "\u001b[93mGradient Check on 15th param: backward Prop error! difference = 0.999555654287\u001b[0m\n",
      "\u001b[93mGradient Check on 16th param: backward Prop error! difference = 0.99955565434\u001b[0m\n",
      "\u001b[93mGradient Check on 17th param: backward Prop error! difference = 0.999555654302\u001b[0m\n",
      "\u001b[93mGradient Check on 18th param: backward Prop error! difference = 0.999555654304\u001b[0m\n",
      "\u001b[93mGradient Check on 19th param: backward Prop error! difference = 0.999555654295\u001b[0m\n",
      "\u001b[93mGradient Check on 20th param: backward Prop error! difference = 0.999555654315\u001b[0m\n",
      "\u001b[93mGradient Check on 21th param: backward Prop error! difference = 0.999555654301\u001b[0m\n",
      "\u001b[93mGradient Check on 22th param: backward Prop error! difference = 0.999555654295\u001b[0m\n",
      "\u001b[93mGradient Check on 23th param: backward Prop error! difference = 0.999555654304\u001b[0m\n",
      "\u001b[93mGradient Check on 24th param: backward Prop error! difference = 0.9995556543\u001b[0m\n",
      "\u001b[93mGradient Check on 25th param: backward Prop error! difference = 0.999555654273\u001b[0m\n",
      "\u001b[93mGradient Check on 26th param: backward Prop error! difference = 0.999555654274\u001b[0m\n",
      "\u001b[93mGradient Check on 27th param: backward Prop error! difference = 0.999555654284\u001b[0m\n",
      "\u001b[93mGradient Check on 28th param: backward Prop error! difference = 0.999555654306\u001b[0m\n",
      "\u001b[93mGradient Check on 29th param: backward Prop error! difference = 0.999555654277\u001b[0m\n",
      "\u001b[93mGradient Check on 30th param: backward Prop error! difference = 0.999555654317\u001b[0m\n",
      "\u001b[93mGradient Check on 31th param: backward Prop error! difference = 0.99955565433\u001b[0m\n",
      "\u001b[93mGradient Check on 32th param: backward Prop error! difference = 0.999555654265\u001b[0m\n",
      "\u001b[93mGradient Check on 33th param: backward Prop error! difference = 0.99955565447\u001b[0m\n",
      "\u001b[93mGradient Check on 34th param: backward Prop error! difference = 0.999555654318\u001b[0m\n",
      "\u001b[93mGradient Check on 35th param: backward Prop error! difference = 0.999555654347\u001b[0m\n",
      "\u001b[93mGradient Check on 36th param: backward Prop error! difference = 0.99955565433\u001b[0m\n",
      "\u001b[93mGradient Check on 37th param: backward Prop error! difference = 0.999555654242\u001b[0m\n",
      "\u001b[93mGradient Check on 38th param: backward Prop error! difference = 0.999555654235\u001b[0m\n",
      "\u001b[93mGradient Check on 39th param: backward Prop error! difference = 0.999555654259\u001b[0m\n",
      "\u001b[93mGradient Check on 40th param: backward Prop error! difference = 0.999555654295\u001b[0m\n",
      "\u001b[93mGradient Check on 41th param: backward Prop error! difference = 0.999555651412\u001b[0m\n",
      "\u001b[93mGradient Check on 42th param: backward Prop error! difference = 0.999555654329\u001b[0m\n",
      "\u001b[93mGradient Check on 43th param: backward Prop error! difference = 0.999555654319\u001b[0m\n",
      "\u001b[93mGradient Check on 44th param: backward Prop error! difference = 0.999555654303\u001b[0m\n",
      "\u001b[93mGradient Check on 45th param: backward Prop error! difference = 0.999555654257\u001b[0m\n",
      "\u001b[93mGradient Check on 46th param: backward Prop error! difference = 0.999555654427\u001b[0m\n",
      "\u001b[93mGradient Check on 47th param: backward Prop error! difference = 0.9995556543\u001b[0m\n",
      "\u001b[93mGradient Check on 48th param: backward Prop error! difference = 0.999555654309\u001b[0m\n",
      "\u001b[93mGradient Check on 49th param: backward Prop error! difference = 0.999555655355\u001b[0m\n",
      "\u001b[93mGradient Check on 50th param: backward Prop error! difference = 0.99955565376\u001b[0m\n",
      "\u001b[93mGradient Check on 51th param: backward Prop error! difference = 0.999555654991\u001b[0m\n",
      "\u001b[93mGradient Check on 52th param: backward Prop error! difference = 0.99955565433\u001b[0m\n",
      "\u001b[93mGradient Check on 53th param: backward Prop error! difference = 0.999555654444\u001b[0m\n",
      "\u001b[93mGradient Check on 54th param: backward Prop error! difference = 0.999555655198\u001b[0m\n",
      "\u001b[93mGradient Check on 55th param: backward Prop error! difference = 0.999555654284\u001b[0m\n",
      "\u001b[93mGradient Check on 56th param: backward Prop error! difference = 0.999555654581\u001b[0m\n",
      "\u001b[93mGradient Check on 57th param: backward Prop error! difference = 0.999555654352\u001b[0m\n",
      "\u001b[93mGradient Check on 58th param: backward Prop error! difference = 0.999555654324\u001b[0m\n",
      "\u001b[93mGradient Check on 59th param: backward Prop error! difference = 0.999555654296\u001b[0m\n",
      "\u001b[93mGradient Check on 60th param: backward Prop error! difference = 0.999555654299\u001b[0m\n",
      "\u001b[93mGradient Check on 61th param: backward Prop error! difference = 0.99955565422\u001b[0m\n",
      "\u001b[93mGradient Check on 62th param: backward Prop error! difference = 0.999555654307\u001b[0m\n",
      "\u001b[93mGradient Check on 63th param: backward Prop error! difference = 0.999555654296\u001b[0m\n",
      "\u001b[93mGradient Check on 64th param: backward Prop error! difference = 0.999555654288\u001b[0m\n",
      "\u001b[93mGradient Check on 65th param: backward Prop error! difference = 0.999555654306\u001b[0m\n",
      "\u001b[93mGradient Check on 66th param: backward Prop error! difference = 0.999555654304\u001b[0m\n",
      "\u001b[93mGradient Check on 67th param: backward Prop error! difference = 0.999555654304\u001b[0m\n",
      "\u001b[93mGradient Check on 68th param: backward Prop error! difference = 0.9995556543\u001b[0m\n",
      "\u001b[93mGradient Check on 69th param: backward Prop error! difference = 0.999555654305\u001b[0m\n",
      "\u001b[93mGradient Check on 70th param: backward Prop error! difference = 0.999555654282\u001b[0m\n",
      "\u001b[93mGradient Check on 71th param: backward Prop error! difference = 0.999555654299\u001b[0m\n",
      "\u001b[93mGradient Check on 72th param: backward Prop error! difference = 0.999555654295\u001b[0m\n",
      "\u001b[93mGradient Check on 73th param: backward Prop error! difference = 0.999555654297\u001b[0m\n",
      "\u001b[93mGradient Check on 74th param: backward Prop error! difference = 0.999555654297\u001b[0m\n",
      "\u001b[93mGradient Check on 75th param: backward Prop error! difference = 0.999555654298\u001b[0m\n",
      "\u001b[93mGradient Check on 76th param: backward Prop error! difference = 0.999555654298\u001b[0m\n",
      "\u001b[93mGradient Check on 77th param: backward Prop error! difference = 0.999555654301\u001b[0m\n",
      "\u001b[93mGradient Check on 78th param: backward Prop error! difference = 0.999555654291\u001b[0m\n",
      "\u001b[93mGradient Check on 79th param: backward Prop error! difference = 0.999555654295\u001b[0m\n",
      "\u001b[93mGradient Check on 80th param: backward Prop error! difference = 0.999555654313\u001b[0m\n",
      "\u001b[93mGradient Check on 81th param: backward Prop error! difference = 0.999555654295\u001b[0m\n",
      "\u001b[93mGradient Check on 82th param: backward Prop error! difference = 0.999555654309\u001b[0m\n",
      "\u001b[93mGradient Check on 83th param: backward Prop error! difference = 0.999555654287\u001b[0m\n",
      "\u001b[93mGradient Check on 84th param: backward Prop error! difference = 0.999555654317\u001b[0m\n",
      "\u001b[93mGradient Check on 85th param: backward Prop error! difference = 0.999555654287\u001b[0m\n",
      "\u001b[93mGradient Check on 86th param: backward Prop error! difference = 0.999555654302\u001b[0m\n",
      "\u001b[93mGradient Check on 87th param: backward Prop error! difference = 0.999555654287\u001b[0m\n",
      "\u001b[93mGradient Check on 88th param: backward Prop error! difference = 0.999555654298\u001b[0m\n",
      "\u001b[93mGradient Check on 89th param: backward Prop error! difference = 0.999555654265\u001b[0m\n",
      "\u001b[93mGradient Check on 90th param: backward Prop error! difference = 0.999555654357\u001b[0m\n",
      "\u001b[93mGradient Check on 91th param: backward Prop error! difference = 0.999555654281\u001b[0m\n",
      "\u001b[93mGradient Check on 92th param: backward Prop error! difference = 0.99955565427\u001b[0m\n",
      "\u001b[93mGradient Check on 93th param: backward Prop error! difference = 0.999555654309\u001b[0m\n",
      "\u001b[93mGradient Check on 94th param: backward Prop error! difference = 0.999555654234\u001b[0m\n",
      "\u001b[93mGradient Check on 95th param: backward Prop error! difference = 0.999555654315\u001b[0m\n",
      "\u001b[93mGradient Check on 96th param: backward Prop error! difference = 0.999555654096\u001b[0m\n",
      "\u001b[93mGradient Check on 97th param: backward Prop error! difference = 0.999555654291\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mGradient Check on 98th param: backward Prop error! difference = 0.9995556543\u001b[0m\n",
      "\u001b[93mGradient Check on 99th param: backward Prop error! difference = 0.999555654274\u001b[0m\n",
      "\u001b[93mGradient Check on 100th param: backward Prop error! difference = 0.999555654265\u001b[0m\n",
      "\u001b[93mGradient Check on 101th param: backward Prop error! difference = 0.99955565428\u001b[0m\n",
      "\u001b[93mGradient Check on 102th param: backward Prop error! difference = 0.999555654135\u001b[0m\n",
      "\u001b[93mGradient Check on 103th param: backward Prop error! difference = 0.999555654307\u001b[0m\n",
      "\u001b[93mGradient Check on 104th param: backward Prop error! difference = 0.999555652677\u001b[0m\n",
      "\u001b[93mGradient Check on 105th param: backward Prop error! difference = 0.999555654299\u001b[0m\n",
      "\u001b[93mGradient Check on 106th param: backward Prop error! difference = 0.999555654307\u001b[0m\n",
      "\u001b[93mGradient Check on 107th param: backward Prop error! difference = 0.999555654307\u001b[0m\n",
      "\u001b[93mGradient Check on 108th param: backward Prop error! difference = 0.999555654313\u001b[0m\n",
      "\u001b[93mGradient Check on 109th param: backward Prop error! difference = 0.999555654313\u001b[0m\n",
      "\u001b[93mGradient Check on 110th param: backward Prop error! difference = 0.999555654341\u001b[0m\n",
      "\u001b[93mGradient Check on 111th param: backward Prop error! difference = 0.999555654292\u001b[0m\n",
      "\u001b[93mGradient Check on 112th param: backward Prop error! difference = 0.99955565431\u001b[0m\n",
      "\u001b[93mGradient Check on 113th param: backward Prop error! difference = 0.999555654296\u001b[0m\n",
      "\u001b[93mGradient Check on 114th param: backward Prop error! difference = 0.9995556543\u001b[0m\n",
      "\u001b[93mGradient Check on 115th param: backward Prop error! difference = 0.999555654302\u001b[0m\n",
      "\u001b[93mGradient Check on 116th param: backward Prop error! difference = 0.99955565429\u001b[0m\n",
      "\u001b[93mGradient Check on 117th param: backward Prop error! difference = 0.999555654278\u001b[0m\n",
      "\u001b[92mGradient Check on 118th param: Backward Prop OKAY! difference = nan\u001b[0m\n",
      "\u001b[92mGradient Check on 119th param: Backward Prop OKAY! difference = nan\u001b[0m\n",
      "\u001b[93mGradient Check on 120th param: backward Prop error! difference = 0.164357987907\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelpun_old/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:64: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mGradient Check on 121th param: backward Prop error! difference = 0.0100598767366\u001b[0m\n",
      "\u001b[93mGradient Check on 122th param: backward Prop error! difference = 0.00270511811663\u001b[0m\n",
      "\u001b[93mGradient Check on 123th param: backward Prop error! difference = 0.00086226381054\u001b[0m\n",
      "\u001b[93mGradient Check on 124th param: backward Prop error! difference = 0.000127634417808\u001b[0m\n",
      "\u001b[93mGradient Check on 125th param: backward Prop error! difference = 9.24799853232e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 126th param: backward Prop error! difference = 3.22114282578e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 127th param: backward Prop error! difference = 0.000511056587669\u001b[0m\n",
      "\u001b[93mGradient Check on 128th param: backward Prop error! difference = 0.00382633405734\u001b[0m\n",
      "\u001b[93mGradient Check on 129th param: backward Prop error! difference = 0.00524571049616\u001b[0m\n",
      "\u001b[93mGradient Check on 130th param: backward Prop error! difference = 0.00138431586772\u001b[0m\n",
      "\u001b[93mGradient Check on 131th param: backward Prop error! difference = 0.000109005375097\u001b[0m\n",
      "\u001b[93mGradient Check on 132th param: backward Prop error! difference = 1.80839587673e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 133th param: backward Prop error! difference = 2.11610725074e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 134th param: backward Prop error! difference = 0.000173722415768\u001b[0m\n",
      "\u001b[93mGradient Check on 135th param: backward Prop error! difference = 0.0948210491997\u001b[0m\n",
      "\u001b[93mGradient Check on 136th param: backward Prop error! difference = 0.238448991451\u001b[0m\n",
      "\u001b[92mGradient Check on 137th param: Backward Prop OKAY! difference = nan\u001b[0m\n",
      "\u001b[93mGradient Check on 138th param: backward Prop error! difference = 0.000598381027559\u001b[0m\n",
      "\u001b[93mGradient Check on 139th param: backward Prop error! difference = 0.00169215286637\u001b[0m\n",
      "\u001b[93mGradient Check on 140th param: backward Prop error! difference = 0.00149760016784\u001b[0m\n",
      "\u001b[93mGradient Check on 141th param: backward Prop error! difference = 0.000495144463988\u001b[0m\n",
      "\u001b[93mGradient Check on 142th param: backward Prop error! difference = 0.000777709390388\u001b[0m\n",
      "\u001b[93mGradient Check on 143th param: backward Prop error! difference = 1.32557710521e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 144th param: backward Prop error! difference = 2.55193678557e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 145th param: backward Prop error! difference = 2.1974989387e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 146th param: backward Prop error! difference = 0.000370092710093\u001b[0m\n",
      "\u001b[93mGradient Check on 147th param: backward Prop error! difference = 0.000138297274144\u001b[0m\n",
      "\u001b[93mGradient Check on 148th param: backward Prop error! difference = 4.47951581254e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 149th param: backward Prop error! difference = 1.27840751019e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 150th param: backward Prop error! difference = 6.13993271095e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 151th param: backward Prop error! difference = 2.45038026045e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 152th param: backward Prop error! difference = 8.60352551033e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 153th param: backward Prop error! difference = 4.49752307572e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 154th param: backward Prop error! difference = 1.16645623251e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 155th param: backward Prop error! difference = 9.48162461003e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 156th param: backward Prop error! difference = 0.00110256710083\u001b[0m\n",
      "\u001b[93mGradient Check on 157th param: backward Prop error! difference = 0.00383498869013\u001b[0m\n",
      "\u001b[93mGradient Check on 158th param: backward Prop error! difference = 0.000720496546717\u001b[0m\n",
      "\u001b[93mGradient Check on 159th param: backward Prop error! difference = 3.11089295631e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 160th param: backward Prop error! difference = 3.13905101056e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 161th param: backward Prop error! difference = 8.23578752985e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 162th param: backward Prop error! difference = 3.02645006776e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 163th param: backward Prop error! difference = 1.26086172928e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 164th param: backward Prop error! difference = 6.69095170601e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 165th param: backward Prop error! difference = 9.09143379987e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 166th param: backward Prop error! difference = 1.5566192972e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 167th param: backward Prop error! difference = 2.62839686198e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 168th param: backward Prop error! difference = 3.57270365322e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 169th param: backward Prop error! difference = 3.43320824864e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 170th param: backward Prop error! difference = 2.65819302954e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 171th param: Backward Prop OKAY! difference = 2.99143646311e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 172th param: backward Prop error! difference = 1.03596217002e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 173th param: Backward Prop OKAY! difference = 4.85105402917e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 174th param: backward Prop error! difference = 1.76314789942e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 175th param: backward Prop error! difference = 4.571866073e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 176th param: backward Prop error! difference = 0.000257055387417\u001b[0m\n",
      "\u001b[93mGradient Check on 177th param: backward Prop error! difference = 0.000491752289385\u001b[0m\n",
      "\u001b[93mGradient Check on 178th param: backward Prop error! difference = 0.000162207113833\u001b[0m\n",
      "\u001b[93mGradient Check on 179th param: backward Prop error! difference = 4.32870006819e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 180th param: backward Prop error! difference = 2.54615520513e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 181th param: backward Prop error! difference = 1.07173368754e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 182th param: backward Prop error! difference = 3.41846642284e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 183th param: backward Prop error! difference = 6.74613012063e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 184th param: backward Prop error! difference = 2.44356159362e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 185th param: backward Prop error! difference = 8.39819010468e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 186th param: backward Prop error! difference = 2.55393347555e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 187th param: backward Prop error! difference = 1.08245450037e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 188th param: backward Prop error! difference = 2.61331042325e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 189th param: Backward Prop OKAY! difference = 4.5441288803e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 190th param: Backward Prop OKAY! difference = 2.78099461648e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 191th param: Backward Prop OKAY! difference = 2.8608443562e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 192th param: Backward Prop OKAY! difference = 2.72396465867e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 193th param: Backward Prop OKAY! difference = 4.66482261029e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 194th param: Backward Prop OKAY! difference = 2.97542115083e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 195th param: backward Prop error! difference = 1.54004178907e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 196th param: backward Prop error! difference = 1.05565421571e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 197th param: backward Prop error! difference = 3.61989971377e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 198th param: backward Prop error! difference = 0.000104329527798\u001b[0m\n",
      "\u001b[93mGradient Check on 199th param: backward Prop error! difference = 4.83597333351e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 200th param: backward Prop error! difference = 2.2132976145e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 201th param: backward Prop error! difference = 3.70596063487e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 202th param: backward Prop error! difference = 3.61251115562e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 203th param: backward Prop error! difference = 3.13115270904e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 204th param: backward Prop error! difference = 3.92212950008e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 205th param: backward Prop error! difference = 3.53312754841e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 206th param: backward Prop error! difference = 1.43408540462e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 207th param: backward Prop error! difference = 1.35405044915e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 208th param: Backward Prop OKAY! difference = 4.73691885903e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 209th param: Backward Prop OKAY! difference = 4.59165523478e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 210th param: Backward Prop OKAY! difference = 3.2356855548e-08\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGradient Check on 211th param: Backward Prop OKAY! difference = 6.91263503361e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 212th param: Backward Prop OKAY! difference = 9.62714603888e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 213th param: Backward Prop OKAY! difference = 1.90961196388e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 214th param: backward Prop error! difference = 1.40035369193e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 215th param: backward Prop error! difference = 3.8007516915e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 216th param: backward Prop error! difference = 2.4645238203e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 217th param: backward Prop error! difference = 5.43123870258e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 218th param: backward Prop error! difference = 4.89185124963e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 219th param: backward Prop error! difference = 4.08302194349e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 220th param: backward Prop error! difference = 8.15950514437e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 221th param: backward Prop error! difference = 2.30788517433e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 222th param: Backward Prop OKAY! difference = 2.55720012443e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 223th param: backward Prop error! difference = 1.32857702416e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 224th param: Backward Prop OKAY! difference = 7.52524152619e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 225th param: backward Prop error! difference = 3.5341995173e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 226th param: backward Prop error! difference = 1.24263097084e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 227th param: backward Prop error! difference = 2.75836587137e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 228th param: Backward Prop OKAY! difference = 2.81774079196e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 229th param: Backward Prop OKAY! difference = 1.21691168286e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 230th param: Backward Prop OKAY! difference = 3.4702022605e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 231th param: Backward Prop OKAY! difference = 1.89008155969e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 232th param: Backward Prop OKAY! difference = 1.88979489693e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 233th param: Backward Prop OKAY! difference = 2.05078432913e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 234th param: backward Prop error! difference = 5.93835631156e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 235th param: backward Prop error! difference = 2.54224693072e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 236th param: backward Prop error! difference = 2.81294177094e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 237th param: backward Prop error! difference = 2.90054258241e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 238th param: backward Prop error! difference = 4.1270505157e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 239th param: backward Prop error! difference = 1.70466009134e-06\u001b[0m\n",
      "\u001b[92mGradient Check on 240th param: Backward Prop OKAY! difference = 2.66518521735e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 241th param: Backward Prop OKAY! difference = 4.58374543502e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 242th param: Backward Prop OKAY! difference = 1.6075474254e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 243th param: Backward Prop OKAY! difference = 4.48572417783e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 244th param: backward Prop error! difference = 2.35004415333e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 245th param: Backward Prop OKAY! difference = 4.13403085028e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 246th param: Backward Prop OKAY! difference = 5.12801459215e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 247th param: Backward Prop OKAY! difference = 1.45874330315e-09\u001b[0m\n",
      "\u001b[93mGradient Check on 248th param: backward Prop error! difference = 1.41816935963e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 249th param: Backward Prop OKAY! difference = 9.82102076299e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 250th param: Backward Prop OKAY! difference = 4.66414604493e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 251th param: Backward Prop OKAY! difference = 2.23037247125e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 252th param: Backward Prop OKAY! difference = 4.03968153143e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 253th param: Backward Prop OKAY! difference = 1.29706873733e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 254th param: backward Prop error! difference = 5.9464292331e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 255th param: Backward Prop OKAY! difference = 9.87249893522e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 256th param: backward Prop error! difference = 1.16888051423e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 257th param: backward Prop error! difference = 9.36025615041e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 258th param: backward Prop error! difference = 7.76616117424e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 259th param: backward Prop error! difference = 7.75501267435e-06\u001b[0m\n",
      "\u001b[92mGradient Check on 260th param: Backward Prop OKAY! difference = 3.98919772648e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 261th param: Backward Prop OKAY! difference = 2.77049461582e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 262th param: Backward Prop OKAY! difference = 3.02653638258e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 263th param: Backward Prop OKAY! difference = 9.64883865744e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 264th param: Backward Prop OKAY! difference = 2.51927426667e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 265th param: Backward Prop OKAY! difference = 1.30131895925e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 266th param: Backward Prop OKAY! difference = 2.44450821005e-10\u001b[0m\n",
      "\u001b[92mGradient Check on 267th param: Backward Prop OKAY! difference = 1.1444088479e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 268th param: Backward Prop OKAY! difference = 6.08350407615e-10\u001b[0m\n",
      "\u001b[93mGradient Check on 269th param: backward Prop error! difference = 7.68500669964e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 270th param: backward Prop error! difference = 1.45895991593e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 271th param: backward Prop error! difference = 1.87550122578e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 272th param: Backward Prop OKAY! difference = 2.1887056758e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 273th param: Backward Prop OKAY! difference = 8.15920631428e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 274th param: backward Prop error! difference = 5.13237985034e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 275th param: Backward Prop OKAY! difference = 1.93981155559e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 276th param: backward Prop error! difference = 4.41826451292e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 277th param: backward Prop error! difference = 1.84164173855e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 278th param: backward Prop error! difference = 0.000112880951491\u001b[0m\n",
      "\u001b[93mGradient Check on 279th param: backward Prop error! difference = 1.35205327777e-06\u001b[0m\n",
      "\u001b[92mGradient Check on 280th param: Backward Prop OKAY! difference = 4.43138600719e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 281th param: Backward Prop OKAY! difference = 1.29286034276e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 282th param: Backward Prop OKAY! difference = 1.08324374851e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 283th param: backward Prop error! difference = 1.11790573889e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 284th param: Backward Prop OKAY! difference = 5.57609055526e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 285th param: Backward Prop OKAY! difference = 6.95199739985e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 286th param: Backward Prop OKAY! difference = 2.02436194517e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 287th param: Backward Prop OKAY! difference = 6.65520423918e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 288th param: Backward Prop OKAY! difference = 2.1131018631e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 289th param: backward Prop error! difference = 3.43634206927e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 290th param: Backward Prop OKAY! difference = 7.08500002753e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 291th param: Backward Prop OKAY! difference = 8.52994326323e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 292th param: Backward Prop OKAY! difference = 2.16421521184e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 293th param: Backward Prop OKAY! difference = 8.20432096222e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 294th param: Backward Prop OKAY! difference = 8.78028016878e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 295th param: Backward Prop OKAY! difference = 8.3124402375e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 296th param: backward Prop error! difference = 8.79993834674e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 297th param: backward Prop error! difference = 7.37733958671e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 298th param: backward Prop error! difference = 0.000112004321894\u001b[0m\n",
      "\u001b[93mGradient Check on 299th param: backward Prop error! difference = 9.3675895776e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 300th param: backward Prop error! difference = 1.97153225349e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 301th param: Backward Prop OKAY! difference = 2.76423426898e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 302th param: Backward Prop OKAY! difference = 5.07736040717e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 303th param: backward Prop error! difference = 2.64342726863e-06\u001b[0m\n",
      "\u001b[92mGradient Check on 304th param: Backward Prop OKAY! difference = 5.78011244977e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 305th param: Backward Prop OKAY! difference = 1.24396518028e-08\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGradient Check on 306th param: Backward Prop OKAY! difference = 2.0785900073e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 307th param: Backward Prop OKAY! difference = 5.46906905177e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 308th param: Backward Prop OKAY! difference = 2.72786686881e-09\u001b[0m\n",
      "\u001b[93mGradient Check on 309th param: backward Prop error! difference = 5.50116197839e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 310th param: Backward Prop OKAY! difference = 9.73011566695e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 311th param: Backward Prop OKAY! difference = 8.20932206145e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 312th param: Backward Prop OKAY! difference = 8.69650092397e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 313th param: Backward Prop OKAY! difference = 8.42288807847e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 314th param: Backward Prop OKAY! difference = 1.42742566313e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 315th param: Backward Prop OKAY! difference = 2.18939833309e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 316th param: Backward Prop OKAY! difference = 9.78235153586e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 317th param: backward Prop error! difference = 7.84097053672e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 318th param: backward Prop error! difference = 4.20681382373e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 319th param: backward Prop error! difference = 1.02187398562e-06\u001b[0m\n",
      "\u001b[92mGradient Check on 320th param: Backward Prop OKAY! difference = 3.87649070281e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 321th param: Backward Prop OKAY! difference = 9.21485849563e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 322th param: Backward Prop OKAY! difference = 4.29651479602e-09\u001b[0m\n",
      "\u001b[93mGradient Check on 323th param: backward Prop error! difference = 3.79264956344e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 324th param: Backward Prop OKAY! difference = 3.17490966713e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 325th param: Backward Prop OKAY! difference = 1.01576672492e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 326th param: backward Prop error! difference = 5.1927577051e-06\u001b[0m\n",
      "\u001b[92mGradient Check on 327th param: Backward Prop OKAY! difference = 3.30455912798e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 328th param: Backward Prop OKAY! difference = 4.85762644326e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 329th param: Backward Prop OKAY! difference = 9.8203570308e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 330th param: backward Prop error! difference = 2.1692237158e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 331th param: backward Prop error! difference = 3.57625307961e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 332th param: Backward Prop OKAY! difference = 1.11797314093e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 333th param: Backward Prop OKAY! difference = 5.5445334618e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 334th param: Backward Prop OKAY! difference = 2.80164362825e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 335th param: Backward Prop OKAY! difference = 8.86266902916e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 336th param: backward Prop error! difference = 4.75477993145e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 337th param: backward Prop error! difference = 4.1201536776e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 338th param: backward Prop error! difference = 7.11677288285e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 339th param: backward Prop error! difference = 2.96216725591e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 340th param: Backward Prop OKAY! difference = 3.75884278337e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 341th param: Backward Prop OKAY! difference = 1.87113093613e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 342th param: Backward Prop OKAY! difference = 1.20283810967e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 343th param: Backward Prop OKAY! difference = 9.65875962524e-09\u001b[0m\n",
      "\u001b[93mGradient Check on 344th param: backward Prop error! difference = 1.61556363888e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 345th param: Backward Prop OKAY! difference = 5.25543282063e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 346th param: Backward Prop OKAY! difference = 1.70366505334e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 347th param: Backward Prop OKAY! difference = 3.08200672666e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 348th param: Backward Prop OKAY! difference = 6.13842922021e-09\u001b[0m\n",
      "\u001b[93mGradient Check on 349th param: backward Prop error! difference = 1.45109642757e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 350th param: Backward Prop OKAY! difference = 7.60932933454e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 351th param: backward Prop error! difference = 1.5293347121e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 352th param: Backward Prop OKAY! difference = 3.55210662331e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 353th param: Backward Prop OKAY! difference = 3.41120714309e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 354th param: Backward Prop OKAY! difference = 3.72246774794e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 355th param: Backward Prop OKAY! difference = 6.81489425895e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 356th param: backward Prop error! difference = 2.72618645711e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 357th param: backward Prop error! difference = 5.48984072951e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 358th param: backward Prop error! difference = 1.71562960131e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 359th param: backward Prop error! difference = 3.37732288034e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 360th param: backward Prop error! difference = 1.75165889625e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 361th param: Backward Prop OKAY! difference = 1.6688738949e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 362th param: Backward Prop OKAY! difference = 2.28143293283e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 363th param: Backward Prop OKAY! difference = 1.95562140652e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 364th param: Backward Prop OKAY! difference = 2.14707556746e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 365th param: Backward Prop OKAY! difference = 1.16186891191e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 366th param: backward Prop error! difference = 4.79876098809e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 367th param: Backward Prop OKAY! difference = 4.23480070027e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 368th param: Backward Prop OKAY! difference = 6.15647995309e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 369th param: Backward Prop OKAY! difference = 2.1724951493e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 370th param: Backward Prop OKAY! difference = 4.32203575132e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 371th param: Backward Prop OKAY! difference = 8.26005985767e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 372th param: backward Prop error! difference = 2.30882206505e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 373th param: backward Prop error! difference = 1.08466328164e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 374th param: Backward Prop OKAY! difference = 1.94476086473e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 375th param: Backward Prop OKAY! difference = 3.6533853357e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 376th param: backward Prop error! difference = 1.22059544454e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 377th param: backward Prop error! difference = 2.28908347191e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 378th param: backward Prop error! difference = 3.28175941309e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 379th param: backward Prop error! difference = 1.54240085382e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 380th param: backward Prop error! difference = 1.82007268868e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 381th param: Backward Prop OKAY! difference = 7.50507880079e-10\u001b[0m\n",
      "\u001b[92mGradient Check on 382th param: Backward Prop OKAY! difference = 5.11972033666e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 383th param: backward Prop error! difference = 2.06406682224e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 384th param: Backward Prop OKAY! difference = 1.00591986422e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 385th param: backward Prop error! difference = 1.33992627029e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 386th param: backward Prop error! difference = 1.6264961107e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 387th param: backward Prop error! difference = 2.11190611363e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 388th param: Backward Prop OKAY! difference = 1.35185344118e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 389th param: Backward Prop OKAY! difference = 2.69251111916e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 390th param: Backward Prop OKAY! difference = 9.61793177998e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 391th param: Backward Prop OKAY! difference = 4.17367238579e-10\u001b[0m\n",
      "\u001b[92mGradient Check on 392th param: Backward Prop OKAY! difference = 9.79519819463e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 393th param: Backward Prop OKAY! difference = 9.37124371788e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 394th param: backward Prop error! difference = 1.27815394827e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 395th param: backward Prop error! difference = 3.29480637629e-07\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mGradient Check on 396th param: backward Prop error! difference = 7.47431984109e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 397th param: backward Prop error! difference = 0.000106330938359\u001b[0m\n",
      "\u001b[93mGradient Check on 398th param: backward Prop error! difference = 1.98099376484e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 399th param: backward Prop error! difference = 3.21647731346e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 400th param: backward Prop error! difference = 1.26659196943e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 401th param: Backward Prop OKAY! difference = 6.77323620993e-10\u001b[0m\n",
      "\u001b[92mGradient Check on 402th param: Backward Prop OKAY! difference = 6.81827375563e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 403th param: backward Prop error! difference = 2.42876792389e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 404th param: backward Prop error! difference = 1.00397448667e-06\u001b[0m\n",
      "\u001b[92mGradient Check on 405th param: Backward Prop OKAY! difference = 7.88998289192e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 406th param: Backward Prop OKAY! difference = 1.87239220249e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 407th param: Backward Prop OKAY! difference = 1.86251844899e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 408th param: Backward Prop OKAY! difference = 3.90121482154e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 409th param: Backward Prop OKAY! difference = 1.47206626221e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 410th param: Backward Prop OKAY! difference = 2.55930308261e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 411th param: Backward Prop OKAY! difference = 1.40159272556e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 412th param: Backward Prop OKAY! difference = 1.5465668924e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 413th param: Backward Prop OKAY! difference = 8.09020815184e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 414th param: Backward Prop OKAY! difference = 3.26869498435e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 415th param: backward Prop error! difference = 7.96033415851e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 416th param: backward Prop error! difference = 1.21032229841e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 417th param: backward Prop error! difference = 5.3579215024e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 418th param: backward Prop error! difference = 2.94319977253e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 419th param: backward Prop error! difference = 5.67998161097e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 420th param: backward Prop error! difference = 3.62847954076e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 421th param: backward Prop error! difference = 8.57132199326e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 422th param: backward Prop error! difference = 1.07587254097e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 423th param: backward Prop error! difference = 1.15061990783e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 424th param: Backward Prop OKAY! difference = 4.17694586045e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 425th param: Backward Prop OKAY! difference = 8.53974761371e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 426th param: Backward Prop OKAY! difference = 2.77266349274e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 427th param: Backward Prop OKAY! difference = 5.36714869938e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 428th param: Backward Prop OKAY! difference = 5.50147329282e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 429th param: Backward Prop OKAY! difference = 7.60270720997e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 430th param: Backward Prop OKAY! difference = 2.16996209731e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 431th param: Backward Prop OKAY! difference = 4.65467408893e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 432th param: Backward Prop OKAY! difference = 4.15523067317e-09\u001b[0m\n",
      "\u001b[93mGradient Check on 433th param: backward Prop error! difference = 1.03653763255e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 434th param: backward Prop error! difference = 4.56721218157e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 435th param: backward Prop error! difference = 5.25145073568e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 436th param: backward Prop error! difference = 1.3485892797e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 437th param: backward Prop error! difference = 3.57407362393e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 438th param: backward Prop error! difference = 3.86913531535e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 439th param: backward Prop error! difference = 7.4024727778e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 440th param: backward Prop error! difference = 5.54307604814e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 441th param: backward Prop error! difference = 3.63471277239e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 442th param: Backward Prop OKAY! difference = 5.51388737024e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 443th param: Backward Prop OKAY! difference = 8.02043239421e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 444th param: Backward Prop OKAY! difference = 4.0096917018e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 445th param: Backward Prop OKAY! difference = 7.39453102533e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 446th param: Backward Prop OKAY! difference = 1.91173932864e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 447th param: Backward Prop OKAY! difference = 8.20517984165e-09\u001b[0m\n",
      "\u001b[92mGradient Check on 448th param: Backward Prop OKAY! difference = 5.36708882169e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 449th param: Backward Prop OKAY! difference = 3.78523203735e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 450th param: Backward Prop OKAY! difference = 5.41290392221e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 451th param: Backward Prop OKAY! difference = 1.7232379038e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 452th param: Backward Prop OKAY! difference = 4.30639458123e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 453th param: Backward Prop OKAY! difference = 5.26129253943e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 454th param: backward Prop error! difference = 6.93727105091e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 455th param: backward Prop error! difference = 2.2127826188e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 456th param: backward Prop error! difference = 5.67799924199e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 457th param: backward Prop error! difference = 9.09992712615e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 458th param: backward Prop error! difference = 3.9565142805e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 459th param: backward Prop error! difference = 1.92771580989e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 460th param: backward Prop error! difference = 0.00514309559457\u001b[0m\n",
      "\u001b[93mGradient Check on 461th param: backward Prop error! difference = 1.01363329502e-06\u001b[0m\n",
      "\u001b[92mGradient Check on 462th param: Backward Prop OKAY! difference = 1.06015275599e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 463th param: Backward Prop OKAY! difference = 1.37823511123e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 464th param: Backward Prop OKAY! difference = 5.04251191846e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 465th param: backward Prop error! difference = 2.47537364927e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 466th param: backward Prop error! difference = 1.09921929164e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 467th param: Backward Prop OKAY! difference = 4.60723063787e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 468th param: backward Prop error! difference = 2.54546432744e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 469th param: Backward Prop OKAY! difference = 4.13021736208e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 470th param: Backward Prop OKAY! difference = 1.42476986251e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 471th param: Backward Prop OKAY! difference = 2.67526460648e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 472th param: Backward Prop OKAY! difference = 3.84994291191e-08\u001b[0m\n",
      "\u001b[92mGradient Check on 473th param: Backward Prop OKAY! difference = 6.92063999644e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 474th param: backward Prop error! difference = 5.57541341936e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 475th param: backward Prop error! difference = 5.27419096468e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 476th param: backward Prop error! difference = 1.19952406288e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 477th param: backward Prop error! difference = 0.000884193218347\u001b[0m\n",
      "\u001b[93mGradient Check on 478th param: backward Prop error! difference = 0.00273266432648\u001b[0m\n",
      "\u001b[93mGradient Check on 479th param: backward Prop error! difference = 0.000233343465644\u001b[0m\n",
      "\u001b[93mGradient Check on 480th param: backward Prop error! difference = 1.79714259656e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 481th param: backward Prop error! difference = 8.86225465279e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 482th param: backward Prop error! difference = 1.42339789654e-07\u001b[0m\n",
      "\u001b[92mGradient Check on 483th param: Backward Prop OKAY! difference = 4.93702731222e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 484th param: backward Prop error! difference = 1.48917412905e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 485th param: backward Prop error! difference = 4.39505062382e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 486th param: backward Prop error! difference = 2.26477924743e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 487th param: backward Prop error! difference = 2.17431793181e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 488th param: backward Prop error! difference = 2.03526722406e-06\u001b[0m\n",
      "\u001b[92mGradient Check on 489th param: Backward Prop OKAY! difference = 7.9891982937e-08\u001b[0m\n",
      "\u001b[93mGradient Check on 490th param: backward Prop error! difference = 2.01551447105e-07\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mGradient Check on 491th param: backward Prop error! difference = 1.73536921774e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 492th param: backward Prop error! difference = 1.29773904632e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 493th param: backward Prop error! difference = 1.22233326905e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 494th param: backward Prop error! difference = 2.27600443522e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 495th param: backward Prop error! difference = 2.23420340031e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 496th param: backward Prop error! difference = 8.4670643905e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 497th param: backward Prop error! difference = 0.00307341303305\u001b[0m\n",
      "\u001b[92mGradient Check on 498th param: Backward Prop OKAY! difference = nan\u001b[0m\n",
      "\u001b[93mGradient Check on 499th param: backward Prop error! difference = 0.0122118376709\u001b[0m\n",
      "\u001b[93mGradient Check on 500th param: backward Prop error! difference = 0.000390644195448\u001b[0m\n",
      "\u001b[93mGradient Check on 501th param: backward Prop error! difference = 6.80054431029e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 502th param: backward Prop error! difference = 1.24572908699e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 503th param: backward Prop error! difference = 1.28758607277e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 504th param: backward Prop error! difference = 5.63486437828e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 505th param: backward Prop error! difference = 3.89999184267e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 506th param: backward Prop error! difference = 3.61862953512e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 507th param: backward Prop error! difference = 2.71086370708e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 508th param: backward Prop error! difference = 1.62015459475e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 509th param: backward Prop error! difference = 2.47681899417e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 510th param: backward Prop error! difference = 9.95579942441e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 511th param: backward Prop error! difference = 6.81367878483e-07\u001b[0m\n",
      "\u001b[93mGradient Check on 512th param: backward Prop error! difference = 9.76930177636e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 513th param: backward Prop error! difference = 1.80484773929e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 514th param: backward Prop error! difference = 9.37444236563e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 515th param: backward Prop error! difference = 8.73463367372e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 516th param: backward Prop error! difference = 0.0392180156317\u001b[0m\n",
      "\u001b[92mGradient Check on 517th param: Backward Prop OKAY! difference = nan\u001b[0m\n",
      "\u001b[92mGradient Check on 518th param: Backward Prop OKAY! difference = nan\u001b[0m\n",
      "\u001b[92mGradient Check on 519th param: Backward Prop OKAY! difference = nan\u001b[0m\n",
      "\u001b[93mGradient Check on 520th param: backward Prop error! difference = 0.270945785394\u001b[0m\n",
      "\u001b[93mGradient Check on 521th param: backward Prop error! difference = 0.0153195628519\u001b[0m\n",
      "\u001b[93mGradient Check on 522th param: backward Prop error! difference = 0.000963894898263\u001b[0m\n",
      "\u001b[93mGradient Check on 523th param: backward Prop error! difference = 1.91826933139e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 524th param: backward Prop error! difference = 1.71794695922e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 525th param: backward Prop error! difference = 1.01652683865e-06\u001b[0m\n",
      "\u001b[93mGradient Check on 526th param: backward Prop error! difference = 1.83546496362e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 527th param: backward Prop error! difference = 5.45822739317e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 528th param: backward Prop error! difference = 0.000867013677505\u001b[0m\n",
      "\u001b[93mGradient Check on 529th param: backward Prop error! difference = 0.000100163762008\u001b[0m\n",
      "\u001b[93mGradient Check on 530th param: backward Prop error! difference = 3.79734934284e-05\u001b[0m\n",
      "\u001b[93mGradient Check on 531th param: backward Prop error! difference = 0.000235895193556\u001b[0m\n",
      "\u001b[93mGradient Check on 532th param: backward Prop error! difference = 0.00060560477087\u001b[0m\n",
      "\u001b[93mGradient Check on 533th param: backward Prop error! difference = 0.00241879200937\u001b[0m\n",
      "\u001b[93mGradient Check on 534th param: backward Prop error! difference = 0.000892420163781\u001b[0m\n",
      "\u001b[93mGradient Check on 535th param: backward Prop error! difference = 0.00388094923028\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6ff2699922f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m final_param_all_class, global_grads = start_training(numberOfClasses, X_train, Y_train, layer_dims, \n\u001b[0;32m---> 39\u001b[0;31m                                                      iterations, alpha, lambd, global_var)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m train_predict, train_prob = make_predictions(X_train, Y_train, numberOfClasses, layer_dims,  \n",
      "\u001b[0;32m<ipython-input-36-9f97b651f8f0>\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(numberOfClasses, X_train, Y_train, layer_dims, iterations, alpha, lambd, global_var)\u001b[0m\n\u001b[1;32m     38\u001b[0m         parameters, grads, cost_array, grad_mean_array = single_class_model_run(X_train, Y_all_class, 0, \n\u001b[1;32m     39\u001b[0m                                                                                 \u001b[0mlayer_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                                                                                 alpha, lambd, global_var)\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'plotGraph'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batchNorm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-600d061566fc>\u001b[0m in \u001b[0;36msingle_class_model_run\u001b[0;34m(X, Y, k, layer_dims, iterations, alpha, lambd, global_var)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m### Conduct Gradient Checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheckGradient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_check_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m### Update Parameters ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-335f70c1537e>\u001b[0m in \u001b[0;36mgradient_check_n\u001b[0;34m(parameters, gradients, X, Y, layer_dims, global_var)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatchNorm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             AL_plus, _ , _ = L_model_forward(X, layer_dims, \n\u001b[0;32m---> 34\u001b[0;31m                                              vector_to_dictionary_custom(keys_labels, thetaplus), global_var)\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             AL_plus, _  = L_model_forward(X, layer_dims, vector_to_dictionary_custom(keys_labels, thetaplus), \n",
      "\u001b[0;32m<ipython-input-20-1de89e19de3f>\u001b[0m in \u001b[0;36mL_model_forward\u001b[0;34m(X, layer_dims, parameters, global_var, i)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatchNorm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'G'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'B'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mnorm_caches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-db2d2ecb3c54>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[0;34m(A_prev, W, b, activation, global_var, G, B)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatchNorm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#linear cache : (A, W, b, G, B)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mZtida\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchnorm_forward_computational_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_activation_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZtida\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-24606c5d1ed3>\u001b[0m in \u001b[0;36mbatchnorm_forward_computational_graph\u001b[0;34m(Z, G, B)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m### normalized Z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mZ_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ_mean_diff\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mZ_std_iver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m### ZGamma: easier for differentiation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#initialization\n",
    "np.random.seed(2)\n",
    "\n",
    "### Data Preparation Starts ###\n",
    "train_data_ratio = 0.90\n",
    "X_train, Y_train, X_dev, Y_dev = load_data(train_data_ratio)\n",
    "#m = X_train.shape[1]\n",
    "#mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)\n",
    "### Data Preparation Ends ###\n",
    "\n",
    "### Model Superparameters Start ###\n",
    "#layer_dims = [X_train.shape[0],40,20,5,1]\n",
    "layer_dims = [X_train.shape[0],25,16,8,1]\n",
    "#layer_dims = [X_train.shape[0],80,20,1]\n",
    "iterations = 500\n",
    "alpha = 0.002\n",
    "lambd = 0.1\n",
    "numberOfClasses =  10\n",
    "### Model Superparameters End ###\n",
    "\n",
    "cost_history_for_all_class = {}\n",
    "global_var = {}\n",
    "\n",
    "### On/Off Hyperparameters Start ###\n",
    "#global_var['checkActivation'] = False\n",
    "global_var['useSoftMax'] = True\n",
    "global_var['dropOut'] = False\n",
    "global_var['checkGradient'] = False\n",
    "global_var['checkGradientPrintDiff'] = True\n",
    "global_var['checkTime'] = False\n",
    "global_var['print_cost'] = True\n",
    "global_var['batchNorm'] = True\n",
    "global_var['batchNormBackMethod'] = \"abstract\"     #abstract -> thankGod function; or use \"computational\"\n",
    "global_var['plotGraph'] = False\n",
    "global_var['update_method'] = \"adams\"    #or \"grads\" or \"momentum\" OR \"adams\"\n",
    "### On/Off Hyperparameters End ###\n",
    "\n",
    "final_param_all_class, global_grads = start_training(numberOfClasses, X_train, Y_train, layer_dims, \n",
    "                                                     iterations, alpha, lambd, global_var)\n",
    "        \n",
    "train_predict, train_prob = make_predictions(X_train, Y_train, numberOfClasses, layer_dims,  \n",
    "                                             final_param_all_class, global_var, dset = \"Training\")\n",
    "dev_predict, dev_prob = make_predictions(X_dev, Y_dev, numberOfClasses, layer_dims,\n",
    "                                         final_param_all_class, global_var, dset = \"Dev\")\n",
    "\n",
    "#showrandomimage(X_dev, Y_dev, dev_predict.reshape(Y_dev.shape), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#showrandomimage(X_dev, Y_dev, dev_predict.reshape(Y_dev.shape), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
