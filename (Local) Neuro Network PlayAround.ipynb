{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Key learnings:\n",
    "# [X_train.shape[0],40,20,5,1] \n",
    "# batchnorm = True, grad = adams, iter=1000, alpha = 0.01, lambda = 0 train acc = 100% test = 92.6%\n",
    "\n",
    "\n",
    "#- Gradient checks work better if other techniques like momentum/RMS are not implemented\n",
    "#- \"division by zero in log\" is catched and attempted to be solved by addiing epsilon to (1 - AL) if <= 0. \n",
    "#   By doing so at least the cost function would not become nan\n",
    "#- it seems that the cost function is easier to subject to exploding grad if alpha is larger\n",
    "#- if mini-batch is not applied, the value of adams is not certain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_array(a):\n",
    "    \n",
    "    n, m = a.shape\n",
    "\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    #a_mean = np.mean(a, axis = 1).reshape(a.shape[0],1)\n",
    "    #a_std = np.std(a, axis = 1).reshape(a.shape[0],1)\n",
    "    \n",
    "    ### mean of data set: no of columns = number of features\n",
    "    a_mean = (1./m) * np.sum(a, axis = 1).reshape(n,1)\n",
    "\n",
    "    ### difference between dataset and mean\n",
    "    a_mean_diff = (a - a_mean)\n",
    "    \n",
    "    ### square of the difference\n",
    "    a_sq = np.power( (a - a_mean) , 2 )\n",
    "    \n",
    "    ### variance\n",
    "    a_var = (1./m) * np.sum( a_sq , axis = 1).reshape(n,1)\n",
    "    \n",
    "    ### standard deviation with addition of epsilon to avoid div by zero\n",
    "    a_std = np.sqrt(a_var + epsilon) \n",
    "    \n",
    "    a_std_iver = 1./a_std\n",
    "    \n",
    "    a_norm = a_mean_diff * a_std_iver\n",
    "    \n",
    "    assert(a_norm.shape == a.shape)\n",
    "    \n",
    "    return a_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showrandomimage(X, Y, Ypredict, showWrongOnly = False):\n",
    "    \n",
    "    dim = int(np.sqrt(X.shape[0]))\n",
    "    \n",
    "    if showWrongOnly == True:\n",
    "        mask = np.where(Y != Ypredict.reshape(Y.shape))\n",
    "        Y = Y[:,mask[1]]\n",
    "        X = X[:,mask[1]]\n",
    "        Ypredict = Ypredict[:,mask[1]]\n",
    "    \n",
    "    i = np.random.randint(0,X.shape[1])\n",
    "    \n",
    "    print(\"Prediction: \" + str(Ypredict[:,i]) + \" ; Y: \" + str(Y[:,i]))\n",
    "    \n",
    "    arr = X[:,i].reshape(dim,dim).T\n",
    "    plt.imshow(arr, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(train_data_ratio):\n",
    "    \n",
    "    mat = scipy.io.loadmat('ex4data1.mat')\n",
    "    X_raw = np.array(mat['X']).T\n",
    "    Y_raw = np.array(mat['y']).T\n",
    "    \n",
    "    #X = np.random.rand(400,5000)       #X n=400, m=5000\n",
    "    #Y = np.round(np.random.rand(1,1000)*100)%2\n",
    "    \n",
    "    m = Y_raw.shape[1]\n",
    "    train_size = round(m * train_data_ratio)   # 80% as training set\n",
    "    test_size = m - train_size\n",
    "    \n",
    "    p = np.random.permutation(m)\n",
    "        \n",
    "    #X_train = normal_array(X_raw)\n",
    "    X_train = X_raw[:, p[ 0 : train_size ] ]\n",
    "    Y_train = Y_raw[:, p[ 0 : train_size ] ]\n",
    "    X_dev = X_raw[:, p[ train_size : train_size + 1 + test_size] ]\n",
    "    Y_dev = Y_raw[:, p[ train_size : train_size + 1 + test_size] ]\n",
    "    \n",
    "    assert(X_train.shape[1] + X_dev.shape[1] == m)\n",
    "    \n",
    "    return X_train, Y_train, X_dev, Y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: random_mini_batches\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "    # number of mini batches of size mini_batch_size in your partitionning\n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:,k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:,(num_complete_minibatches * mini_batch_size) : m]\n",
    "        mini_batch_Y = shuffled_Y[:,(num_complete_minibatches * mini_batch_size) : m]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareSoftMaxY(Y_train, numberOfClasses): \n",
    "    Y_all_class = np.zeros([numberOfClasses, Y_train.shape[1]])\n",
    "    Y_trainSoftMax = Y_train\n",
    "    Y_trainSoftMax[Y_train == 10] = 0\n",
    "        \n",
    "    # change Y to a numberOfClasses x m matrix with class of pos = 1\n",
    "    for i in range(Y_trainSoftMax.shape[1]):\n",
    "        Y_all_class[Y_trainSoftMax[0,i], i] = 1\n",
    "    \n",
    "    return Y_all_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims, global_var):\n",
    "\n",
    "    parameters = {}\n",
    "    momentumGrad = {}            # for moving average gradient\n",
    "    RMSGrad = {}\n",
    "    ActRecord = {}\n",
    "    batchNorm = global_var['batchNorm']\n",
    "    #checkAct = global_var['checkActivation']\n",
    "    \n",
    "    #if checkAct == True:\n",
    "    #    actStatus = {}\n",
    "    \n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        ### Avoid Gradient Vanish or Exploding\n",
    "        smooth_gradient_adj = np.sqrt(2/layer_dims[l-1])    # to avoid vanishing or exploding gradients\n",
    "        \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * smooth_gradient_adj\n",
    "       \n",
    "        ### for moving average gradient ###\n",
    "        momentumGrad['dW' + str(l)]  = np.zeros([layer_dims[l],layer_dims[l-1]])\n",
    "       \n",
    "        ### for RMS moving average gradient ###\n",
    "        RMSGrad['dW' + str(l)]  = np.zeros([layer_dims[l],layer_dims[l-1]])\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            ### Creating Gamma and Beta for Z normalization\n",
    "            parameters['G' + str(l)] = np.ones([layer_dims[l],1]) * smooth_gradient_adj \n",
    "            #parameters['G' + str(l)] = np.random.randn(layer_dims[l],1) * smooth_gradient_adj \n",
    "            parameters['B' + str(l)] = np.zeros([layer_dims[l],1])  \n",
    "        \n",
    "            momentumGrad['dG' + str(l)]  = np.zeros([layer_dims[l],1])\n",
    "            momentumGrad['dB' + str(l)]  = np.zeros([layer_dims[l],1])     \n",
    "        \n",
    "            RMSGrad['dG' + str(l)]  = np.zeros([layer_dims[l],1])\n",
    "            RMSGrad['dB' + str(l)]  = np.zeros([layer_dims[l],1])   \n",
    "        \n",
    "            assert(parameters['G' + str(l)].shape == (layer_dims[l], 1))\n",
    "            assert(parameters['B' + str(l)].shape == (layer_dims[l], 1))\n",
    "            assert(momentumGrad['dG' + str(l)].shape == (layer_dims[l], 1))\n",
    "            assert(momentumGrad['dB' + str(l)].shape == (layer_dims[l], 1))\n",
    "        else:\n",
    "            parameters['b' + str(l)] = np.zeros([layer_dims[l],1])\n",
    "            momentumGrad['db' + str(l)] = np.zeros([layer_dims[l],1]) \n",
    "            RMSGrad['db' + str(l)] = np.zeros([layer_dims[l],1])\n",
    "            \n",
    "            assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "            \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))   \n",
    "        \n",
    "        #if checkAct == True:\n",
    "            # set all initial status to false\n",
    "            #actStatus['a' + str(l)] = np.zeros(([layer_dims[l],1]) , dtype=bool) \n",
    "    \n",
    "    #if checkAct == True:\n",
    "        #global_var['act'] = actStatus\n",
    "        \n",
    "            \n",
    "    return parameters, momentumGrad, RMSGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_graph(cost_array, stitle):\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    i = cost_array.shape[0]\n",
    "    \n",
    "    plt.plot(np.arange(0,i), cost_array,'-')\n",
    "    plt.title(stitle)\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 5), dpi=100)    \n",
    "    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    cache = (A, W, b)           #linear_cache\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    a = np.maximum(0,Z)\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_forward(Z):\n",
    "    \n",
    "    Zshift = Z - np.max(Z)\n",
    "    t = np.exp(Zshift)\n",
    "    a = np.divide(t, (np.sum(t, axis=0, keepdims=True)))\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    a = 1./(1+np.exp(-Z))\n",
    "    \n",
    "    #assert(np.sum(a <= 0) == 0 and np.sum(a >= 1) == 0)\n",
    "    \n",
    "    return a, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_forward(Z, G, B):    \n",
    "    \n",
    "    n, m = Z.shape    # n is the number of features and m is the number of training examples\n",
    "    \n",
    "    epsilon = 1e-8    # to \n",
    "    \n",
    "    ### mean of data set: no of columns = number of features\n",
    "    Z_mean = np.mean(Z, axis = 1).reshape(n,1)\n",
    "    \n",
    "    ### difference between dataset and mean\n",
    "    Z_mean_diff = (Z - Z_mean)\n",
    "    \n",
    "    ### square of the difference\n",
    "    Z_sq = Z_mean_diff ** 2 \n",
    "    \n",
    "    ### variance\n",
    "    Z_var = np.mean( Z_sq , axis = 1).reshape(n,1)\n",
    "    \n",
    "    ### standard deviation with addition of epsilon to avoid div by zero\n",
    "    Z_std = np.sqrt(Z_var + epsilon) \n",
    "    \n",
    "    ### standard deviation with addition of epsilon to avoid div by zero\n",
    "    Z_std_iver = 1./Z_std\n",
    "    \n",
    "    ### normalized Z\n",
    "    Z_norm = Z_mean_diff * Z_std_iver\n",
    "    \n",
    "    ### ZGamma: easier for differentiation\n",
    "    ZGamma = G * Z_norm\n",
    "    \n",
    "    Z_tda = ZGamma + B\n",
    "        \n",
    "    \n",
    "    norm_cache = Z_norm, G, B, Z_mean, Z_std_iver, Z_var\n",
    "        \n",
    "    assert (Z_tda.shape == Z.shape)\n",
    "    \n",
    "    return Z_tda, norm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_forward_computational_graph(Z, G, B):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Reference: \n",
    "    https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "    But seems that gradient check does not add up\n",
    "    last check: 25 Sept 2017 19:00\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n, m = Z.shape    # n is the number of features and m is the number of training examples\n",
    "    \n",
    "    epsilon = 1e-8    # to \n",
    "    \n",
    "    ### mean of data set: no of columns = number of features\n",
    "    Z_mean = (1./m) * np.sum(Z, axis = 1).reshape(n,1)\n",
    "    \n",
    "    ### difference between dataset and mean\n",
    "    Z_mean_diff = (Z - Z_mean)\n",
    "    \n",
    "    ### square of the difference\n",
    "    Z_sq = Z_mean_diff ** 2 \n",
    "    \n",
    "    ### variance\n",
    "    Z_var = (1./m) * np.sum( Z_sq , axis = 1).reshape(n,1)\n",
    "    \n",
    "    ### standard deviation with addition of epsilon to avoid div by zero\n",
    "    Z_std = np.sqrt(Z_var + epsilon) \n",
    "    \n",
    "    ### standard deviation with addition of epsilon to avoid div by zero\n",
    "    Z_std_iver = 1./Z_std\n",
    "    \n",
    "    ### normalized Z\n",
    "    Z_norm = Z_mean_diff * Z_std_iver\n",
    "    \n",
    "    ### ZGamma: easier for differentiation\n",
    "    ZGamma = G * Z_norm\n",
    "    \n",
    "    Z_tda = ZGamma + B\n",
    "    \n",
    "    norm_cache = Z_norm, G, B, Z_mean_diff, Z_std_iver, Z_var\n",
    "    \n",
    "    #norm_cache = Z_norm, G, B, Z_mean, Z_std_iver, Z_var\n",
    "            \n",
    "    assert (Z_tda.shape == Z.shape)\n",
    "    \n",
    "    return Z_tda, norm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_backward_computational_graph(dZ_tda, activation_cache, norm_cache):\n",
    "    \"\"\"\n",
    "    Reference: \n",
    "    https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "    But seems that gradient check does not add up\n",
    "    last check: 25 Sept 2017 19:00\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Z = activation_cache\n",
    "    Z_norm, G, B, Z_mean_diff, Z_std_iver, Z_var = norm_cache\n",
    "    \n",
    "    \n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    n, m = dZ_tda.shape           \n",
    "    \n",
    "    #assuming we have the right dJ_dZ_tda\n",
    "    \n",
    "    #Z_tda = GammaZ + B\n",
    "    dGammaZ = dZ_tda * 1\n",
    "    dB = np.sum((dZ_tda * 1), axis = 1).reshape(n, 1)                    #dJdB\n",
    "    \n",
    "    #GammaZ = G * Z_norm\n",
    "    dG = np.sum((dGammaZ * Z_norm), axis = 1).reshape(n, 1)               #dJdG    \n",
    "    dZ_norm = dZ_tda * G                                                          #dJdZ_norm    \n",
    "    \n",
    "    \n",
    "    #Z_norm = Z_mean_diff * Z_std_iver\n",
    "    \n",
    "    dZ_mean_diff1 = dZ_norm * Z_std_iver                                                   #dJdZ_mean_diff\n",
    "    dZ_std_iver = np.sum((dZ_norm * Z_mean_diff), axis = 1).reshape(n, 1)         #dJdZ_std_iver\n",
    "        \n",
    "    dZ_std = ( -1./ (Z_std_iver ** 2) ) * dZ_std_iver                      #dJdstd\n",
    "    \n",
    "    #Z_std = np.power((Z_var +  epsilon), (1/2))\n",
    "    \n",
    "    dZ_var = (0.5 * (1./ np.sqrt(Z_var +  epsilon) * 1)) * dZ_std            #dJddZ_var\n",
    "    \n",
    "    #Z_var = 1./m * np.sum(Z_sq)\n",
    "    \n",
    "    dZ_sq =  (1./m * np.ones(Z.shape)) * dZ_var                                    #dJdZ_sq\n",
    "    \n",
    "    #Z_sq = Z_mean_diff**2\n",
    "    \n",
    "    dZ_mean_diff2 = (2 * Z_mean_diff) * dZ_sq                            #dJdZ_mean_diff\n",
    "    \n",
    "    dZ_mean_diff = dZ_mean_diff1 + dZ_mean_diff2\n",
    "    \n",
    "    #dZ_mean_diff = z - dZ_mean\n",
    "    \n",
    "    dZ1 = 1 * (dZ_mean_diff)                                      \n",
    "                       \n",
    "    dZ_mean = np.sum((dZ_mean_diff), axis = 1).reshape(n, 1) * -1        #dJdZ_mean\n",
    "    \n",
    "    #dZ_mean = (1/m) * np.sum(z)\n",
    "    \n",
    "    dZ2 = dZ_mean * 1./m * np.ones(dZ_tda.shape)                                 #dJdZ\n",
    "    \n",
    "    dZ = dZ1 + dZ2\n",
    "    \n",
    "    ##########\n",
    "        \n",
    "    assert (dZ_tda.shape == dZ.shape)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    assert (dG.shape == (Z.shape[0],1))\n",
    "    assert (dB.shape == (Z.shape[0],1))\n",
    "    \n",
    "    \n",
    "    return dZ, dG, dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_backward_complicated(dout, activation_cache, norm_cache):\n",
    "    \"\"\"\n",
    "    Reference:\n",
    "    https://wiseodd.github.io/techblog/2016/07/04/batchnorm/\n",
    "    Seems that this one works for gradient check\n",
    "    Last update: 25 Sept 2016 20:00\n",
    "    \"\"\"\n",
    "\n",
    "    X = activation_cache\n",
    "    X_norm, gamma, beta, Z_mean_diff, Z_std_iver, var = norm_cache\n",
    "\n",
    "    assert(X.shape == dout.shape)\n",
    "    \n",
    "    n, m = dout.shape\n",
    "    \n",
    "    mu = X - Z_mean_diff\n",
    "\n",
    "    std_inv = 1. / np.sqrt(var + 1e-8)\n",
    "\n",
    "    dX_norm = dout * gamma\n",
    "    \n",
    "    \n",
    "    dvar = np.sum(dX_norm * Z_mean_diff, axis=1).reshape(n,1) * -.5 * std_inv**3\n",
    "    dmu = np.sum(dX_norm * -std_inv, axis=1, keepdims=True) + dvar * np.mean(-2. * Z_mean_diff, axis=1, keepdims=True)\n",
    "\n",
    "    dX = (dX_norm * std_inv) + (dvar * 2 * Z_mean_diff / m) + (dmu / m)\n",
    "    dgamma = np.sum(dout * X_norm, axis=1, keepdims=True)\n",
    "    dbeta = np.sum(dout, axis=1, keepdims=True)\n",
    "\n",
    "    dX = dX.reshape(n,m)\n",
    "\n",
    "    \n",
    "    return dX, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_backward_thankGod(dJ_dZtda, activation_cache, norm_cache):\n",
    "\n",
    "    # retrieve cache(s)\n",
    "    Z = activation_cache\n",
    "    Znorm, Gamma, Beta, Zmeandiff, Z_std_iver, Zvar = norm_cache\n",
    "    \n",
    "    # needed variables Z, gamma, beta, Zmean, Zvar,\n",
    "    \n",
    "    # prepare variable(s)\n",
    "    n, m = dJ_dZtda.shape\n",
    "    eps = 1e-8\n",
    "    \n",
    "    #assert(np.sum(1./(np.sqrt(Zvar + eps))) == np.sum(Z_std_iver))\n",
    "    assert(Z.shape == dJ_dZtda.shape)\n",
    "    \n",
    "    #dZtda = Gamma * Znorm + Beta\n",
    "    dZtda_dZnorm = Gamma\n",
    "    \n",
    "    dJ_dZnorm = dJ_dZtda * dZtda_dZnorm   # 1\n",
    "    #(n. m)\n",
    "    \n",
    "    #Znorm = (Z - Zmean) *  1/((Zvar + eps)**1/2)\n",
    "    # we need to compute dJ_dZ by breaking into 3 paths:\n",
    "    # 1) dJ_dZvar\n",
    "    # 2) dJ_dZmu\n",
    "    # 3) dJ_dZc (assuming other interdependent variables like Zmu, Zvar are constant)\n",
    "    # then dJ_dZ (total) = dJ_dZc + dJ_dZvar + dJ_dZmu\n",
    "    \n",
    "    #compute the derivative of Znorm wrt \"Zvar\" assuming other variables are constant\n",
    "    dZnorm_dZvar = np.sum( Zmeandiff, axis = 1, keepdims=True) * (-0.5) * ( (1./np.sqrt(Zvar + eps)) **3 )\n",
    "    # (n, 1)\n",
    "    #dJ_dZvar = np.sum(dJ_dZnorm, axis = 1).reshape(n, 1)  * dZnorm_dZvar\n",
    "    #(n, 1) cannot do this because np.sum elements have to be processed together\n",
    "    dJ_dZvar = np.sum( dJ_dZnorm * Zmeandiff, axis = 1, keepdims=True) * (-0.5) * ( (1./np.sqrt(Zvar + eps)) **3 )\n",
    "    # (n, 1)\n",
    "    \n",
    "    \n",
    "    #compute the derivative of Znorm wrt \"Zmu\" assuming other variablesare constant\n",
    "    dZnorm_dZmu = np.sum( (-1./np.sqrt(Zvar + eps)) , \n",
    "                         axis = 1).reshape(n,1) + dJ_dZvar * (-2./m) * np.sum( Zmeandiff, axis = 1).reshape(n, 1) \n",
    "    #(n, 1)\n",
    "    \n",
    "    #dJ_dZmu = np.sum(dJ_dZnorm, axis = 1).reshape(n, 1)  * dZnorm_dZmu\n",
    "    #(n, 1) cannot do this because np.sum elements have to be processed together\n",
    "    \n",
    "    dJ_dZmu = np.sum(dJ_dZnorm * (-1./np.sqrt(Zvar + eps)) , \n",
    "                     axis = 1, keepdims = True) + dJ_dZvar * (-2./m) * np.sum( Zmeandiff , axis = 1, keepdims = True)\n",
    "    \n",
    "    #(n, 1)\n",
    "    \n",
    "    #compute the derivative of Znorm wrt \"Z\" assuming other variables are constant\n",
    "    dZnorm_dZc = 1./(np.sqrt(Zvar + eps)) * 1\n",
    "    #dJ_dZc = dJ_dZnorm * dZnorm_dZc\n",
    "    #(n, m)\n",
    "    \n",
    "    #we need to compute dZmu_dZvar and dZmu_dZ before adding them up\n",
    "    #Zvar = 1/m * np.sum( (Z - Zmu) ** 2)\n",
    "    #dZvar_dZ =  2 * (Z - Zmu)/m \n",
    "    #(n, m)\n",
    "    \n",
    "    #Zmu = 1/m * np.sum(Z)\n",
    "    #dZmu_dZ = 1/m\n",
    "    #(n, m)\n",
    "    \n",
    "    #then we compuete the total dJ_dZ\n",
    "\n",
    "    dJ_dZ = (dJ_dZnorm * dZnorm_dZc) + (dJ_dZvar * 2 * Zmeandiff/m ) + (dJ_dZmu / m)\n",
    "    #(n, m)\n",
    "    \n",
    "    \n",
    "    dJ_dGamma = np.sum(dJ_dZtda * Znorm, axis=1).reshape(n, 1)\n",
    "    dJ_dBeta = np.sum(dJ_dZtda * 1, axis=1).reshape(n, 1)\n",
    "    \n",
    "    return dJ_dZ, dJ_dGamma, dJ_dBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation, global_var, G = 0, B = 0):\n",
    "\n",
    "    ####def linear_activation_forward(A_prev, W, b, activation):    \n",
    "    \n",
    "    batchNorm = global_var['batchNorm']\n",
    "    dropOut = global_var['dropOut']\n",
    "    dropOutRate = 0.15\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b, G, B)\n",
    "            Ztida, norm_cache = batchnorm_forward_computational_graph(Z, G, B)\n",
    "            if global_var['useSoftMax'] == False:\n",
    "                A, z_activation_cache = sigmoid(Ztida) \n",
    "            else:\n",
    "                A, z_activation_cache = softmax_forward(Ztida)\n",
    "        else:\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b)   \n",
    "            if global_var['useSoftMax'] == False:\n",
    "                A, z_activation_cache = sigmoid(Z)\n",
    "            else:\n",
    "                A, z_activation_cache = softmax_forward(Z)\n",
    "\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b, G, B)\n",
    "            Ztida, norm_cache = batchnorm_forward_computational_graph(Z, G, B)\n",
    "            A, z_activation_cache = relu(Ztida)\n",
    "        else:\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)    #linear cache : (A, W, b)\n",
    "            A, z_activation_cache = relu(Z)\n",
    "            \n",
    "        if dropOut == True and A.shape[0] >= 10:\n",
    "            d = np.random.rand(A.shape[0], A.shape[1])\n",
    "            A = A * (d > dropOutRate)\n",
    "            #print(\"dropout applied!\")\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    cache = (linear_cache, z_activation_cache)         #linear_cache is A, W, b, activation_cache is Z\n",
    "\n",
    "    if batchNorm == True:\n",
    "        return A, cache, norm_cache\n",
    "    else:\n",
    "        return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, layer_dims, parameters, global_var, i = -1):\n",
    "    \"\"\"    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    batchNorm = global_var['batchNorm']\n",
    "    isPredict = global_var['isPredict']\n",
    "    #checkAct = global_var['checkActivation']\n",
    "    \n",
    "    caches = []\n",
    "    norm_caches = []\n",
    "    A = X\n",
    "    L = len(layer_dims)                  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L - 1):\n",
    "        A_prev = A \n",
    "                \n",
    "        if batchNorm == True:\n",
    "            A, cache, norm_cache = linear_activation_forward(A_prev, parameters['W' + str(l)], 0, \"relu\", global_var, parameters['G' + str(l)], parameters['B' + str(l)])\n",
    "            norm_caches.append(norm_cache)\n",
    "        else:\n",
    "            A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\", global_var)\n",
    "        \n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    \n",
    "    if batchNorm == True:\n",
    "        AL, cache, norm_cache = linear_activation_forward(A, parameters['W' + str(L-1)], 0, \"sigmoid\", global_var, parameters['G' + str(L-1)], parameters['B' + str(L-1)])\n",
    "        norm_caches.append(norm_cache)\n",
    "    else:\n",
    "        AL, cache = linear_activation_forward(A, parameters['W' + str(L-1)], parameters['b' + str(L-1)], \"sigmoid\", global_var)\n",
    "        \n",
    "    \n",
    "    caches.append(cache)          # (linear_cache, z_activation_cache) \n",
    "        \n",
    "    assert(AL.shape == (parameters['W' + str(L-1)].shape[0],X.shape[1]))\n",
    "    \n",
    "    if batchNorm == True:       \n",
    "        return AL, caches, norm_caches\n",
    "    else:\n",
    "        return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, layer_dims, parameters, lambd, global_var):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    Y = np.array(Y, dtype=float)     # to avoid division by zero\n",
    "    SumSqW = 0                       # for regularization\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "\n",
    "    #cost = (1/m)*np.sum(-(Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    \n",
    "    if np.sum(AL <= 0) > 0:                    #check if there is any instances, true = 1\n",
    "        AL[AL <= 0] = 1e-7\n",
    "        print(\"AL below zeros detected\")\n",
    "        \n",
    "    if np.sum(AL >= 1) > 0:\n",
    "        sub = 1 - 1e-7\n",
    "        AL[AL > 1] = sub      #make it just slightly smaller than 1\n",
    "        print(\"(1 - AL) below zeros detected\")\n",
    "      \n",
    "    if global_var['useSoftMax'] == False:    \n",
    "        logprobs = np.multiply(-np.log(AL),Y) + np.multiply(-np.log(1 - AL), 1 - Y)\n",
    "    else:\n",
    "        logprobs = -1 * np.sum(Y * (np.log(AL)), axis=0, keepdims=True)\n",
    "    \n",
    "    ### Regularization ###\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(L-1): \n",
    "        SumSqW = SumSqW + np.sum(np.square(parameters[\"W\" + str(l + 1)]))\n",
    "        L2_reg = (1./(2 * m)) * lambd * SumSqW\n",
    "    \n",
    "    cost = 1./m * np.sum(logprobs) + L2_reg\n",
    "        \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    #dJ_dZ = dJ_dA * dA_dZ\n",
    "    #dA_dZ = 0 when Z <=0\n",
    "    #dA_dZ = 1 when Z > 0\n",
    "    #dJ_dZ = 0 when z <=0; = dJ_dA when Z > 0\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0 \n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache     # activation cache\n",
    "    \n",
    "    a = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    #dZ = np.multiply(np.multiply(a, (1-a)), dA)\n",
    "    dZ = dA * a * (1-a)       # dAL/dZ = a * (1-a)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_backward_extended(dA, cache):\n",
    "    \n",
    "    Z = cache    \n",
    "    a, _ = softmax_forward(Z)\n",
    "    needVerify = False\n",
    "    useForLoop = False\n",
    "    \n",
    "    assert(Z.shape == dA.shape)\n",
    "    \n",
    "    noOfClass, m = Z.shape\n",
    "    dJdZ = np.zeros([noOfClass, m])\n",
    "    \n",
    "    for k in range(m):\n",
    "        dAdZMatrix = np.zeros((noOfClass, noOfClass))\n",
    "        \n",
    "        if useForLoop == False:\n",
    "            dAdZMatrix = -np.outer(a[:, k], a[:, k]) + np.diag(a[:, k].flatten())\n",
    "        else:\n",
    "            dAdZ_forLoop = np.zeros((noOfClass, noOfClass))\n",
    "        \n",
    "            for i in range(noOfClass):\n",
    "                for j in range(noOfClass):\n",
    "                    dAdZ_forLoop[i, j] = a[i, k] * ((i == j) - a[j, k])\n",
    "            \n",
    "            dAdZMatrix = dAdZ_forLoop\n",
    "        \n",
    "        if needVerify == True and useForLoop == True:\n",
    "            if (np.sum(dAdZ_forLoop) - np.sum(dAdZMatrix)) > 1e-15:\n",
    "                print(\"difference between dAdZ_forLoop and Matrix is too big\")\n",
    "        \n",
    "            assert(dAdZMatrix.shape ==  dAdZ_forLoop.shape)\n",
    "        \n",
    "        assert(dAdZMatrix.shape == (noOfClass,noOfClass))\n",
    "\n",
    "        new_vector = np.sum ( (dA[:,k].reshape(noOfClass,1) * dAdZMatrix).T, axis=1, keepdims=True)\n",
    "    \n",
    "        if k == 0:\n",
    "            dJdZMatrix = new_vector\n",
    "\n",
    "        else:\n",
    "            dJdZMatrix = np.concatenate((dJdZMatrix, new_vector), axis=1)\n",
    "    \n",
    "    #hardcoded answer\n",
    "    dJdZa = a + dA*a\n",
    "    \n",
    "    #print(dJdZMatrix.shape)\n",
    "    \n",
    "    if np.sum(dJdZMatrix) - np.sum(dJdZa) > 1e-10:\n",
    "        print(\"difference between dJdZMatrix and hardcode calculation is too big\")\n",
    "    \n",
    "    return dJdZMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    t = np.exp(Z)\n",
    "    a = t/(np.sum(t, axis=0))\n",
    "    \n",
    "    dZ = a + dA * a\n",
    "    \n",
    "    assert (dZ.shape == dA.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache, batchNorm):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "       \n",
    "    dW = 1./m * np.dot(dZ, A_prev.T)   \n",
    "    dA_prev = np.dot(W.T, dZ)    \n",
    "        \n",
    "    if batchNorm ==  False:\n",
    "        db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        assert (db.shape == b.shape)\n",
    "        \n",
    "        \n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    \n",
    "    if batchNorm ==  True:\n",
    "        return dA_prev, dW\n",
    "    else:\n",
    "        return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, norm_cache, activation, batchNorm):\n",
    "    \n",
    "    batchNorm = global_var['batchNorm']\n",
    "    useSoftMax = global_var['useSoftMax']\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "\n",
    "        if batchNorm == True:\n",
    "            dZ_tda = relu_backward(dA, activation_cache)    \n",
    "            dZ, dG, dB = batchnorm_backward_thankGod(dZ_tda, activation_cache, norm_cache)     \n",
    "            dA_prev, dW = linear_backward(dZ, linear_cache, batchNorm)\n",
    "        else:\n",
    "            dZ = relu_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache, batchNorm)\n",
    "        \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            if useSoftMax == False:\n",
    "                dZ_tda = sigmoid_backward(dA, activation_cache)\n",
    "            else:\n",
    "                dZ_tda = softmax_backward_extended(dA, activation_cache)\n",
    "            dZ, dG, dB = batchnorm_backward_thankGod(dZ_tda, activation_cache, norm_cache)   \n",
    "            dA_prev, dW = linear_backward(dZ, linear_cache, batchNorm)  \n",
    "\n",
    "        else:\n",
    "            if useSoftMax == False:\n",
    "                dZ = sigmoid_backward(dA, activation_cache)\n",
    "            else:\n",
    "                dZ = softmax_backward_extended(dA, activation_cache)\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache, batchNorm)\n",
    "                \n",
    "\n",
    "    if batchNorm == True:  \n",
    "        return dA_prev, dW, dG, dB\n",
    "    else:\n",
    "        return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictionary_to_vector_custom(parameters):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    keys_labels = np.array(range(len(parameters)*3), dtype='U8').reshape(len(parameters),3)\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for key, value in sorted(parameters.items()):\n",
    "        \n",
    "        #Storing key names and dimenson\n",
    "        keys_labels[count, 0] = key\n",
    "        keys_labels[count,1] = value.shape[0]\n",
    "        keys_labels[count,2] = value.shape[1]\n",
    "        \n",
    "        #storing a N x 1 dimensional value vector\n",
    "        new_vector = np.reshape(parameters[key], (-1,1))\n",
    "        \n",
    "        if count == 0:\n",
    "            param_values = new_vector\n",
    "\n",
    "        else:\n",
    "            param_values = np.concatenate((param_values, new_vector), axis=0)\n",
    "    \n",
    "                \n",
    "        count = count + 1\n",
    "        \n",
    "    return keys_labels, param_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_to_dictionary_custom(keys_labels, param_values):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    last_index = 0\n",
    "    parameters = {}\n",
    "    \n",
    "    for i in range(keys_labels.shape[0]):\n",
    "        \n",
    "        key = keys_labels[i][0]\n",
    "        dim0 = int(keys_labels[i][1])\n",
    "        dim1 = int(keys_labels[i][2])\n",
    "        index_length = (dim0 * dim1)\n",
    "        \n",
    "        temp_array = param_values[last_index:last_index+index_length,0]\n",
    "        \n",
    "        temp_array = temp_array.reshape(dim0, dim1)\n",
    "        parameters[key] = temp_array\n",
    "        \n",
    "        last_index = last_index + index_length\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradients_to_vector_custom(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    len_no_dA = sum(1 for i in gradients if 'dA' not in i)  # find the length of vector without dA*\n",
    "    \n",
    "    no_dA_grad_labels = np.array(range(len_no_dA*3), dtype='U8').reshape(len_no_dA,3)\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for key, value in sorted(gradients.items()):\n",
    "        \n",
    "        if 'dA' not in key:\n",
    "        \n",
    "            #Storing key names and dimenson\n",
    "            no_dA_grad_labels[count, 0] = key\n",
    "            no_dA_grad_labels[count,1] = value.shape[0]\n",
    "            no_dA_grad_labels[count,2] = value.shape[1]\n",
    "        \n",
    "            #storing a N x 1 dimensional value vector\n",
    "            new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        \n",
    "            if count == 0:\n",
    "                no_dA_grad_values = new_vector\n",
    "\n",
    "            else:\n",
    "\n",
    "                no_dA_grad_values = np.concatenate((no_dA_grad_values, new_vector), axis=0)\n",
    "                \n",
    "            count = count + 1\n",
    "        \n",
    "    \n",
    "    return no_dA_grad_labels, no_dA_grad_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, batchNorm, norm_caches = []):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    batchNorm = global_var['batchNorm']\n",
    "    useSoftMax = global_var['useSoftMax']\n",
    "\n",
    "    \n",
    "    m = AL.shape[1] # A or Z retains the dimension of number of training examples m\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    noOfClass = Y.shape[0]\n",
    "    \n",
    "    if useSoftMax == False:\n",
    "        dAL = ( - (np.divide( Y, AL ) - np.divide(1 - Y, 1 - AL )) ) # derivative of cost with respect to AL\n",
    "    else:\n",
    "        dAL = -1 * np.divide( Y, AL ) \n",
    "        \n",
    "\n",
    "    \n",
    "    current_cache = caches[L-1]   # contains of linear cache (A, W, b,) and activation cache (Z)\n",
    "    \n",
    "    if batchNorm == True:\n",
    "        current_norm_cache  = norm_caches[L-1]\n",
    "    else:\n",
    "        current_norm_cache = norm_caches\n",
    "    \n",
    "    #### MAKE norm_caches to append ####\n",
    "    \n",
    "    ### first backpropagation :-> sigmoid\n",
    "    if batchNorm == True:\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"dG\" + str(L)], grads[\"dB\" + str(L)] = linear_activation_backward(dAL, current_cache, current_norm_cache, 'sigmoid', global_var)\n",
    "        #print(np.sum(grads[\"dG\" + str(L)]))\n",
    "    else:\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, current_norm_cache, 'sigmoid', global_var)\n",
    "        #print(np.sum(grads[\"dW\" + str(L)]))\n",
    "\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            dA_prev_temp, dW_temp, dG_temp, dB_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], caches[l], norm_caches[l] , 'relu', global_var)\n",
    "        else:\n",
    "            dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], caches[l], norm_caches , 'relu', global_var)\n",
    "\n",
    "        \n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            grads[\"dG\" + str(l+1)] = dG_temp\n",
    "            grads[\"dB\" + str(l+1)] = dB_temp\n",
    "        else:\n",
    "            grads[\"db\" + str(l+1)] = db_temp\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, layer_dims, global_var):\n",
    "\n",
    "    \n",
    "    # Set-up variables\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    batchNorm = global_var['batchNorm']\n",
    "    printdiff = global_var['checkGradientPrintDiff']\n",
    "    dropOut = global_var['dropOut']\n",
    "    keys_labels, param_values = dictionary_to_vector_custom(parameters)\n",
    "    no_dA_grad_labels, no_dA_grad_values = gradients_to_vector_custom(gradients)\n",
    "\n",
    "    num_parameters = param_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    assert (no_dA_grad_values.shape == param_values.shape)\n",
    "    \n",
    "    if dropOut == True:\n",
    "        print(\"Warning Dropout is ON!\")\n",
    "    \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        #print(\"Testing \" + str(i) + \"th parameter...\")\n",
    "        \n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaplus = np.copy(param_values)                           # Step 1\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon                 # Step 2\n",
    "        if batchNorm == True:\n",
    "            AL_plus, _ , _ = L_model_forward(X, layer_dims, \n",
    "                                             vector_to_dictionary_custom(keys_labels, thetaplus), global_var)\n",
    "        else:\n",
    "            AL_plus, _  = L_model_forward(X, layer_dims, vector_to_dictionary_custom(keys_labels, thetaplus), \n",
    "                                          global_var)\n",
    "        J_plus[i] = compute_cost(AL_plus, Y, layer_dims, parameters, 0.0, global_var)     # Step 3\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaminus = np.copy(param_values)                          # Step 1\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon               # Step 2        \n",
    "\n",
    "        if batchNorm == True:\n",
    "            AL_minus, _ , _ = L_model_forward(X, layer_dims, \n",
    "                                              vector_to_dictionary_custom(keys_labels, thetaminus), global_var)\n",
    "        else:\n",
    "            AL_minus, _ = L_model_forward(X, layer_dims, vector_to_dictionary_custom(keys_labels, thetaminus), \n",
    "                                          global_var)            \n",
    "        J_minus[i] = compute_cost(AL_minus, Y, layer_dims, parameters, 0.0, global_var)   # Step 3\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute gradapprox[i]\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i])/(2 * epsilon)\n",
    "        #print(gradapprox.shape)\n",
    "        #print(no_dA_grad_values.shape)\n",
    "    \n",
    "        # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "\n",
    "        numerator = np.linalg.norm(no_dA_grad_values[i] - gradapprox[i])                          # Step 1'\n",
    "        denominator = np.linalg.norm(no_dA_grad_values[i]) + np.linalg.norm(gradapprox[i])        # Step 2'\n",
    "        difference = np.divide(numerator, denominator)                                            # Step 3'\n",
    "\n",
    "        if printdiff == True:\n",
    "            if difference > 1e-7:\n",
    "                print (\"\\033[93m\" + \"Gradient Check on \" + str(i) + \"th param: backward Prop error! difference = \" + str(difference) + \"\\033[0m\")\n",
    "                #subprocess.call([\"afplay\", \"beep-08b.wav\"])\n",
    "            else:\n",
    "                print (\"\\033[92m\" + \"Gradient Check on \" + str(i) + \"th param: Backward Prop OKAY! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(m, layer_dims, parameters, grads, momentumGrad, RMSGrad, alpha, lambd, i, global_var):\n",
    "\n",
    "    B1 = 0.9\n",
    "    B2 = 0.98\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    momentumGrad_corrected = {}\n",
    "    RMSGrad_corrected = {}\n",
    "\n",
    "    batchNorm = global_var['batchNorm']\n",
    "    update_method = global_var['update_method']\n",
    "        \n",
    "\n",
    "    for l in range(L-1): \n",
    "\n",
    "        \n",
    "        ### Update Velocity by using B1 and (!-B1) and Grads ###\n",
    "        momentumGrad[\"dW\" + str(l+1)] = B1 * momentumGrad[\"dW\" + str(l+1)] + ((1 - B1) * grads[\"dW\" + str(l+1)])\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            momentumGrad[\"dG\" + str(l+1)] = B1 * momentumGrad[\"dG\" + str(l+1)] + ((1 - B1) * grads[\"dG\" + str(l+1)])\n",
    "            momentumGrad[\"dB\" + str(l+1)] = B1 * momentumGrad[\"dB\" + str(l+1)] + ((1 - B1) * grads[\"dB\" + str(l+1)])\n",
    "        else:\n",
    "            momentumGrad[\"db\" + str(l+1)] = B1 * momentumGrad[\"db\" + str(l+1)] + ((1 - B1) * grads[\"db\" + str(l+1)])\n",
    "\n",
    "        \n",
    "        ### Calculate corrected Velocity     \n",
    "        momentumGrad_corrected[\"dW\" + str(l+1)] = np.divide(momentumGrad[\"dW\" + str(l+1)], (1 - B1**i))\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            momentumGrad_corrected[\"dG\" + str(l+1)] = np.divide(momentumGrad[\"dG\" + str(l+1)], (1 - B1**i))\n",
    "            momentumGrad_corrected[\"dB\" + str(l+1)] = np.divide(momentumGrad[\"dB\" + str(l+1)], (1 - B1**i))        \n",
    "        else:\n",
    "            momentumGrad_corrected[\"db\" + str(l+1)] = np.divide(momentumGrad[\"db\" + str(l+1)], (1 - B1**i))        \n",
    "           \n",
    "        \n",
    "        ### Update RMS using B2 and Grads ###\n",
    "        RMSGrad[\"dW\" + str(l+1)] = B2 * RMSGrad[\"dW\" + str(l+1)] + np.multiply((1 - B2) , np.power( grads[\"dW\" + str(l+1)], 2))\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            RMSGrad[\"dG\" + str(l+1)] = B2 * RMSGrad[\"dG\" + str(l+1)] + np.multiply((1 - B2) , np.power( grads[\"dG\" + str(l+1)], 2))\n",
    "            RMSGrad[\"dB\" + str(l+1)] = B2 * RMSGrad[\"dB\" + str(l+1)] + np.multiply((1 - B2) , np.power( grads[\"dB\" + str(l+1)], 2))\n",
    "        else:\n",
    "            RMSGrad[\"db\" + str(l+1)] = B2 * RMSGrad[\"db\" + str(l+1)] + np.multiply((1 - B2) , np.power( grads[\"db\" + str(l+1)], 2))\n",
    "            \n",
    "        \n",
    "        ### Calculate corrected RMSVelocity\n",
    "        RMSGrad_corrected[\"dW\" + str(l+1)] = np.divide(RMSGrad[\"dW\" + str(l+1)], (1 - B2**i))\n",
    "        \n",
    "        if batchNorm == True:\n",
    "            RMSGrad_corrected[\"dG\" + str(l+1)] = np.divide(RMSGrad[\"dG\" + str(l+1)], (1 - B2**i))\n",
    "            RMSGrad_corrected[\"dB\" + str(l+1)] = np.divide(RMSGrad[\"dB\" + str(l+1)], (1 - B2**i))\n",
    "        else:\n",
    "            RMSGrad_corrected[\"db\" + str(l+1)] = np.divide(RMSGrad[\"db\" + str(l+1)], (1 - B2**i))\n",
    "                \n",
    "        ### UPDATE PARAMETERS ####\n",
    "        \n",
    "        if update_method == \"grads\":\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - alpha * (grads[\"dW\" + str(l+1)] + (parameters[\"W\" + str(l+1)] * (lambd/m)) )  \n",
    "            \n",
    "            if batchNorm == True:\n",
    "                parameters[\"G\" + str(l+1)] = parameters[\"G\" + str(l+1)] - alpha * (grads[\"dG\" + str(l+1)] + (parameters[\"G\" + str(l+1)] * (lambd/m)))\n",
    "                parameters[\"B\" + str(l+1)] = parameters[\"B\" + str(l+1)] - alpha * grads[\"dB\" + str(l+1)]\n",
    "            else:\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - alpha * grads[\"db\" + str(l+1)]\n",
    "                \n",
    "            \n",
    "        elif update_method == \"momentum\":\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - alpha * ( momentumGrad_corrected[\"dW\" + str(l+1)] + (parameters[\"W\" + str(l+1)] * (lambd/m)) )\n",
    "           \n",
    "            if batchNorm == True:\n",
    "                parameters[\"G\" + str(l+1)] = parameters[\"G\" + str(l+1)] - alpha * (momentumGrad_corrected[\"dG\" + str(l+1)] + (parameters[\"G\" + str(l+1)] * (lambd/m)))\n",
    "                parameters[\"B\" + str(l+1)] = parameters[\"B\" + str(l+1)] - alpha * momentumGrad_corrected[\"dB\" + str(l+1)]               \n",
    "            else:\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - alpha * momentumGrad_corrected[\"db\" + str(l+1)]               \n",
    "                 \n",
    "        \n",
    "        elif update_method == \"adams\":\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - alpha * ( np.divide(momentumGrad_corrected[\"dW\" + str(l+1)] , (np.sqrt(RMSGrad_corrected[\"dW\" + str(l+1)]) + epsilon)) + (parameters[\"W\" + str(l+1)] * (lambd/m)) )\n",
    "            \n",
    "            if batchNorm == True:\n",
    "                parameters[\"G\" + str(l+1)] = parameters[\"G\" + str(l+1)]- alpha * (np.divide(momentumGrad_corrected[\"dG\" + str(l+1)] , (np.sqrt(RMSGrad_corrected[\"dG\" + str(l+1)]) + epsilon)) + (parameters[\"G\" + str(l+1)] * (lambd/m)))\n",
    "                parameters[\"B\" + str(l+1)] = parameters[\"B\" + str(l+1)] - alpha * np.divide(momentumGrad_corrected[\"dB\" + str(l+1)] , (np.sqrt(RMSGrad_corrected[\"dB\" + str(l+1)]) + epsilon))        \n",
    "            else:\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - alpha * np.divide(momentumGrad_corrected[\"db\" + str(l+1)] , (np.sqrt(RMSGrad_corrected[\"db\" + str(l+1)]) + epsilon))        \n",
    "                \n",
    "            \n",
    "    return parameters, momentumGrad, RMSGrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(X, Y, numberOfClasses, layer_dims, final_params, global_var, dset):\n",
    "    \n",
    "    batchNorm = global_var['batchNorm']\n",
    "    useSoftMax = global_var['useSoftMax']\n",
    "    \n",
    "    predictions = np.zeros([1, Y.shape[1] ])\n",
    "    probability = np.zeros([numberOfClasses , Y.shape[1] ]) \n",
    "    \n",
    "    Y[Y == 10] = 0\n",
    "    \n",
    "    if useSoftMax == False:\n",
    "        \n",
    "        for p in range(numberOfClasses):\n",
    "            if batchNorm == True:\n",
    "                probability[p,:], caches, z_norm_caches = L_model_forward(X, layer_dims, \n",
    "                                                                      final_params[\"param\" + str(p)], global_var)\n",
    "            else:\n",
    "                probability[p,:], caches = L_model_forward(X, layer_dims, final_params[\"param\" + str(p)], global_var)\n",
    "    else:\n",
    "        if batchNorm == True:\n",
    "            probability, caches, z_norm_caches = L_model_forward(X, layer_dims, \n",
    "                                                                 final_param_all_class[\"finalparam\"], global_var)\n",
    "        else:\n",
    "            probability, caches = L_model_forward(X, layer_dims, final_param_all_class[\"finalparam\"], global_var)\n",
    "    \n",
    "    predictions = np.argmax(probability,axis=0)\n",
    "    predictions = predictions.T\n",
    "            \n",
    "    print(dset + \" accruracy: is \" + str(np.sum(predictions == Y)/Y.shape[1]*100) + \"%\")\n",
    "    \n",
    "    return predictions, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_class_model_run(X, Y, k, layer_dims, iterations, alpha, lambd, global_var):\n",
    "    \n",
    "    initial_parameters, momentumGrad, RMSGrad = initialize_parameters_deep(layer_dims, global_var)\n",
    "    \n",
    "    parameters = initial_parameters\n",
    "    \n",
    "    print_cost = global_var['print_cost']\n",
    "    checkGradient = global_var['checkGradient']\n",
    "    update_method = global_var['update_method']\n",
    "    batchNorm = global_var['batchNorm']\n",
    "    useSoftMax = global_var['useSoftMax']\n",
    "    checkTime = global_var['checkTime']\n",
    "    timerStart = False\n",
    "    td = []\n",
    "    \n",
    "\n",
    "    cost_array = np.zeros([iterations,1])\n",
    "    gradient_mean_array = np.zeros([iterations,3])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        if checkTime == True and i == round(iterations/10):\n",
    "            ts = time.time()\n",
    "            timerStart = True\n",
    "        \n",
    "        ### ONE EPOCH STARTS ###\n",
    "        \n",
    "        # Forward Propagation\n",
    "        if batchNorm == True:\n",
    "            AL, caches, norm_caches = L_model_forward(X, layer_dims, parameters, global_var, i)\n",
    "                \n",
    "        else:\n",
    "            AL, caches = L_model_forward(X, layer_dims, parameters, global_var, i)\n",
    "                \n",
    "        #Cost compute    \n",
    "        cost = compute_cost(AL, Y, layer_dims, parameters, lambd, global_var)\n",
    "            \n",
    "        cost_array[i, 0] = cost   \n",
    "            \n",
    "        if i % 100 == 0 and print_cost == True:\n",
    "            if useSoftMax == False:\n",
    "                print(\"Cost for class \" + str(k) + \" on the \" + str(i+1) + \"th iterations: \" + str(cost))\n",
    "            else:\n",
    "                print(\"SoftMax cost on \" +  str(i+1) + \"th iterations: \" + str(cost))\n",
    "                   \n",
    "        #Backward Propation\n",
    "            \n",
    "        if batchNorm == True:\n",
    "            grads = L_model_backward(AL, Y, caches, global_var, norm_caches)\n",
    "        else:\n",
    "            grads = L_model_backward(AL, Y, caches, global_var)\n",
    "\n",
    "            \n",
    "        gradient_mean_array[i, 0] = np.std(grads['dW1'])\n",
    "            \n",
    "        if batchNorm == True:\n",
    "            gradient_mean_array[i, 1] = np.mean(grads['dG1'])\n",
    "            gradient_mean_array[i, 2] = np.mean(grads['dB1'])\n",
    "\n",
    "        ### Conduct Gradient Checks\n",
    "        if i % 500 == 0 and checkGradient == True:\n",
    "            diff = gradient_check_n(parameters, grads, X, Y, layer_dims, global_var)\n",
    "                \n",
    "        ### Update Parameters ###    \n",
    "        parameters, momentumGrad, RMSGrad = update_parameters(Y.shape[1] , layer_dims, \n",
    "                                                              parameters, grads, momentumGrad, \n",
    "                                                              RMSGrad, alpha, lambd, i + 1, global_var)\n",
    "        if timerStart == True:\n",
    "            te = time.time()\n",
    "            td.append(te-ts)\n",
    "            timerStart = False\n",
    "        \n",
    "        ### ONE EPOCH ENDS ###\n",
    "    \n",
    "    if checkTime == True:\n",
    "        at = sum(td)/len(td)\n",
    "        print(\"Average time per EPOCH is: \" + str(at))\n",
    "        \n",
    "    return parameters, grads, cost_array, gradient_mean_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(numberOfClasses, X_train, Y_train, layer_dims, iterations, alpha, lambd, global_var):\n",
    "    \n",
    "    final_param_all_class = {}\n",
    "    global_grads = {}\n",
    "    global_var['isPredict'] = False\n",
    "    \n",
    "    if global_var['useSoftMax'] == False:\n",
    "        \n",
    "        for k in range(numberOfClasses):\n",
    "            \n",
    "            if k == 0:\n",
    "                Y_class = (Y_train==10)*Y_train\n",
    "            else: \n",
    "                Y_class = (Y_train==k)*Y_train    \n",
    "            Y_class[ Y_class > 0 ] = 1\n",
    "        \n",
    "             \n",
    "            parameters, grads, cost_array, grad_mean_array = single_class_model_run(X_train, Y_class, k, layer_dims, \n",
    "                                                                                iterations, alpha, lambd, global_var)\n",
    "            if global_var['plotGraph'] == True:\n",
    "                if global_var['batchNorm'] == True:\n",
    "                    plot_graph(grad_mean_array[:,0], 'Mean of dW1 per iteration')\n",
    "                    plot_graph(grad_mean_array[:,1], 'Mean of dG2 per iteration')\n",
    "                    plot_graph(grad_mean_array[:,1], 'Mean of dB2 per iteration')                \n",
    "                else:\n",
    "                    plot_graph(cost_array, (\"Cost function change per iteration for class \" +  str(k)))\n",
    "        \n",
    "            final_param_all_class[\"param\" + str(k)] = parameters\n",
    "            global_grads[\"grad\" + str(k)] = grads\n",
    "        \n",
    "    else:\n",
    "        Y_all_class = prepareSoftMaxY(Y_train, numberOfClasses)\n",
    "        # numberOfClasses x m\n",
    "        \n",
    "        # change the last layer to numberOfClasses nodes (instead of one)\n",
    "        layer_dims[-1] = numberOfClasses\n",
    "        \n",
    "        parameters, grads, cost_array, grad_mean_array = single_class_model_run(X_train, Y_all_class, 0, \n",
    "                                                                                layer_dims, iterations, \n",
    "                                                                                alpha, lambd, global_var)\n",
    "        if global_var['plotGraph'] == True:\n",
    "            if global_var['batchNorm'] == True:\n",
    "                plot_graph(grad_mean_array[:,0], 'Mean of dW1 per iteration')\n",
    "                plot_graph(grad_mean_array[:,1], 'Mean of dG2 per iteration')\n",
    "                plot_graph(grad_mean_array[:,1], 'Mean of dB2 per iteration')                \n",
    "            else:\n",
    "                plot_graph(cost_array, \"SoftMax cost function change per iteration\")\n",
    "        \n",
    "        final_param_all_class[\"finalparam\"] = parameters\n",
    "        global_grads[\"finalparam\"] = grads\n",
    "\n",
    "    \n",
    "    global_var['isPredict'] = True\n",
    "    \n",
    "    return final_param_all_class, global_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax cost on 1th iterations: 2.40441830688\n",
      "SoftMax cost on 101th iterations: 0.99067041587\n",
      "SoftMax cost on 201th iterations: 0.56180903327\n",
      "SoftMax cost on 301th iterations: 0.337277102595\n",
      "SoftMax cost on 401th iterations: 0.211388234314\n",
      "Training accruracy: is 99.8352941176%\n",
      "Dev accruracy: is 87.7333333333%\n"
     ]
    }
   ],
   "source": [
    "#initialization\n",
    "np.random.seed(2)\n",
    "\n",
    "### Data Preparation Starts ###\n",
    "train_data_ratio = 0.85\n",
    "X_train, Y_train, X_dev, Y_dev = load_data(train_data_ratio)\n",
    "#m = X_train.shape[1]\n",
    "#mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)\n",
    "### Data Preparation Ends ###\n",
    "\n",
    "### Model Superparameters Start ###\n",
    "#layer_dims = [X_train.shape[0],40,20,5,1]\n",
    "layer_dims = [X_train.shape[0],25,16,8,1]\n",
    "#layer_dims = [X_train.shape[0],80,20,1]\n",
    "iterations = 500\n",
    "alpha = 0.002\n",
    "lambd = 0.2\n",
    "numberOfClasses =  10\n",
    "### Model Superparameters End ###\n",
    "\n",
    "cost_history_for_all_class = {}\n",
    "global_var = {}\n",
    "\n",
    "### On/Off Hyperparameters Start ###\n",
    "#global_var['checkActivation'] = False\n",
    "global_var['useSoftMax'] = True\n",
    "global_var['dropOut'] = False\n",
    "global_var['checkGradient'] = False\n",
    "global_var['checkGradientPrintDiff'] = True\n",
    "global_var['checkTime'] = False\n",
    "global_var['print_cost'] = True\n",
    "global_var['batchNorm'] = True\n",
    "global_var['plotGraph'] = False\n",
    "global_var['update_method'] = \"adams\"    #or \"grads\" or \"momentum\" OR \"adams\"\n",
    "### On/Off Hyperparameters End ###\n",
    "\n",
    "final_param_all_class, global_grads = start_training(numberOfClasses, X_train, Y_train, layer_dims, \n",
    "                                                     iterations, alpha, lambd, global_var)\n",
    "        \n",
    "train_predict, train_prob = make_predictions(X_train, Y_train, numberOfClasses, layer_dims,  \n",
    "                                             final_param_all_class, global_var, dset = \"Training\")\n",
    "dev_predict, dev_prob = make_predictions(X_dev, Y_dev, numberOfClasses, layer_dims,\n",
    "                                         final_param_all_class, global_var, dset = \"Dev\")\n",
    "\n",
    "#showrandomimage(X_dev, Y_dev, dev_predict.reshape(Y_dev.shape), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showrandomimage(X_dev, Y_dev, dev_predict.reshape(Y_dev.shape), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
